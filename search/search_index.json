{"config":{"lang":["en","zh"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"index.html","title":"EvoAgentX","text":"<p> An automated framework for evaluating and evolving agentic workflows. </p> <p> </p>"},{"location":"index.html#-introduction","title":"\ud83d\ude80 Introduction","text":"<p>EvoAgentX is an open-source framework designed to automate the generation, execution, evaluation and optimization of agentic workflows. By leveraging large language models (LLMs), EvoAgentX enables developers and researchers to prototype, test, and deploy multi-agent systems that grow in complexity and capability over time. </p>"},{"location":"index.html#-key-features","title":"\u2728 Key Features","text":"<ul> <li>Easy Agent and Workflow Customization: Easily create customized agents and workflows using natural language prompts. EvoAgentX makes it easy to turn your high-level ideas to working systems. </li> <li>Automatic Workflow Generation &amp; Execution: Automatically generate and execute agentic workflows from simple goal descriptions, reducing manual workload in multi-agent system design. </li> <li>WorkFlow Optimization: Integrates existing workflow optimization techniques that iteratively refine workflows for improved performance. </li> <li>Benchmarking &amp; Evaluation: Includes built-in benchmarks and standardized evaluation metrics to measure workflow. effectiveness across different tasks and agent configurations </li> <li>Workflow Execution Toolkit: Offers a suite of tools essential for executing complex workflows, such as search components and the Model Context Protocol (MCP). </li> </ul>"},{"location":"index.html#-how-it-works","title":"\ud83d\udd0d How It Works","text":"<p>EvoAgentX uses a modular architecture with the following core components:</p> <ol> <li>Workflow Generator: Creates agentic workflows based on your goals</li> <li>Agent Manager: Handles agent creation, configuration, and deployment</li> <li>Workflow Executor: Runs workflows efficiently with proper agent communication</li> <li>Evaluators: Provides performance metrics and improvement suggestions</li> <li>Optimizers: Evolves workflows to enhance performance over time</li> </ol>"},{"location":"index.html#-community","title":"\ud83d\udc65 Community","text":"<ul> <li>Discord: Join our Discord Channel for discussions and support</li> <li>GitHub: Contribute to the project on GitHub</li> <li>Email: Contact us at evoagentx.ai@gmail.com</li> <li>WeChat: Connect with us on WeChat for updates and support.</li> </ul>"},{"location":"index.html#-contributing","title":"\ud83e\udd1d Contributing","text":"<p>We welcome contributions from the community! Please refer to our Contributing Guidelines for more details.</p>"},{"location":"installation.html","title":"Installation Guide for EvoAgentX","text":"<p>This guide will walk you through the process of installing EvoAgentX on your system, setting up the required dependencies, and configuring the framework for your projects.</p>"},{"location":"installation.html#prerequisites","title":"Prerequisites","text":"<p>Before installing EvoAgentX, make sure you have the following prerequisites:</p> <ul> <li>Python 3.10 or higher</li> <li>pip (Python package installer)</li> <li>Git (for cloning the repository)</li> <li>Conda (recommended for environment management, but optional)</li> </ul>"},{"location":"installation.html#installation-methods","title":"Installation Methods","text":"<p>There are several ways to install EvoAgentX. Choose the method that best suits your needs.</p>"},{"location":"installation.html#method-1-using-pip-recommended","title":"Method 1: Using pip (Recommended)","text":"<p>The simplest way to install EvoAgentX is using pip:</p> <pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre>"},{"location":"installation.html#method-2-from-source-for-development","title":"Method 2: From Source (For Development)","text":"<p>If you want to contribute to EvoAgentX or need the latest development version, you can install it directly from the source:</p> <pre><code># Clone the repository\ngit clone https://github.com/EvoAgentX/EvoAgentX/\n\n# Navigate to the project directory\ncd EvoAgentX\n\n# Install the package in development mode\npip install -e .\n</code></pre>"},{"location":"installation.html#method-3-using-conda-environment-recommended-for-isolation","title":"Method 3: Using Conda Environment (Recommended for Isolation)","text":"<p>If you prefer to use Conda for managing your Python environments, follow these steps:</p> <pre><code># Create a new conda environment\nconda create -n evoagentx python=3.10\n\n# Activate the environment\nconda activate evoagentx\n\n# Install the package\npip install -r requirements.txt\n# OR install in development mode\npip install -e .\n</code></pre>"},{"location":"installation.html#verifying-installation","title":"Verifying Installation","text":"<p>To verify that EvoAgentX has been installed correctly, run the following Python code:</p> <pre><code>import evoagentx\n\n# Print the version\nprint(evoagentx.__version__)\n</code></pre> <p>You should see the current version of EvoAgentX printed to the console.</p>"},{"location":"quickstart.html","title":"EvoAgentX Quickstart Guide","text":"<p>This quickstart guide will walk you through the essential steps to set up and start using EvoAgentX. In this tutorial, you'll learn how to:</p> <ol> <li>Configure your API keys to access LLMs </li> <li>Automatically create and execute workflows </li> </ol>"},{"location":"quickstart.html#installation","title":"Installation","text":"<p><pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre> Please refere to Installation Guide for more details about the installation. </p>"},{"location":"quickstart.html#api-key--llm-setup","title":"API Key &amp; LLM Setup","text":"<p>The first step to execute a workflow in EvoAgentX is configuring your API keys to access LLMs. There are two recommended methods to configure your API keys:</p>"},{"location":"quickstart.html#method-1-set-environment-variables-in-the-terminal","title":"Method 1: Set Environment Variables in the Terminal","text":"<p>This method sets the API key directly in your system environment.</p> <p>For Linux/macOS:  <pre><code>export OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre></p> <p>For Windows Command Prompt:  <pre><code>set OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre></p> <p>For Windows PowerShell: <pre><code>$env:OPENAI_API_KEY=\"&lt;your-openai-api-key&gt;\" # \" is required \n</code></pre></p> <p>Once set, you can access the key in your Python code with: <pre><code>import os\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre></p>"},{"location":"quickstart.html#method-2-use-a-env-file","title":"Method 2: Use a <code>.env</code> File","text":"<p>You can also store your API key in a <code>.env</code> file inside the root folder of your project.</p> <p>Create a file named <code>.env</code> with the following content: <pre><code>OPENAI_API_KEY=&lt;your-openai-api-key&gt;\n</code></pre></p> <p>Then, in your Python code, you can load the environment settings using <code>python-dotenv</code>: <pre><code>from dotenv import load_dotenv \nimport os \n\nload_dotenv() # Loads environment variables from .env file\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre> \ud83d\udd10 Tip: Never commit your <code>.env</code> file to public platform (e.g., GitHub). Add it to <code>.gitignore</code>.</p>"},{"location":"quickstart.html#configure-and-use-the-llm-in-evoagentx","title":"Configure and Use the LLM in EvoAgentX","text":"<p>Once your API key is configured, you can initialize and use the LLM as follows:</p> <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# Load the API key from environment\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Define LLM configuration\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",       # Specify the model name\n    openai_key=OPENAI_API_KEY, # Pass the key directly\n    stream=True,               # Enable streaming response\n    output_response=True       # Print response to stdout\n)\n\n# Initialize the language model\nllm = OpenAILLM(config=openai_config)\n\n# Generate a response from the LLM\nresponse = llm.generate(prompt=\"What is Agentic Workflow?\")\n</code></pre> <p>You can find more details about supported LLM types and their parameters in the LLM module guide.</p>"},{"location":"quickstart.html#automatic-workflow-generation-and-execution","title":"Automatic WorkFlow Generation and Execution","text":"<p>Once your API key and language model are configured, you can automatically generate and execute agentic workflows in EvoAgentX. This section walks you through the core steps: generating a workflow from a goal, instantiating agents, and running the workflow to get results.</p> <p>First, let's import the necessary modules:</p> <pre><code>from evoagentx.workflow import WorkFlowGenerator, WorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\n</code></pre>"},{"location":"quickstart.html#step-1-generate-workflow-and-agents","title":"Step 1: Generate WorkFlow and Agents","text":"<p>Use the <code>WorkFlowGenerator</code> to automatically create a workflow based on a natural language goal: <pre><code>goal = \"Generate html code for the Tetris game that can be played in the browser.\"\nwf_generator = WorkFlowGenerator(llm=llm)\nworkflow_graph: WorkFlowGraph = wf_generator.generate_workflow(goal=goal)\n</code></pre> <code>WorkFlowGraph</code> is a data structure that stores the overall workflow plan \u2014 including task nodes and their relationships \u2014 but does not yet include executable agents.</p> <p>You can optionally visualize or save the generated workflow: <pre><code># Visualize the workflow structure (optional)\nworkflow_graph.display()\n\n# Save the workflow to a JSON file (optional)\nworkflow_graph.save_module(\"/path/to/save/workflow_demo.json\")\n</code></pre> We provide an example generated workflow here. You can reload the saved workflow: <pre><code>workflow_graph = WorkFlowGraph.from_file(\"/path/to/save/workflow_demo.json\")\n</code></pre></p>"},{"location":"quickstart.html#step-2-create-and-manage-executable-agents","title":"Step 2: Create and Manage Executable Agents","text":"<p>Use <code>AgentManager</code> to instantiate and manage agents based on the workflow graph: <pre><code>agent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=openai_config)\n</code></pre></p>"},{"location":"quickstart.html#step-3-execute-the-workflow","title":"Step 3: Execute the Workflow","text":"<p>Once agents are ready, you can create a <code>WorkFlow</code> instance and run it: <pre><code>workflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\noutput = workflow.execute()\nprint(output)\n</code></pre></p> <p>For a complete working example, check out the full workflow demo.</p>"},{"location":"api/actions.html","title":"\ud83c\udfaf Actions","text":""},{"location":"api/actions.html#evoagentx.actions","title":"evoagentx.actions","text":""},{"location":"api/actions.html#evoagentx.actions.Action","title":"Action","text":"<pre><code>Action(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base class for all actions in the EvoAgentX framework.</p> <p>Actions represent discrete operations that can be performed by agents. They define inputs, outputs, and execution behavior, and can optionally use tools to accomplish their tasks.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for the action.</p> <code>description</code> <code>str</code> <p>Human-readable description of what the action does.</p> <code>prompt</code> <code>Optional[str]</code> <p>Optional prompt template for this action.</p> <code>tools</code> <code>Optional[List[Tool]]</code> <p>Optional list of tools that can be used by this action.</p> <code>inputs_format</code> <code>Optional[Type[ActionInput]]</code> <p>Optional class defining the expected input structure.</p> <code>outputs_format</code> <code>Optional[Type[Parser]]</code> <p>Optional class defining the expected output structure.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.Action.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Initialize the action module.</p> <p>This method is called after the action is instantiated. Subclasses can override this to perform custom initialization.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def init_module(self):\n    \"\"\"Initialize the action module.\n\n    This method is called after the action is instantiated.\n    Subclasses can override this to perform custom initialization.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.Action.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the action to a dictionary for saving.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the action to a dictionary for saving.  \n    \"\"\"\n    data = super().to_dict(exclude_none=exclude_none, ignore=ignore, **kwargs)\n    if self.inputs_format:\n        data[\"inputs_format\"] = self.inputs_format.__name__ \n    if self.outputs_format:\n        data[\"outputs_format\"] = self.outputs_format.__name__ \n    # TODO: customize serialization for the tools \n    return data \n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.Action.execute","title":"execute","text":"<pre><code>execute(llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str] = None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]\n</code></pre> <p>Execute the action to produce a result.</p> <p>This is the main entry point for executing an action. Subclasses must implement this method to define the action's behavior.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Optional[BaseLLM]</code> <p>The LLM used to execute the action.</p> <code>None</code> <code>inputs</code> <code>Optional[dict]</code> <p>Input data for the action execution. The input data should be a dictionary that matches the input format of the provided prompt.  For example, if the prompt contains a variable <code>{input_var}</code>, the <code>inputs</code> dictionary should have a key <code>input_var</code>, otherwise the variable will be set to empty string. </p> <code>None</code> <code>sys_msg</code> <code>Optional[str]</code> <p>Optional system message for the LLM.</p> <code>None</code> <code>return_prompt</code> <code>bool</code> <p>Whether to return the complete prompt passed to the LLM.</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for the execution.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Optional[Union[Parser, Tuple[Parser, str]]]</code> <p>If <code>return_prompt</code> is False, the method returns a Parser object containing the structured result of the action.</p> <code>Optional[Union[Parser, Tuple[Parser, str]]]</code> <p>If <code>return_prompt</code> is True, the method returns a tuple containing the Parser object and the complete prompt passed to the LLM.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]:\n    \"\"\"Execute the action to produce a result.\n\n    This is the main entry point for executing an action. Subclasses must\n    implement this method to define the action's behavior.\n\n    Args:\n        llm (Optional[BaseLLM]): The LLM used to execute the action.\n        inputs (Optional[dict]): Input data for the action execution. The input data should be a dictionary that matches the input format of the provided prompt. \n            For example, if the prompt contains a variable `{input_var}`, the `inputs` dictionary should have a key `input_var`, otherwise the variable will be set to empty string. \n        sys_msg (Optional[str]): Optional system message for the LLM.\n        return_prompt (bool): Whether to return the complete prompt passed to the LLM.\n        **kwargs (Any): Additional keyword arguments for the execution.\n\n    Returns:\n        If `return_prompt` is False, the method returns a Parser object containing the structured result of the action.\n        If `return_prompt` is True, the method returns a tuple containing the Parser object and the complete prompt passed to the LLM.\n    \"\"\"\n    raise NotImplementedError(f\"`execute` function of {type(self).__name__} is not implemented!\")\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.Action.async_execute","title":"async_execute  <code>async</code>","text":"<pre><code>async_execute(llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str] = None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]\n</code></pre> <p>Asynchronous execution of the action.</p> <p>This method is the asynchronous counterpart of the <code>execute</code> method. It allows the action to be executed asynchronously using an LLM.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>async def async_execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; Optional[Union[Parser, Tuple[Parser, str]]]:\n    \"\"\"\n    Asynchronous execution of the action.\n\n    This method is the asynchronous counterpart of the `execute` method.\n    It allows the action to be executed asynchronously using an LLM.\n    \"\"\"\n    raise NotImplementedError(f\"`async_execute` function of {type(self).__name__} is not implemented!\")\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.ActionInput","title":"ActionInput","text":"<pre><code>ActionInput(**kwargs)\n</code></pre> <p>               Bases: <code>LLMOutputParser</code></p> <p>Input specification and parsing for actions.</p> <p>This class defines the input requirements for actions and provides methods to generate structured input specifications. It inherits from LLMOutputParser  to allow parsing of LLM outputs into structured inputs for actions.</p> Notes <p>Parameters in ActionInput should be defined in Pydantic Field format. For optional variables, use format:  var: Optional[int] = Field(default=None, description=\"xxx\") Remember to add <code>default=None</code> for optional parameters.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.ActionInput.get_input_specification","title":"get_input_specification  <code>classmethod</code>","text":"<pre><code>get_input_specification(ignore_fields: List[str] = []) -&gt; str\n</code></pre> <p>Generate a JSON specification of the input requirements.</p> <p>Examines the class fields and produces a structured specification of the input parameters, including their types, descriptions, and whether they are required.</p> <p>Parameters:</p> Name Type Description Default <code>ignore_fields</code> <code>List[str]</code> <p>List of field names to exclude from the specification.</p> <code>[]</code> <p>Returns:</p> Type Description <code>str</code> <p>A JSON string containing the input specification, or an empty string</p> <code>str</code> <p>if no fields are defined or all are ignored.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>@classmethod\ndef get_input_specification(cls, ignore_fields: List[str] = []) -&gt; str:\n    \"\"\"Generate a JSON specification of the input requirements.\n\n    Examines the class fields and produces a structured specification of\n    the input parameters, including their types, descriptions, and whether\n    they are required.\n\n    Args:\n        ignore_fields (List[str]): List of field names to exclude from the specification.\n\n    Returns:\n        A JSON string containing the input specification, or an empty string\n        if no fields are defined or all are ignored.\n    \"\"\"\n    fields_info = {}\n    attrs = cls.get_attrs()\n    for field_name, field_info in cls.model_fields.items():\n        if field_name in ignore_fields:\n            continue\n        if field_name not in attrs:\n            continue\n        field_type = get_type_name(field_info.annotation)\n        field_desc = field_info.description if field_info.description is not None else None\n        # field_required = field_info.is_required()\n        field_default = str(field_info.default) if field_info.default is not PydanticUndefined else None\n        field_required = True if field_default is None else False\n        description = field_type + \", \"\n        if field_desc is not None:\n            description += (field_desc.strip() + \", \") \n        description += (\"required\" if field_required else \"optional\")\n        if field_default is not None:\n            description += (\", Default value: \" + field_default)\n        fields_info[field_name] = description\n\n    if len(fields_info) == 0:\n        return \"\" \n    fields_info_str = json.dumps(fields_info, indent=4)\n    return fields_info_str\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.ActionInput.get_required_input_names","title":"get_required_input_names  <code>classmethod</code>","text":"<pre><code>get_required_input_names() -&gt; List[str]\n</code></pre> <p>Get a list of all required input parameter names.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Names of all parameters that are required (don't have default values).</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>@classmethod\ndef get_required_input_names(cls) -&gt; List[str]:\n    \"\"\"Get a list of all required input parameter names.\n\n    Returns:\n        List[str]: Names of all parameters that are required (don't have default values).\n    \"\"\"\n    required_fields = []\n    attrs = cls.get_attrs()\n    for field_name, field_info in cls.model_fields.items():\n        if field_name not in attrs:\n            continue\n        field_default = field_info.default\n        # A field is required if it doesn't have a default value\n        if field_default is PydanticUndefined:\n            required_fields.append(field_name)\n    return required_fields\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.ActionOutput","title":"ActionOutput","text":"<pre><code>ActionOutput(**kwargs)\n</code></pre> <p>               Bases: <code>LLMOutputParser</code></p> <p>Output representation for actions.</p> <p>This class handles the structured output of actions, providing methods to convert the output to structured data. It inherits from LLMOutputParser to support parsing of LLM outputs into structured action results.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.ActionOutput.to_str","title":"to_str","text":"<pre><code>to_str() -&gt; str\n</code></pre> <p>Convert the output to a formatted JSON string.</p> <p>Returns:</p> Type Description <code>str</code> <p>A pretty-printed JSON string representation of the structured data.</p> Source code in <code>evoagentx/actions/action.py</code> <pre><code>def to_str(self) -&gt; str:\n    \"\"\"Convert the output to a formatted JSON string.\n\n    Returns:\n        A pretty-printed JSON string representation of the structured data.\n    \"\"\"\n    return json.dumps(self.get_structured_data(), indent=4)\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.CodeExtraction","title":"CodeExtraction","text":"<pre><code>CodeExtraction(**kwargs)\n</code></pre> <p>               Bases: <code>Action</code></p> <p>An action that extracts and organizes code blocks from text.</p> <p>This action uses an LLM to analyze text containing code blocks, extract them, suggest appropriate filenames, and save them to a specified directory. It can also identify which file is likely the main entry point based on heuristics.</p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the action.</p> <code>description</code> <code>str</code> <p>A description of what the action does.</p> <code>prompt</code> <code>Optional[str]</code> <p>The prompt template used by the action.</p> <code>inputs_format</code> <code>Optional[Type[ActionInput]]</code> <p>The expected format of inputs to this action.</p> <code>outputs_format</code> <code>Optional[Type[Parser]]</code> <p>The format of the action's output.</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def __init__(self, **kwargs):\n\n    name = kwargs.pop(\"name\") if \"name\" in kwargs else CODE_EXTRACTION[\"name\"]\n    description = kwargs.pop(\"description\") if \"description\" in kwargs else CODE_EXTRACTION[\"description\"]\n    prompt = kwargs.pop(\"prompt\") if \"prompt\" in kwargs else CODE_EXTRACTION[\"prompt\"]\n    # inputs_format = kwargs.pop(\"inputs_format\") if \"inputs_format\" in kwargs else CodeExtractionInput\n    # outputs_format = kwargs.pop(\"outputs_format\") if \"outputs_format\" in kwargs else CodeExtractionOutput\n    inputs_format = kwargs.pop(\"inputs_format\", None) or CodeExtractionInput\n    outputs_format = kwargs.pop(\"outputs_format\", None) or CodeExtractionOutput\n    super().__init__(name=name, description=description, prompt=prompt, inputs_format=inputs_format, outputs_format=outputs_format, **kwargs)\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.CodeExtraction.identify_main_file","title":"identify_main_file","text":"<pre><code>identify_main_file(saved_files: Dict[str, str]) -&gt; Optional[str]\n</code></pre> <p>Identify the main file from the saved files based on content and file type.</p> <p>This method uses a combination of common filename conventions and content analysis to determine which file is likely the main entry point of a project.</p> <p>Parameters:</p> Name Type Description Default <code>saved_files</code> <code>Dict[str, str]</code> <p>Dictionary mapping filenames to their full paths</p> required <p>Returns:</p> Type Description <code>Optional[str]</code> <p>Path to the main file if found, None otherwise</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def identify_main_file(self, saved_files: Dict[str, str]) -&gt; Optional[str]:\n    \"\"\"Identify the main file from the saved files based on content and file type.\n\n    This method uses a combination of common filename conventions and content\n    analysis to determine which file is likely the main entry point of a project.\n\n    Args:\n        saved_files: Dictionary mapping filenames to their full paths\n\n    Returns:\n        Path to the main file if found, None otherwise\n\n    \"\"\"\n    # Priority lookup for common main files by language\n    main_file_priorities = [\n        # HTML files\n        \"index.html\",\n        # Python files\n        \"main.py\", \n        \"app.py\",\n        # JavaScript files\n        \"index.js\",\n        \"main.js\",\n        \"app.js\",\n        # Java files\n        \"Main.java\",\n        # C/C++ files\n        \"main.cpp\", \n        \"main.c\",\n        # Go files\n        \"main.go\",\n        # Other common entry points\n        \"index.php\",\n        \"Program.cs\"\n    ]\n\n    # First check priority list\n    for main_file in main_file_priorities:\n        if main_file in saved_files:\n            return saved_files[main_file]\n\n    # If no priority file found, use heuristics based on file extensions\n\n    # If we have HTML files, use the first one\n    html_files = {k: v for k, v in saved_files.items() if k.endswith('.html')}\n    if html_files:\n        return next(iter(html_files.values()))\n\n    # Check for Python files with \"__main__\" section\n    py_files = {k: v for k, v in saved_files.items() if k.endswith('.py')}\n    if py_files:\n        for filename, path in py_files.items():\n            with open(path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                if \"if __name__ == '__main__'\" in content or 'if __name__ == \"__main__\"' in content:\n                    return path\n        # If no main found, return the first Python file\n        if py_files:\n            return next(iter(py_files.values()))\n\n    # If we have Java files, look for one with a main method\n    java_files = {k: v for k, v in saved_files.items() if k.endswith('.java')}\n    if java_files:\n        for filename, path in java_files.items():\n            with open(path, 'r', encoding='utf-8') as f:\n                content = f.read()\n                if \"public static void main\" in content:\n                    return path\n        # If no main found, return the first Java file\n        if java_files:\n            return next(iter(java_files.values()))\n\n    # For JavaScript applications\n    js_files = {k: v for k, v in saved_files.items() if k.endswith('.js')}\n    if js_files:\n        return next(iter(js_files.values()))\n\n    # If all else fails, return the first file\n    if saved_files:\n        return next(iter(saved_files.values()))\n\n    # No files found\n    return None\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.CodeExtraction.save_code_blocks","title":"save_code_blocks","text":"<pre><code>save_code_blocks(code_blocks: List[Dict], target_directory: str) -&gt; Dict[str, str]\n</code></pre> <p>Save code blocks to files in the target directory.</p> <p>Creates the target directory if it doesn't exist and saves each code block to a file with an appropriate name, handling filename conflicts.</p> <p>Parameters:</p> Name Type Description Default <code>code_blocks</code> <code>List[Dict]</code> <p>List of dictionaries containing code block information</p> required <code>target_directory</code> <code>str</code> <p>Directory path where files should be saved</p> required <p>Returns:</p> Type Description <code>Dict[str, str]</code> <p>Dictionary mapping filenames to their full paths</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def save_code_blocks(self, code_blocks: List[Dict], target_directory: str) -&gt; Dict[str, str]:\n    \"\"\"Save code blocks to files in the target directory.\n\n    Creates the target directory if it doesn't exist and saves each code block\n    to a file with an appropriate name, handling filename conflicts.\n\n    Args:\n        code_blocks: List of dictionaries containing code block information\n        target_directory: Directory path where files should be saved\n\n    Returns:\n        Dictionary mapping filenames to their full paths\n    \"\"\"\n    os.makedirs(target_directory, exist_ok=True)\n    saved_files = {}\n\n    for block in code_blocks:\n        filename = block.get(\"filename\", \"unknown.txt\")\n        content = block.get(\"content\", \"\")\n\n        # Skip empty blocks\n        if not content.strip():\n            continue\n\n        # Handle filename conflicts\n        base_filename = filename\n        counter = 1\n        while filename in saved_files:\n            name_parts = base_filename.split('.')\n            if len(name_parts) &gt; 1:\n                filename = f\"{'.'.join(name_parts[:-1])}_{counter}.{name_parts[-1]}\"\n            else:\n                filename = f\"{base_filename}_{counter}\"\n            counter += 1\n\n        # Save to file\n        file_path = os.path.join(target_directory, filename)\n        with open(file_path, 'w', encoding='utf-8') as f:\n            f.write(content)\n\n        # Add to map\n        saved_files[filename] = file_path\n\n    return saved_files\n</code></pre>"},{"location":"api/actions.html#evoagentx.actions.CodeExtraction.execute","title":"execute","text":"<pre><code>execute(llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str] = None, return_prompt: bool = False, **kwargs) -&gt; CodeExtractionOutput\n</code></pre> <p>Execute the CodeExtraction action.</p> <p>Extracts code blocks from the provided text using the specified LLM, saves them to the target directory, and identifies the main file.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>Optional[BaseLLM]</code> <p>The LLM to use for code extraction</p> <code>None</code> <code>inputs</code> <code>Optional[dict]</code> <p>Dictionary containing: - code_string: The string with code blocks to extract - target_directory: Where to save the files - project_name: Optional project folder name</p> <code>None</code> <code>sys_msg</code> <code>Optional[str]</code> <p>Optional system message override for the LLM</p> <code>None</code> <code>return_prompt</code> <code>bool</code> <p>Whether to return the prompt along with the result</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Type Description <code>CodeExtractionOutput</code> <p>CodeExtractionOutput with extracted file information</p> Source code in <code>evoagentx/actions/code_extraction.py</code> <pre><code>def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; CodeExtractionOutput:\n    \"\"\"Execute the CodeExtraction action.\n\n    Extracts code blocks from the provided text using the specified LLM,\n    saves them to the target directory, and identifies the main file.\n\n    Args:\n        llm: The LLM to use for code extraction\n        inputs: Dictionary containing:\n            - code_string: The string with code blocks to extract\n            - target_directory: Where to save the files\n            - project_name: Optional project folder name\n        sys_msg: Optional system message override for the LLM\n        return_prompt: Whether to return the prompt along with the result\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        CodeExtractionOutput with extracted file information\n    \"\"\"\n    if not llm:\n        error_msg = \"CodeExtraction action requires an LLM.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    if not inputs:\n        error_msg = \"CodeExtraction action received invalid `inputs`: None or empty.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    code_string = inputs.get(\"code_string\", \"\")\n    target_directory = inputs.get(\"target_directory\", \"\")\n    project_name = inputs.get(\"project_name\", None)\n\n    if not code_string:\n        error_msg = \"No code string provided.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    if not target_directory:\n        error_msg = \"No target directory provided.\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n\n    # Create project folder if name is provided\n    if project_name:\n        project_dir = os.path.join(target_directory, project_name)\n    else:\n        project_dir = target_directory\n\n    try:\n        # Use LLM to extract code blocks and suggest filenames\n        prompt_params = {\"code_string\": code_string}\n        system_message = CODE_EXTRACTION[\"system_prompt\"] if sys_msg is None else sys_msg\n\n        llm_response: CodeBlockList = llm.generate(\n            prompt=self.prompt.format(**prompt_params),\n            system_message=system_message,\n            parser=CodeBlockList,\n            parse_mode=\"json\"\n        )\n        code_blocks = llm_response.get_structured_data().get(\"code_blocks\", [])\n\n        # Save code blocks to files\n        saved_files = self.save_code_blocks(code_blocks, project_dir)\n\n        # Identify main file\n        main_file = self.identify_main_file(saved_files)\n\n        result = CodeExtractionOutput(\n            extracted_files=saved_files,\n            main_file=main_file\n        )\n\n        if return_prompt:\n            return result, self.prompt.format(**prompt_params)\n\n        return result\n\n    except Exception as e:\n        error_msg = f\"Error extracting code: {str(e)}\"\n        return CodeExtractionOutput(extracted_files={}, error=error_msg)\n</code></pre>"},{"location":"api/agents.html","title":"\ud83e\udd16 Agents","text":""},{"location":"api/agents.html#evoagentx.agents","title":"evoagentx.agents","text":""},{"location":"api/agents.html#evoagentx.agents.Agent","title":"Agent","text":"<pre><code>Agent(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base class for all agents. </p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>Unique identifier for the agent</p> <code>description</code> <code>str</code> <p>Human-readable description of the agent's purpose</p> <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>Configuration for the language model. If provided, a new LLM instance will be created.  Otherwise, the existing LLM instance specified in the <code>llm</code> field will be used.   </p> <code>llm</code> <code>Optional[BaseLLM]</code> <p>Language model instance. If provided, the existing LLM instance will be used. </p> <code>agent_id</code> <code>Optional[str]</code> <p>Unique ID for the agent, auto-generated if not provided</p> <code>system_prompt</code> <code>Optional[str]</code> <p>System prompt for the Agent.</p> <code>actions</code> <code>List[Action]</code> <p>List of available actions</p> <code>n</code> <code>Optional[int]</code> <p>Number of latest messages used to provide context for action execution. It uses all the messages in short term memory by default. </p> <code>is_human</code> <code>bool</code> <p>Whether this agent represents a human user</p> <code>version</code> <code>int</code> <p>Version number of the agent, default is 0.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.__call__","title":"__call__","text":"<pre><code>__call__(*args: Any, **kwargs: Any) -&gt; Union[dict, Coroutine[Any, Any, dict]]\n</code></pre> <p>Make the operator callable and automatically choose between sync and async execution.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def __call__(self, *args: Any, **kwargs: Any) -&gt; Union[dict, Coroutine[Any, Any, dict]]:\n    \"\"\"Make the operator callable and automatically choose between sync and async execution.\"\"\"\n    try:\n        # Safe way to check if we're inside an async environment\n        asyncio.get_running_loop()\n        return self.async_execute(*args, **kwargs)\n    except RuntimeError:\n        # No running loop \u2014 likely in sync context or worker thread\n        return self.execute(*args, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.async_execute","title":"async_execute  <code>async</code>","text":"<pre><code>async_execute(action_name: str, msgs: Optional[List[Message]] = None, action_input_data: Optional[dict] = None, return_msg_type: Optional[MessageType] = UNKNOWN, return_action_input_data: Optional[bool] = False, **kwargs) -&gt; Union[Message, Tuple[Message, dict]]\n</code></pre> <p>Execute an action asynchronously with the given context and return results.</p> <p>This is the async version of the execute method, allowing it to perform actions based on the current conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>The name of the action to execute</p> required <code>msgs</code> <code>Optional[List[Message]]</code> <p>Optional list of messages providing context for the action</p> <code>None</code> <code>action_input_data</code> <code>Optional[dict]</code> <p>Optional pre-extracted input data for the action</p> <code>None</code> <code>return_msg_type</code> <code>Optional[MessageType]</code> <p>Message type for the return message</p> <code>UNKNOWN</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters, may include workflow information</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Union[Message, Tuple[Message, dict]]</code> <p>A message containing the execution results</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>async def async_execute(\n    self, \n    action_name: str, \n    msgs: Optional[List[Message]] = None, \n    action_input_data: Optional[dict] = None, \n    return_msg_type: Optional[MessageType] = MessageType.UNKNOWN,\n    return_action_input_data: Optional[bool] = False, \n    **kwargs\n) -&gt; Union[Message, Tuple[Message, dict]]:\n    \"\"\"Execute an action asynchronously with the given context and return results.\n\n    This is the async version of the execute method, allowing it to perform actions\n    based on the current conversation context.\n\n    Args:\n        action_name: The name of the action to execute\n        msgs: Optional list of messages providing context for the action\n        action_input_data: Optional pre-extracted input data for the action\n        return_msg_type: Message type for the return message\n        **kwargs (Any): Additional parameters, may include workflow information\n\n    Returns:\n        Message: A message containing the execution results\n    \"\"\"\n    action, action_input_data = self._prepare_execution(\n        action_name=action_name,\n        msgs=msgs,\n        action_input_data=action_input_data,\n        **kwargs\n    )\n\n    # execute action asynchronously\n    async_execute_source = inspect.getsource(action.async_execute)\n    if \"NotImplementedError\" in async_execute_source:\n        # if the async_execute method is not implemented, use the execute method instead\n        execution_results = action.execute(\n            llm=self.llm, \n            inputs=action_input_data, \n            sys_msg=self.system_prompt,\n            return_prompt=True,\n            **kwargs\n        )\n    else:\n        execution_results = await action.async_execute(\n            llm=self.llm, \n            inputs=action_input_data, \n            sys_msg=self.system_prompt,\n            return_prompt=True,\n            **kwargs\n    )\n    action_output, prompt = execution_results\n\n    message = self._create_output_message(\n        action_output=action_output,\n        prompt=prompt,\n        action_name=action_name,\n        return_msg_type=return_msg_type,\n        **kwargs\n    )\n    if return_action_input_data:\n        return message, action_input_data\n    return message\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.execute","title":"execute","text":"<pre><code>execute(action_name: str, msgs: Optional[List[Message]] = None, action_input_data: Optional[dict] = None, return_msg_type: Optional[MessageType] = UNKNOWN, return_action_input_data: Optional[bool] = False, **kwargs) -&gt; Union[Message, Tuple[Message, dict]]\n</code></pre> <p>Execute an action with the given context and return results.</p> <p>This is the core method for agent functionality, allowing it to perform actions based on the current conversation context.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>The name of the action to execute</p> required <code>msgs</code> <code>Optional[List[Message]]</code> <p>Optional list of messages providing context for the action</p> <code>None</code> <code>action_input_data</code> <code>Optional[dict]</code> <p>Optional pre-extracted input data for the action</p> <code>None</code> <code>return_msg_type</code> <code>Optional[MessageType]</code> <p>Message type for the return message</p> <code>UNKNOWN</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters, may include workflow information</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Message</code> <code>Union[Message, Tuple[Message, dict]]</code> <p>A message containing the execution results</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def execute(\n    self, \n    action_name: str, \n    msgs: Optional[List[Message]] = None, \n    action_input_data: Optional[dict] = None, \n    return_msg_type: Optional[MessageType] = MessageType.UNKNOWN,\n    return_action_input_data: Optional[bool] = False, \n    **kwargs\n) -&gt; Union[Message, Tuple[Message, dict]]:\n    \"\"\"Execute an action with the given context and return results.\n\n    This is the core method for agent functionality, allowing it to perform actions\n    based on the current conversation context.\n\n    Args:\n        action_name: The name of the action to execute\n        msgs: Optional list of messages providing context for the action\n        action_input_data: Optional pre-extracted input data for the action\n        return_msg_type: Message type for the return message\n        **kwargs (Any): Additional parameters, may include workflow information\n\n    Returns:\n        Message: A message containing the execution results\n    \"\"\"\n    action, action_input_data = self._prepare_execution(\n        action_name=action_name,\n        msgs=msgs,\n        action_input_data=action_input_data,\n        **kwargs\n    )\n\n    # execute action\n    execution_results = action.execute(\n        llm=self.llm, \n        inputs=action_input_data, \n        sys_msg=self.system_prompt,\n        return_prompt=True,\n        **kwargs\n    )\n    action_output, prompt = execution_results\n\n    message = self._create_output_message(\n        action_output=action_output,\n        prompt=prompt,\n        action_name=action_name,\n        return_msg_type=return_msg_type,\n        **kwargs\n    )\n    if return_action_input_data:\n        return message, action_input_data\n    return message\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.init_llm","title":"init_llm","text":"<pre><code>init_llm()\n</code></pre> <p>Initialize the language model for the agent.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def init_llm(self):\n    \"\"\"\n    Initialize the language model for the agent.\n    \"\"\"\n    assert self.llm_config or self.llm, \"must provide either 'llm_config' or 'llm' when is_human=False\"\n    if self.llm_config and not self.llm:\n        llm_cls = MODEL_REGISTRY.get_model(self.llm_config.llm_type)\n        self.llm = llm_cls(config=self.llm_config)\n    if self.llm:\n        self.llm_config = self.llm.config\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.init_long_term_memory","title":"init_long_term_memory","text":"<pre><code>init_long_term_memory()\n</code></pre> <p>Initialize long-term memory components.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def init_long_term_memory(self):\n    \"\"\"\n    Initialize long-term memory components.\n    \"\"\"\n    assert self.storage_handler is not None, \"must provide ``storage_handler`` when use_long_term_memory=True\"\n    # TODO revise the initialisation of long_term_memory and long_term_memory_manager\n    if not self.long_term_memory:\n        self.long_term_memory = LongTermMemory()\n    if not self.long_term_memory_manager:\n        self.long_term_memory_manager = MemoryManager(\n            storage_handler=self.storage_handler,\n            memory=self.long_term_memory\n        )\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.init_context_extractor","title":"init_context_extractor","text":"<pre><code>init_context_extractor()\n</code></pre> <p>Initialize the context extraction action.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def init_context_extractor(self):\n    \"\"\"\n    Initialize the context extraction action.\n    \"\"\"\n    cext_action = ContextExtraction()\n    self.cext_action_name = cext_action.name\n    self.add_action(cext_action)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.add_action","title":"add_action","text":"<pre><code>add_action(action: Type[Action])\n</code></pre> <p>Add a new action to the agent's available actions.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>Type[Action]</code> <p>The action instance to add</p> required Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def add_action(self, action: Type[Action]):\n    \"\"\"\n    Add a new action to the agent's available actions.\n\n    Args:\n        action: The action instance to add\n    \"\"\"\n    action_name  = action.name\n    if action_name in self._action_map:\n        return\n    self.actions.append(action)\n    self._action_map[action_name] = action\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.check_action_name","title":"check_action_name","text":"<pre><code>check_action_name(action_name: str)\n</code></pre> <p>Check if an action name is valid for this agent.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action to check</p> required Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def check_action_name(self, action_name: str):\n    \"\"\"\n    Check if an action name is valid for this agent.\n\n    Args:\n        action_name: Name of the action to check\n    \"\"\"\n    if action_name not in self._action_map:\n        raise KeyError(f\"'{action_name}' is an invalid action for {self.name}! Available action names: {list(self._action_map.keys())}\")\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_action","title":"get_action","text":"<pre><code>get_action(action_name: str) -&gt; Action\n</code></pre> <p>Retrieves the Action instance associated with the given name.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action to retrieve</p> required <p>Returns:</p> Type Description <code>Action</code> <p>The Action instance with the specified name</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_action(self, action_name: str) -&gt; Action:\n    \"\"\"\n    Retrieves the Action instance associated with the given name.\n\n    Args:\n        action_name: Name of the action to retrieve\n\n    Returns:\n        The Action instance with the specified name\n    \"\"\"\n    self.check_action_name(action_name=action_name)\n    return self._action_map[action_name]\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_action_name","title":"get_action_name","text":"<pre><code>get_action_name(action_cls: Type[Action]) -&gt; str\n</code></pre> <p>Searches through the agent's actions to find one matching the specified type.</p> <p>Parameters:</p> Name Type Description Default <code>action_cls</code> <code>Type[Action]</code> <p>The Action class type to search for</p> required <p>Returns:</p> Type Description <code>str</code> <p>The name of the matching action</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_action_name(self, action_cls: Type[Action]) -&gt; str:\n    \"\"\"\n    Searches through the agent's actions to find one matching the specified type.\n\n    Args:\n        action_cls: The Action class type to search for\n\n    Returns:\n        The name of the matching action\n    \"\"\"\n    for name, action in self._action_map.items():\n        if isinstance(action, action_cls):\n            return name\n    raise ValueError(f\"Couldn't find an action that matches Type '{action_cls.__name__}'\")\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_action_inputs","title":"get_action_inputs","text":"<pre><code>get_action_inputs(action: Action) -&gt; Union[dict, None]\n</code></pre> <p>Uses the context extraction action to determine appropriate inputs for the specified action based on the conversation history.</p> <p>Parameters:</p> Name Type Description Default <code>action</code> <code>Action</code> <p>The action for which to extract inputs</p> required <p>Returns:</p> Type Description <code>Union[dict, None]</code> <p>Dictionary of extracted input data, or None if extraction fails</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_action_inputs(self, action: Action) -&gt; Union[dict, None]:\n    \"\"\"\n    Uses the context extraction action to determine appropriate inputs\n    for the specified action based on the conversation history.\n\n    Args:\n        action: The action for which to extract inputs\n\n    Returns:\n        Dictionary of extracted input data, or None if extraction fails\n    \"\"\"\n    # return the input data of an action.\n    context = self.short_term_memory.get(n=self.n)\n    cext_action = self.get_action(self.cext_action_name)\n    action_inputs = cext_action.execute(llm=self.llm, action=action, context=context)\n    return action_inputs\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_all_actions","title":"get_all_actions","text":"<pre><code>get_all_actions() -&gt; List[Action]\n</code></pre> <p>Get all actions except the context extraction action.</p> <p>Returns:</p> Type Description <code>List[Action]</code> <p>List of Action instances available for execution</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_all_actions(self) -&gt; List[Action]:\n    \"\"\"Get all actions except the context extraction action.\n\n    Returns:\n        List of Action instances available for execution\n    \"\"\"\n    actions = [action for action in self.actions if action.name != self.cext_action_name]\n    return actions\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_agent_profile","title":"get_agent_profile","text":"<pre><code>get_agent_profile(action_names: List[str] = None) -&gt; str\n</code></pre> <p>Generate a human-readable profile of the agent and its capabilities.</p> <p>Parameters:</p> Name Type Description Default <code>action_names</code> <code>List[str]</code> <p>Optional list of action names to include in the profile.           If None, all actions are included.</p> <code>None</code> <p>Returns:</p> Type Description <code>str</code> <p>A formatted string containing the agent profile</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_agent_profile(self, action_names: List[str] = None) -&gt; str:\n    \"\"\"Generate a human-readable profile of the agent and its capabilities.\n\n    Args:\n        action_names: Optional list of action names to include in the profile.\n                      If None, all actions are included.\n\n    Returns:\n        A formatted string containing the agent profile\n    \"\"\"\n    all_actions = self.get_all_actions()\n    if action_names is None:\n        # if `action_names` is None, return description of all actions \n        action_descriptions = \"\\n\".join([f\"  - {action.name}: {action.description}\" for action in all_actions])\n    else: \n        # otherwise, only return description of actions that matches `action_names`\n        action_descriptions = \"\\n\".join([f\"  - {action.name}: {action.description}\" for action in all_actions if action.name in action_names])\n    profile = f\"Agent Name: {self.name}\\nDescription: {self.description}\\nAvailable Actions:\\n{action_descriptions}\"\n    return profile\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.clear_short_term_memory","title":"clear_short_term_memory","text":"<pre><code>clear_short_term_memory()\n</code></pre> <p>Remove all content from the agent's short-term memory.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def clear_short_term_memory(self):\n    \"\"\"\n    Remove all content from the agent's short-term memory.\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_prompts","title":"get_prompts","text":"<pre><code>get_prompts() -&gt; dict\n</code></pre> <p>Get all the prompts of the agent.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A dictionary with keys in the format 'agent_name::action_name' and values containing the system_prompt and action prompt.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_prompts(self) -&gt; dict:\n    \"\"\"\n    Get all the prompts of the agent.\n\n    Returns:\n        dict: A dictionary with keys in the format 'agent_name::action_name' and values\n            containing the system_prompt and action prompt.\n    \"\"\"\n    prompts = {}\n    for action in self.get_all_actions():\n        prompts[action.name] = {\n            \"system_prompt\": self.system_prompt, \n            \"prompt\": action.prompt\n        }\n    return prompts\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.set_prompt","title":"set_prompt","text":"<pre><code>set_prompt(action_name: str, prompt: str, system_prompt: Optional[str] = None) -&gt; bool\n</code></pre> <p>Set the prompt for a specific action of this agent.</p> <p>Parameters:</p> Name Type Description Default <code>action_name</code> <code>str</code> <p>Name of the action whose prompt should be updated</p> required <code>prompt</code> <code>str</code> <p>New prompt text to set for the action</p> required <code>system_prompt</code> <code>Optional[str]</code> <p>Optional new system prompt to set for the agent</p> <code>None</code> <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the prompt was successfully updated, False otherwise</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If the action_name does not exist for this agent</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def set_prompt(self, action_name: str, prompt: str, system_prompt: Optional[str] = None) -&gt; bool:\n    \"\"\"\n    Set the prompt for a specific action of this agent.\n\n    Args:\n        action_name: Name of the action whose prompt should be updated\n        prompt: New prompt text to set for the action\n        system_prompt: Optional new system prompt to set for the agent\n\n    Returns:\n        bool: True if the prompt was successfully updated, False otherwise\n\n    Raises:\n        KeyError: If the action_name does not exist for this agent\n    \"\"\"\n    try:\n        action = self.get_action(action_name)\n        action.prompt = prompt\n\n        if system_prompt is not None:\n            self.system_prompt = system_prompt\n\n        return True\n    except KeyError:\n        raise KeyError(f\"Action '{action_name}' not found in agent '{self.name}'\")\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.set_prompts","title":"set_prompts","text":"<pre><code>set_prompts(prompts: dict) -&gt; bool\n</code></pre> <p>Set the prompts for all actions of this agent.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>dict</code> <p>A dictionary with keys in the format 'action_name' and values containing the system_prompt and action prompt.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the prompts were successfully updated, False otherwise</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def set_prompts(self, prompts: dict) -&gt; bool:\n    \"\"\"\n    Set the prompts for all actions of this agent.\n\n    Args:\n        prompts: A dictionary with keys in the format 'action_name' and values\n            containing the system_prompt and action prompt.\n\n    Returns:\n        bool: True if the prompts were successfully updated, False otherwise\n    \"\"\"\n    for action_name, prompt_data in prompts.items():\n        # self.set_prompt(action_name, prompt_data[\"prompt\"], prompt_data[\"system_prompt\"])\n        if not isinstance(prompt_data, dict):\n            raise ValueError(f\"Invalid prompt data for action '{action_name}'. Expected a dictionary with 'prompt' and 'system_prompt' (optional) keys.\")\n        if \"prompt\" not in prompt_data:\n            raise ValueError(f\"Missing 'prompt' key in prompt data for action '{action_name}'.\")\n        self.set_prompt(action_name, prompt_data[\"prompt\"], prompt_data.get(\"system_prompt\", None))\n    return True\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the agent to persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>Path where the agent should be saved</p> required <code>ignore</code> <code>List[str]</code> <p>List of field names to exclude from serialization</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for the save operation</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path where the agent was saved</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"Save the agent to persistent storage.\n\n    Args:\n        path: Path where the agent should be saved\n        ignore: List of field names to exclude from serialization\n        **kwargs (Any): Additional parameters for the save operation\n\n    Returns:\n        The path where the agent was saved\n    \"\"\"\n    ignore_fields = self._save_ignore_fields + ignore\n    super().save_module(path=path, ignore=ignore_fields, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, llm_config: LLMConfig = None, **kwargs) -&gt; Agent\n</code></pre> <p>load the agent from local storage. Must provide <code>llm_config</code> when loading the agent from local storage. </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>llm_config</code> <code>LLMConfig</code> <p>The LLMConfig instance</p> <code>None</code> <p>Returns:</p> Name Type Description <code>Agent</code> <code>Agent</code> <p>The loaded agent instance</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>@classmethod\ndef load_module(cls, path: str, llm_config: LLMConfig = None, **kwargs) -&gt; \"Agent\":\n    \"\"\"\n    load the agent from local storage. Must provide `llm_config` when loading the agent from local storage. \n\n    Args:\n        path: The path of the file\n        llm_config: The LLMConfig instance\n\n    Returns:\n        Agent: The loaded agent instance\n    \"\"\"\n    assert llm_config is not None, \"must provide `llm_config` when using `load_module` or `from_file` to load the agent from local storage\"\n    agent = super().load_module(path=path, **kwargs)\n    agent[\"llm_config\"] = llm_config.to_dict()\n    return agent \n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.Agent.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; dict\n</code></pre> <p>Get a dictionary containing all necessary configuration to recreate this agent.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A configuration dictionary that can be used to initialize a new Agent instance</p> <code>dict</code> <p>with the same properties as this one.</p> Source code in <code>evoagentx/agents/agent.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"\n    Get a dictionary containing all necessary configuration to recreate this agent.\n\n    Returns:\n        dict: A configuration dictionary that can be used to initialize a new Agent instance\n        with the same properties as this one.\n    \"\"\"\n    config = self.to_dict()\n    return config\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent","title":"CustomizeAgent","text":"<pre><code>CustomizeAgent(name: str, description: str, prompt: Optional[str] = None, prompt_template: Optional[PromptTemplate] = None, llm_config: Optional[LLMConfig] = None, inputs: Optional[List[dict]] = None, outputs: Optional[List[dict]] = None, system_prompt: Optional[str] = None, output_parser: Optional[Type[ActionOutput]] = None, parse_mode: Optional[str] = 'title', parse_func: Optional[Callable] = None, title_format: Optional[str] = None, custom_output_format: Optional[str] = None, **kwargs)\n</code></pre> <p>               Bases: <code>Agent</code></p> <p>CustomizeAgent provides a flexible framework for creating specialized LLM-powered agents without  writing custom code. It enables the creation of agents with well-defined inputs and outputs,  custom prompt templates, and configurable parsing strategies. </p> <p>Attributes:</p> Name Type Description <code>name</code> <code>str</code> <p>The name of the agent.</p> <code>description</code> <code>str</code> <p>A description of the agent's purpose and capabilities.</p> <code>prompt_template</code> <code>PromptTemplate</code> <p>The prompt template that will be used for the agent's primary action. </p> <code>prompt</code> <code>str</code> <p>The prompt template that will be used for the agent's primary action. Should contain placeholders in the format <code>{input_name}</code> for each input parameter.</p> <code>llm_config</code> <code>LLMConfig</code> <p>Configuration for the language model.</p> <code>inputs</code> <code>List[dict]</code> <p>List of input specifications, where each dict (e.g., <code>{\"name\": str, \"type\": str, \"description\": str, [\"required\": bool]}</code>) contains: - name (str): Name of the input parameter - type (str): Type of the input - description (str): Description of what the input represents - required (bool, optional): Whether this input is required (default: True)</p> <code>outputs</code> <code>List[dict]</code> <p>List of output specifications, where each dict (e.g., <code>{\"name\": str, \"type\": str, \"description\": str, [\"required\": bool]}</code>) contains: - name (str): Name of the output field - type (str): Type of the output - description (str): Description of what the output represents - required (bool, optional): Whether this output is required (default: True)</p> <code>system_prompt</code> <code>str</code> <p>The system prompt for the LLM. Defaults to DEFAULT_SYSTEM_PROMPT.</p> <code>output_parser</code> <code>Type[ActionOutput]</code> <p>A custom class for parsing the LLM's output. Must be a subclass of ActionOutput.</p> <code>parse_mode</code> <code>str</code> <p>Mode for parsing LLM output. Options are: - \"title\": Parse outputs using section titles (default) - \"str\": Parse as plain text - \"json\": Parse as JSON - \"xml\": Parse as XML - \"custom\": Use a custom parsing function</p> <code>parse_func</code> <code>Callable</code> <p>Custom function for parsing LLM output when parse_mode is \"custom\". Must accept a \"content\" parameter and return a dictionary.</p> <code>title_format</code> <code>str</code> <p>Format string for title parsing mode with {title} placeholder. Default is \"## {title}\".</p> <code>custom_output_format</code> <code>str</code> <p>Specify the output format. Only used when <code>prompt_template</code> is used.  If not provided, the output format will be constructed from the <code>outputs</code> specification and <code>parse_mode</code>.</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def __init__(\n    self, \n    name: str, \n    description: str, \n    prompt: Optional[str] = None, \n    prompt_template: Optional[PromptTemplate] = None, \n    llm_config: Optional[LLMConfig] = None, \n    inputs: Optional[List[dict]] = None, \n    outputs: Optional[List[dict]] = None, \n    system_prompt: Optional[str] = None,\n    output_parser: Optional[Type[ActionOutput]] = None, \n    parse_mode: Optional[str] = \"title\", \n    parse_func: Optional[Callable] = None, \n    title_format: Optional[str] = None, \n    custom_output_format: Optional[str] = None, \n    **kwargs\n):\n    system_prompt = system_prompt or DEFAULT_SYSTEM_PROMPT\n    inputs = inputs or [] \n    outputs = outputs or [] \n\n    if prompt is not None and prompt_template is not None:\n        logger.warning(\"Both `prompt` and `prompt_template` are provided in `CustomizeAgent`. `prompt_template` will be used.\")\n        prompt = None \n\n    if isinstance(parse_func, str):\n        if not PARSE_FUNCTION_REGISTRY.has_function(parse_func):\n            raise ValueError(f\"parse function `{parse_func}` is not registered! To instantiate a CustomizeAgent from a file, you should use decorator `@register_parse_function` to register the parse function.\")\n        parse_func = PARSE_FUNCTION_REGISTRY.get_function(parse_func)\n\n    if isinstance(output_parser, str):\n        output_parser = MODULE_REGISTRY.get_module(output_parser)\n\n    # set default title format \n    if parse_mode == \"title\" and title_format is None:\n        title_format = \"## {title}\"\n\n    # validate the data \n    self.validate_data(\n        prompt = prompt, \n        prompt_template = prompt_template, \n        inputs = inputs, \n        outputs = outputs, \n        output_parser = output_parser, \n        parse_mode = parse_mode, \n        parse_func = parse_func, \n        title_format = title_format\n    )\n\n    customize_action = self.create_customize_action(\n        name=name, \n        desc=description, \n        prompt=prompt, \n        prompt_template=prompt_template, \n        inputs=inputs, \n        outputs=outputs, \n        parse_mode=parse_mode, \n        parse_func=parse_func,\n        output_parser=output_parser,\n        title_format=title_format,\n        custom_output_format=custom_output_format \n    )\n    super().__init__(\n        name=name, \n        description=description, \n        llm_config=llm_config, \n        system_prompt=system_prompt, \n        actions=[customize_action], \n        **kwargs\n    )\n    self._store_inputs_outputs_info(inputs, outputs)\n    self.output_parser = output_parser \n    self.parse_mode = parse_mode \n    self.parse_func = parse_func \n    self.title_format = title_format\n    self.custom_output_format = custom_output_format\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.customize_action_name","title":"customize_action_name  <code>property</code>","text":"<pre><code>customize_action_name: str\n</code></pre> <p>Get the name of the primary custom action for this agent.</p> <p>Returns:</p> Type Description <code>str</code> <p>The name of the primary custom action</p>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.action","title":"action  <code>property</code>","text":"<pre><code>action: Action\n</code></pre> <p>Get the primary custom action for this agent.</p> <p>Returns:</p> Type Description <code>Action</code> <p>The primary custom action</p>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.prompt","title":"prompt  <code>property</code>","text":"<pre><code>prompt: str\n</code></pre> <p>Get the prompt for the primary custom action.</p> <p>Returns:</p> Type Description <code>str</code> <p>The prompt for the primary custom action</p>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.prompt_template","title":"prompt_template  <code>property</code>","text":"<pre><code>prompt_template: PromptTemplate\n</code></pre> <p>Get the prompt template for the primary custom action.</p> <p>Returns:</p> Type Description <code>PromptTemplate</code> <p>The prompt template for the primary custom action</p>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.create_customize_action","title":"create_customize_action","text":"<pre><code>create_customize_action(name: str, desc: str, prompt: str, prompt_template: PromptTemplate, inputs: List[dict], outputs: List[dict], parse_mode: str, parse_func: Optional[Callable] = None, output_parser: Optional[ActionOutput] = None, title_format: Optional[str] = '## {title}', custom_output_format: Optional[str] = None) -&gt; Action\n</code></pre> <p>Create a custom action based on the provided specifications.</p> <p>This method dynamically generates an Action class and instance with: - Input parameters defined by the inputs specification - Output format defined by the outputs specification - Custom execution logic using the customize_action_execute function</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Base name for the action</p> required <code>desc</code> <code>str</code> <p>Description of the action</p> required <code>prompt</code> <code>str</code> <p>Prompt template for the action</p> required <code>prompt_template</code> <code>PromptTemplate</code> <p>Prompt template for the action</p> required <code>inputs</code> <code>List[dict]</code> <p>List of input field specifications</p> required <code>outputs</code> <code>List[dict]</code> <p>List of output field specifications</p> required <code>parse_mode</code> <code>str</code> <p>Mode to use for parsing LLM output</p> required <code>parse_func</code> <code>Optional[Callable]</code> <p>Optional custom parsing function</p> <code>None</code> <code>output_parser</code> <code>Optional[ActionOutput]</code> <p>Optional custom output parser class</p> <code>None</code> <p>Returns:</p> Type Description <code>Action</code> <p>A newly created Action instance</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def create_customize_action(\n    self, \n    name: str, \n    desc: str, \n    prompt: str, \n    prompt_template: PromptTemplate, \n    inputs: List[dict], \n    outputs: List[dict], \n    parse_mode: str, \n    parse_func: Optional[Callable] = None,\n    output_parser: Optional[ActionOutput] = None,\n    title_format: Optional[str] = \"## {title}\",\n    custom_output_format: Optional[str] = None\n) -&gt; Action:\n    \"\"\"Create a custom action based on the provided specifications.\n\n    This method dynamically generates an Action class and instance with:\n    - Input parameters defined by the inputs specification\n    - Output format defined by the outputs specification\n    - Custom execution logic using the customize_action_execute function\n\n    Args:\n        name: Base name for the action\n        desc: Description of the action\n        prompt: Prompt template for the action\n        prompt_template: Prompt template for the action\n        inputs: List of input field specifications\n        outputs: List of output field specifications\n        parse_mode: Mode to use for parsing LLM output\n        parse_func: Optional custom parsing function\n        output_parser: Optional custom output parser class\n\n    Returns:\n        A newly created Action instance\n    \"\"\"\n    assert prompt is not None or prompt_template is not None, \"must provide `prompt` or `prompt_template` when creating CustomizeAgent\"\n\n    # create the action input type\n    action_input_fields = {}\n    for field in inputs:\n        required = field.get(\"required\", True)\n        if required:\n            action_input_fields[field[\"name\"]] = (str, Field(description=field[\"description\"]))\n        else:\n            action_input_fields[field[\"name\"]] = (Optional[str], Field(default=None, description=field[\"description\"]))\n\n    action_input_type = create_model(\n        self._get_unique_class_name(\n            generate_dynamic_class_name(name+\" action_input\")\n        ),\n        **action_input_fields, \n        __base__=ActionInput\n    )\n\n    # create the action output type\n    if output_parser is None:\n        action_output_fields = {}\n        for field in outputs:\n            required = field.get(\"required\", True)\n            if required:\n                action_output_fields[field[\"name\"]] = (Union[str, dict, list], Field(description=field[\"description\"]))\n            else:\n                action_output_fields[field[\"name\"]] = (Optional[Union[str, dict, list]], Field(default=None, description=field[\"description\"]))\n\n        action_output_type = create_model(\n            self._get_unique_class_name(\n                generate_dynamic_class_name(name+\" action_output\")\n            ),\n            **action_output_fields, \n            __base__=ActionOutput,\n            # get_content_data=customize_get_content_data,\n            # to_str=customize_to_str\n        )\n    else:\n        # self._check_output_parser(outputs, output_parser)\n        action_output_type = output_parser\n\n    action_cls_name = self._get_unique_class_name(\n        generate_dynamic_class_name(name+\" action\")\n    )\n    customize_action_cls = create_model(\n        action_cls_name,\n        parse_mode=(Optional[str], Field(default=\"title\", description=\"the parse mode of the action, must be one of: ['title', 'str', 'json', 'xml', 'custom']\")),\n        parse_func=(Optional[Callable], Field(default=None, exclude=True, description=\"the function to parse the LLM output. It receives the LLM output and returns a dict.\")),\n        title_format=(Optional[str], Field(default=\"## {title}\", exclude=True, description=\"the format of the title. It is used when the `parse_mode` is 'title'.\")),\n        custom_output_format=(Optional[str], Field(default=None, exclude=True, description=\"the format of the output. It is used when the `prompt_template` is provided.\")),\n        __base__=Action, \n        execute=customize_action_execute,\n        async_execute=customize_action_async_execute,\n        prepare_action_prompt=prepare_action_prompt,\n        prepare_extraction_prompt=prepare_extraction_prompt\n    )\n\n    customize_action = customize_action_cls(\n        name = action_cls_name,\n        description=desc, \n        prompt=prompt, \n        prompt_template=prompt_template, \n        inputs_format=action_input_type, \n        outputs_format=action_output_type,\n        parse_mode=parse_mode,\n        parse_func=parse_func,\n        title_format=title_format\n    )\n    return customize_action\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.__call__","title":"__call__","text":"<pre><code>__call__(inputs: dict = None, return_msg_type: MessageType = UNKNOWN, **kwargs) -&gt; Message\n</code></pre> <p>Call the customize action.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>The inputs to the customize action.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>ActionOutput</code> <code>Message</code> <p>The output of the customize action.</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def __call__(self, inputs: dict = None, return_msg_type: MessageType = MessageType.UNKNOWN, **kwargs) -&gt; Message:\n    \"\"\"\n    Call the customize action.\n\n    Args:\n        inputs (dict): The inputs to the customize action.\n        **kwargs (Any): Additional keyword arguments.\n\n    Returns:\n        ActionOutput: The output of the customize action.\n    \"\"\"\n    # return self.execute(action_name=self.customize_action_name, action_input_data=inputs, **kwargs) \n    inputs = inputs or {} \n    return super().__call__(action_name=self.customize_action_name, action_input_data=inputs, return_msg_type=return_msg_type, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.get_customize_agent_info","title":"get_customize_agent_info","text":"<pre><code>get_customize_agent_info() -&gt; dict\n</code></pre> <p>Get the information of the customize agent.</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def get_customize_agent_info(self) -&gt; dict:\n    \"\"\"\n    Get the information of the customize agent.\n    \"\"\"\n    customize_action = self.get_action(self.customize_action_name)\n    action_input_params = customize_action.inputs_format.get_attrs()\n    action_output_params = customize_action.outputs_format.get_attrs()\n\n    config = {\n        \"class_name\": \"CustomizeAgent\",\n        \"name\": self.name,\n        \"description\": self.description,\n        \"prompt\": customize_action.prompt,\n        \"prompt_template\": customize_action.prompt_template.to_dict() if customize_action.prompt_template is not None else None, \n        # \"llm_config\": self.llm_config.to_dict(exclude_none=True),\n        \"inputs\": [\n            {\n                \"name\": field,\n                \"type\": self._action_input_types[field],\n                \"description\": field_info.description,\n                \"required\": self._action_input_required[field]\n            }\n            for field, field_info in customize_action.inputs_format.model_fields.items() if field in action_input_params\n        ],\n        \"outputs\": [\n            {\n                \"name\": field,\n                \"type\": self._action_output_types[field],\n                \"description\": field_info.description,\n                \"required\": self._action_output_required[field]\n            }\n            for field, field_info in customize_action.outputs_format.model_fields.items() if field in action_output_params\n        ],\n        \"system_prompt\": self.system_prompt,\n        \"output_parser\": self.output_parser.__name__ if self.output_parser is not None else None,\n        \"parse_mode\": self.parse_mode,\n        \"parse_func\": self.parse_func.__name__ if self.parse_func is not None else None,\n        \"title_format\": self.title_format \n    }\n    return config\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the customize agent's configuration to a JSON file.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>File path where the configuration should be saved</p> required <code>ignore</code> <code>List[str]</code> <p>List of keys to exclude from the saved configuration</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for the save operation</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The path where the configuration was saved</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"Save the customize agent's configuration to a JSON file.\n\n    Args:\n        path: File path where the configuration should be saved\n        ignore: List of keys to exclude from the saved configuration\n        **kwargs (Any): Additional parameters for the save operation\n\n    Returns:\n        The path where the configuration was saved\n    \"\"\"\n    config = self.get_customize_agent_info()\n\n    for ignore_key in ignore:\n        config.pop(ignore_key, None)\n\n    # Save to JSON file\n    make_parent_folder(path)\n    with open(path, 'w', encoding='utf-8') as f:\n        json.dump(config, f, indent=4, ensure_ascii=False)\n\n    return path\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.CustomizeAgent.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; dict\n</code></pre> <p>Get a dictionary containing all necessary configuration to recreate this agent.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A configuration dictionary that can be used to initialize a new Agent instance</p> <code>dict</code> <p>with the same properties as this one.</p> Source code in <code>evoagentx/agents/customize_agent.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"\n    Get a dictionary containing all necessary configuration to recreate this agent.\n\n    Returns:\n        dict: A configuration dictionary that can be used to initialize a new Agent instance\n        with the same properties as this one.\n    \"\"\"\n    config = self.get_customize_agent_info()\n    config[\"llm_config\"] = self.llm_config.to_dict()\n    return config \n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager","title":"AgentManager","text":"<pre><code>AgentManager(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Responsible for creating and managing all Agent objects required for workflow operation.</p> <p>Attributes:</p> Name Type Description <code>storage_handler</code> <code>StorageHandler</code> <p>Used to load and save agents from/to storage.</p> <code>agents</code> <code>List[Agent]</code> <p>A list to keep track of all managed Agent instances.</p> <code>agent_states</code> <code>Dict[str, AgentState]</code> <p>A dictionary to track the state of each Agent by name.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.size","title":"size  <code>property</code>","text":"<pre><code>size\n</code></pre> <p>Get the total number of agents managed by this manager.</p>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.check_agents","title":"check_agents","text":"<pre><code>check_agents()\n</code></pre> <p>Validate agent list integrity and state consistency.</p> <p>Performs thorough validation of the agent manager's internal state: 1. Checks for duplicate agent names 2. Verifies that agent states exist for all agents 3. Ensures agent list and state dictionary sizes match</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def check_agents(self):\n    \"\"\"Validate agent list integrity and state consistency.\n\n    Performs thorough validation of the agent manager's internal state:\n    1. Checks for duplicate agent names\n    2. Verifies that agent states exist for all agents\n    3. Ensures agent list and state dictionary sizes match\n    \"\"\"\n    # check that the names of self.agents should be unique\n    duplicate_agent_names = self.find_duplicate_agents(self.agents)\n    if duplicate_agent_names:\n        raise ValueError(f\"The agents should be unique. Found duplicate agent names: {duplicate_agent_names}!\")\n    # check agent states\n    if len(self.agents) != len(self.agent_states):\n        raise ValueError(f\"The lengths of self.agents ({len(self.agents)}) and self.agent_states ({len(self.agent_states)}) are different!\")\n    missing_agents = self.find_missing_agent_states()\n    if missing_agents:\n        raise ValueError(f\"The following agents' states were not found: {missing_agents}\")\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.has_agent","title":"has_agent","text":"<pre><code>has_agent(agent_name: str) -&gt; bool\n</code></pre> <p>Check if an agent with the given name exists in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if an agent with the given name exists, False otherwise</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def has_agent(self, agent_name: str) -&gt; bool:\n    \"\"\"Check if an agent with the given name exists in the manager.\n\n    Args:\n        agent_name: The name of the agent to check\n\n    Returns:\n        True if an agent with the given name exists, False otherwise\n    \"\"\"\n    all_agent_names = self.list_agents()\n    return agent_name in all_agent_names\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.load_agent","title":"load_agent","text":"<pre><code>load_agent(agent_name: str, **kwargs) -&gt; Agent\n</code></pre> <p>Load an agent from local storage through storage_handler.</p> <p>Retrieves agent data from storage and creates an Agent instance.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to load</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters for agent creation</p> <code>{}</code> <p>Returns:</p> Type Description <code>Agent</code> <p>Agent instance with data loaded from storage</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def load_agent(self, agent_name: str, **kwargs) -&gt; Agent:\n    \"\"\"Load an agent from local storage through storage_handler.\n\n    Retrieves agent data from storage and creates an Agent instance.\n\n    Args:\n        agent_name: The name of the agent to load\n        **kwargs (Any): Additional parameters for agent creation\n\n    Returns:\n        Agent instance with data loaded from storage\n    \"\"\"\n    if not self.storage_handler:\n        raise ValueError(\"must provide ``self.storage_handler`` to use ``load_agent``\")\n    agent_data = self.storage_handler.load_agent(agent_name=agent_name)\n    agent: Agent = self.create_customize_agent(agent_data=agent_data)\n    return agent\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.load_all_agents","title":"load_all_agents","text":"<pre><code>load_all_agents(**kwargs)\n</code></pre> <p>Load all agents from storage and add them to the manager.</p> <p>Retrieves all available agents from storage and adds them to the managed agents collection.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to storage handler</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def load_all_agents(self, **kwargs):\n    \"\"\"Load all agents from storage and add them to the manager.\n\n    Retrieves all available agents from storage and adds them to the\n    managed agents collection.\n\n    Args:\n        **kwargs (Any): Additional parameters passed to storage handler\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.create_customize_agent","title":"create_customize_agent","text":"<pre><code>create_customize_agent(agent_data: dict, llm_config: Optional[Union[LLMConfig, dict]] = None, **kwargs) -&gt; CustomizeAgent\n</code></pre> <p>create a customized agent from the provided <code>agent_data</code>. </p> <p>Parameters:</p> Name Type Description Default <code>agent_data</code> <code>dict</code> <p>The data used to create an Agent instance, must contain 'name', 'description' and 'prompt' keys.</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agent.  It will be used as the default LLM for agents without a <code>llm_config</code> key.  If not provided, the <code>agent_data</code> should contain a <code>llm_config</code> key.  If provided and <code>agent_data</code> contains a <code>llm_config</code> key, the <code>llm_config</code> in <code>agent_data</code> will be used.  </p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for agent creation</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>Agent</code> <code>CustomizeAgent</code> <p>the instantiated agent instance.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def create_customize_agent(self, agent_data: dict, llm_config: Optional[Union[LLMConfig, dict]]=None, **kwargs) -&gt; CustomizeAgent:\n    \"\"\"\n    create a customized agent from the provided `agent_data`. \n\n    Args:\n        agent_data: The data used to create an Agent instance, must contain 'name', 'description' and 'prompt' keys.\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agent. \n            It will be used as the default LLM for agents without a `llm_config` key. \n            If not provided, the `agent_data` should contain a `llm_config` key. \n            If provided and `agent_data` contains a `llm_config` key, the `llm_config` in `agent_data` will be used.  \n        **kwargs (Any): Additional parameters for agent creation\n\n    Returns:\n        Agent: the instantiated agent instance.\n    \"\"\"\n    agent_data = deepcopy(agent_data)\n    agent_llm_config = agent_data.get(\"llm_config\", llm_config)\n    if not agent_data.get(\"is_human\", False) and not agent_llm_config:\n        raise ValueError(\"`agent_data` should contain a `llm_config` key or `llm_config` should be provided.\")\n\n    if agent_llm_config:\n        if isinstance(agent_llm_config, dict):\n            agent_data[\"llm_config\"] = agent_llm_config\n        elif isinstance(agent_llm_config, LLMConfig):\n            agent_data[\"llm_config\"] = agent_llm_config.to_dict()\n        else:\n            raise ValueError(f\"llm_config must be a dictionary or an instance of LLMConfig. Got {type(agent_llm_config)}.\") \n\n    return CustomizeAgent.from_dict(data=agent_data)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.get_agent_name","title":"get_agent_name","text":"<pre><code>get_agent_name(agent: Union[str, dict, Agent]) -&gt; str\n</code></pre> <p>Extract agent name from different agent representations.</p> <p>Handles different ways to specify an agent (string name, dictionary, or Agent instance) and extracts the agent name.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Union[str, dict, Agent]</code> <p>Agent specified as a string name, dictionary with 'name' key,   or Agent instance</p> required <p>Returns:</p> Type Description <code>str</code> <p>The extracted agent name as a string</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_agent_name(self, agent: Union[str, dict, Agent]) -&gt; str:\n    \"\"\"Extract agent name from different agent representations.\n\n    Handles different ways to specify an agent (string name, dictionary, or\n    Agent instance) and extracts the agent name.\n\n    Args:\n        agent: Agent specified as a string name, dictionary with 'name' key,\n              or Agent instance\n\n    Returns:\n        The extracted agent name as a string\n    \"\"\"\n    if isinstance(agent, str):\n        agent_name = agent\n    elif isinstance(agent, dict):\n        agent_name = agent[\"name\"]\n    elif isinstance(agent, Agent):\n        agent_name = agent.name\n    else:\n        raise ValueError(f\"{type(agent)} is not a supported type for ``get_agent_name``. Supported types: [str, dict, Agent].\")\n    return agent_name\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.add_agent","title":"add_agent","text":"<pre><code>add_agent(agent: Union[str, dict, Agent], llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>add a single agent, ignore if the agent already exists (judged by the name of an agent).</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Union[str, dict, Agent]</code> <p>The agent to be added, specified as: - String: Agent name to load from storage - Dictionary: Agent specification to create a CustomizeAgent - Agent: Existing Agent instance to add directly</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agent. Only used when the <code>agent</code> is a dictionary, used to create a CustomizeAgent. </p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for agent creation</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef add_agent(self, agent: Union[str, dict, Agent], llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    add a single agent, ignore if the agent already exists (judged by the name of an agent).\n\n    Args:\n        agent: The agent to be added, specified as:\n            - String: Agent name to load from storage\n            - Dictionary: Agent specification to create a CustomizeAgent\n            - Agent: Existing Agent instance to add directly\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agent. Only used when the `agent` is a dictionary, used to create a CustomizeAgent. \n        **kwargs (Any): Additional parameters for agent creation\n    \"\"\"\n    agent_name = self.get_agent_name(agent=agent)\n    if self.has_agent(agent_name=agent_name):\n        return\n    agent_instance = self.create_agent(agent=agent, llm_config=llm_config, **kwargs)\n    self.agents.append(agent_instance)\n    self.agent_states[agent_instance.name] = AgentState.AVAILABLE\n    if agent_instance.name not in self._state_conditions:\n        self._state_conditions[agent_instance.name] = threading.Condition()\n    self.check_agents()\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.add_agents","title":"add_agents","text":"<pre><code>add_agents(agents: List[Union[str, dict, Agent]], llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>add several agents by using self.add_agent().</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def add_agents(self, agents: List[Union[str, dict, Agent]], llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    add several agents by using self.add_agent().\n    \"\"\"\n    for agent in agents:\n        self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.add_agents_from_workflow","title":"add_agents_from_workflow","text":"<pre><code>add_agents_from_workflow(workflow_graph, llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>Initialize agents from the nodes of a given WorkFlowGraph and add these agents to self.agents. </p> <p>Parameters:</p> Name Type Description Default <code>workflow_graph</code> <code>WorkFlowGraph</code> <p>The workflow graph containing nodes with agents information.</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agents.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to add_agent</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def add_agents_from_workflow(self, workflow_graph, llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    Initialize agents from the nodes of a given WorkFlowGraph and add these agents to self.agents. \n\n    Args:\n        workflow_graph (WorkFlowGraph): The workflow graph containing nodes with agents information.\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agents.\n        **kwargs (Any): Additional parameters passed to add_agent\n    \"\"\"\n    from ..workflow.workflow_graph import WorkFlowGraph\n    if not isinstance(workflow_graph, WorkFlowGraph):\n        raise TypeError(\"workflow_graph must be an instance of WorkFlowGraph\")\n    for node in workflow_graph.nodes:\n        if node.agents:\n            for agent in node.agents:\n                self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.update_agents_from_workflow","title":"update_agents_from_workflow","text":"<pre><code>update_agents_from_workflow(workflow_graph, llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>Update agents from a given WorkFlowGraph.</p> <p>Parameters:</p> Name Type Description Default <code>workflow_graph</code> <code>WorkFlowGraph</code> <p>The workflow graph containing nodes with agents information.</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agents.</p> <code>None</code> <code>**kwargs</code> <p>Additional parameters passed to update_agent</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def update_agents_from_workflow(self, workflow_graph, llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    Update agents from a given WorkFlowGraph.\n\n    Args:\n        workflow_graph (WorkFlowGraph): The workflow graph containing nodes with agents information.\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agents.\n        **kwargs: Additional parameters passed to update_agent\n    \"\"\"\n    from ..workflow.workflow_graph import WorkFlowGraph\n    if not isinstance(workflow_graph, WorkFlowGraph):\n        raise TypeError(\"workflow_graph must be an instance of WorkFlowGraph\")\n    for node in workflow_graph.nodes:\n        if node.agents:\n            for agent in node.agents:\n                agent_name = self.get_agent_name(agent=agent)\n                if self.has_agent(agent_name=agent_name):\n                    # use the llm_config of the existing agent\n                    agent_llm_config = self.get_agent(agent_name).llm_config\n                    self.update_agent(agent=agent, llm_config=agent_llm_config, **kwargs)\n                else:\n                    self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.get_agent","title":"get_agent","text":"<pre><code>get_agent(agent_name: str, **kwargs) -&gt; Agent\n</code></pre> <p>Retrieve an agent by its name from managed agents.</p> <p>Searches the list of managed agents for an agent with the specified name.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to retrieve</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused)</p> <code>{}</code> <p>Returns:</p> Type Description <code>Agent</code> <p>The Agent instance with the specified name</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_agent(self, agent_name: str, **kwargs) -&gt; Agent:\n    \"\"\"Retrieve an agent by its name from managed agents.\n\n    Searches the list of managed agents for an agent with the specified name.\n\n    Args:\n        agent_name: The name of the agent to retrieve\n        **kwargs (Any): Additional parameters (unused)\n\n    Returns:\n        The Agent instance with the specified name\n    \"\"\"\n    for agent in self.agents:\n        if agent.name == agent_name:\n            return agent\n    raise ValueError(f\"Agent ``{agent_name}`` does not exists!\")\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.update_agent","title":"update_agent","text":"<pre><code>update_agent(agent: Union[dict, Agent], llm_config: Optional[LLMConfig] = None, **kwargs)\n</code></pre> <p>Update an agent in the manager.</p> <p>Parameters:</p> Name Type Description Default <code>agent</code> <code>Union[dict, Agent]</code> <p>The agent to be updated, specified as: - Dictionary: Agent specification to update a CustomizeAgent - Agent: Existing Agent instance to update</p> required <code>llm_config</code> <code>Optional[LLMConfig]</code> <p>The LLM configuration to be used for the agent.</p> <code>None</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def update_agent(self, agent: Union[dict, Agent], llm_config: Optional[LLMConfig]=None, **kwargs):\n    \"\"\"\n    Update an agent in the manager.\n\n    Args:\n        agent: The agent to be updated, specified as:\n            - Dictionary: Agent specification to update a CustomizeAgent\n            - Agent: Existing Agent instance to update\n        llm_config (Optional[LLMConfig]): The LLM configuration to be used for the agent.\n    \"\"\"\n    agent_name = self.get_agent_name(agent=agent)\n    self.remove_agent(agent_name=agent_name)\n    self.add_agent(agent=agent, llm_config=llm_config, **kwargs)\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.remove_agent","title":"remove_agent","text":"<pre><code>remove_agent(agent_name: str, remove_from_storage: bool = False, **kwargs)\n</code></pre> <p>Remove an agent from the manager and optionally from storage.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to remove</p> required <code>remove_from_storage</code> <code>bool</code> <p>If True, also remove the agent from storage</p> <code>False</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to storage_handler.remove_agent</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef remove_agent(self, agent_name: str, remove_from_storage: bool=False, **kwargs):\n    \"\"\"\n    Remove an agent from the manager and optionally from storage.\n\n    Args:\n        agent_name: The name of the agent to remove\n        remove_from_storage: If True, also remove the agent from storage\n        **kwargs (Any): Additional parameters passed to storage_handler.remove_agent\n    \"\"\"\n    self.agents = [agent for agent in self.agents if agent.name != agent_name]\n    self.agent_states.pop(agent_name, None)\n    self._state_conditions.pop(agent_name, None) \n    if remove_from_storage:\n        self.storage_handler.remove_agent(agent_name=agent_name, **kwargs)\n    self.check_agents()\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.get_agent_state","title":"get_agent_state","text":"<pre><code>get_agent_state(agent_name: str) -&gt; AgentState\n</code></pre> <p>Get the state of a specific agent by its name.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent.</p> required <p>Returns:</p> Name Type Description <code>AgentState</code> <code>AgentState</code> <p>The current state of the agent.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_agent_state(self, agent_name: str) -&gt; AgentState:\n    \"\"\"\n    Get the state of a specific agent by its name.\n\n    Args:\n        agent_name: The name of the agent.\n\n    Returns:\n        AgentState: The current state of the agent.\n    \"\"\"\n    return self.agent_states[agent_name]\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.set_agent_state","title":"set_agent_state","text":"<pre><code>set_agent_state(agent_name: str, new_state: AgentState) -&gt; bool\n</code></pre> <p>Changes an agent's state and notifies any threads waiting on that agent's state. Thread-safe operation for coordinating multi-threaded agent execution.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent</p> required <code>new_state</code> <code>AgentState</code> <p>The new state to set</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the state was updated successfully, False otherwise</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef set_agent_state(self, agent_name: str, new_state: AgentState) -&gt; bool:\n    \"\"\"\n    Changes an agent's state and notifies any threads waiting on that agent's state.\n    Thread-safe operation for coordinating multi-threaded agent execution.\n\n    Args:\n        agent_name: The name of the agent\n        new_state: The new state to set\n\n    Returns:\n        True if the state was updated successfully, False otherwise\n    \"\"\"\n\n    # if agent_name in self.agent_states and isinstance(new_state, AgentState):\n    #     # self.agent_states[agent_name] = new_state\n    #     with self._state_conditions[agent_name]:\n    #         self.agent_states[agent_name] = new_state\n    #         self._state_conditions[agent_name].notify_all()\n    #     self.check_agents()\n    #     return True\n    # else:\n    #     return False\n    if agent_name in self.agent_states and isinstance(new_state, AgentState):\n        if agent_name not in self._state_conditions:\n            self._state_conditions[agent_name] = threading.Condition()\n        with self._state_conditions[agent_name]:\n            self.agent_states[agent_name] = new_state\n            self._state_conditions[agent_name].notify_all()\n        return True\n    return False\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.get_all_agent_states","title":"get_all_agent_states","text":"<pre><code>get_all_agent_states() -&gt; Dict[str, AgentState]\n</code></pre> <p>Get the states of all managed agents.</p> <p>Returns:</p> Type Description <code>Dict[str, AgentState]</code> <p>Dict[str, AgentState]: A dictionary mapping agent names to their states.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def get_all_agent_states(self) -&gt; Dict[str, AgentState]:\n    \"\"\"Get the states of all managed agents.\n\n    Returns:\n        Dict[str, AgentState]: A dictionary mapping agent names to their states.\n    \"\"\"\n    return self.agent_states\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.save_all_agents","title":"save_all_agents","text":"<pre><code>save_all_agents(**kwargs)\n</code></pre> <p>Save all managed agents to persistent storage.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional parameters passed to the storage handler</p> <code>{}</code> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef save_all_agents(self, **kwargs):\n    \"\"\"Save all managed agents to persistent storage.\n\n    Args:\n        **kwargs (Any): Additional parameters passed to the storage handler\n    \"\"\"\n    pass \n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.clear_agents","title":"clear_agents","text":"<pre><code>clear_agents()\n</code></pre> <p>Remove all agents from the manager.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>@atomic_method\ndef clear_agents(self):\n    \"\"\"\n    Remove all agents from the manager.\n    \"\"\"\n    self.agents = [] \n    self.agent_states = {}\n    self._state_conditions = {}\n    self.check_agents()\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.wait_for_agent_available","title":"wait_for_agent_available","text":"<pre><code>wait_for_agent_available(agent_name: str, timeout: Optional[float] = None) -&gt; bool\n</code></pre> <p>Wait for an agent to be available.</p> <p>Parameters:</p> Name Type Description Default <code>agent_name</code> <code>str</code> <p>The name of the agent to wait for</p> required <code>timeout</code> <code>Optional[float]</code> <p>Maximum time to wait in seconds, or None to wait indefinitely</p> <code>None</code> <p>Returns:</p> Type Description <code>bool</code> <p>True if the agent became available, False if timed out</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def wait_for_agent_available(self, agent_name: str, timeout: Optional[float] = None) -&gt; bool:\n    \"\"\"Wait for an agent to be available.\n\n    Args:\n        agent_name: The name of the agent to wait for\n        timeout: Maximum time to wait in seconds, or None to wait indefinitely\n\n    Returns:\n        True if the agent became available, False if timed out\n    \"\"\"\n    if agent_name not in self._state_conditions:\n        self._state_conditions[agent_name] = threading.Condition()\n    condition = self._state_conditions[agent_name]\n\n    with condition:\n        return condition.wait_for(\n            lambda: self.agent_states.get(agent_name) == AgentState.AVAILABLE,\n            timeout=timeout\n        )\n</code></pre>"},{"location":"api/agents.html#evoagentx.agents.AgentManager.copy","title":"copy","text":"<pre><code>copy() -&gt; AgentManager\n</code></pre> <p>Create a shallow copy of the AgentManager.</p> Source code in <code>evoagentx/agents/agent_manager.py</code> <pre><code>def copy(self) -&gt; \"AgentManager\":\n    \"\"\"\n    Create a shallow copy of the AgentManager.\n    \"\"\"\n    return AgentManager(agents=self.agents, storage_handler=self.storage_handler)\n</code></pre>"},{"location":"api/benchmark.html","title":"\ud83e\uddea Benchmark","text":""},{"location":"api/benchmark.html#evoagentx.benchmark","title":"evoagentx.benchmark","text":""},{"location":"api/benchmark.html#evoagentx.benchmark.NQ","title":"NQ","text":"<pre><code>NQ(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating question answering on Natural Questions dataset.</p> <p>Natural Questions (NQ) is a dataset for open-domain question answering, containing real questions from Google Search and answers from Wikipedia. This class handles loading the dataset, evaluating answers, and computing metrics like exact match and F1 score.</p> <p>Each NQ example has the following structure: {     \"id\": str,      \"question\": str,      \"answers\": List[str] }</p> <p>The benchmark evaluates answers using exact match, F1 score, and accuracy metrics.</p> Source code in <code>evoagentx/benchmark/nq.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/nq\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.HotPotQA","title":"HotPotQA","text":"<pre><code>HotPotQA(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating multi-hop question answering on HotPotQA dataset.</p> <p>Each HotPotQA example has the following structure: {     \"_id\": str,      \"question\": str,      \"answer\": str,      \"context\": [[\"context_title\", [\"context_sentence\", \"another_sentence\"]]],     \"supporting_facts\": [[\"supporting_title\", supporting_sentence_index]],     \"type\": str,     \"level\": str }</p> <p>The benchmark evaluates answers using exact match, F1 score, and accuracy metrics.</p> Source code in <code>evoagentx/benchmark/hotpotqa.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/hotpotqa\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.AFlowHotPotQA","title":"AFlowHotPotQA","text":"<pre><code>AFlowHotPotQA(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>HotPotQA</code></p> <p>AFlow-specific implementation of HotPotQA benchmark.</p> Source code in <code>evoagentx/benchmark/hotpotqa.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/hotpotqa\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.GSM8K","title":"GSM8K","text":"<pre><code>GSM8K(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating math reasoning on GSM8K dataset.</p> <p>GSM8K (Grade School Math 8K) is a dataset of math word problems that test a model's ability to solve grade school level math problems requiring multi-step reasoning. This class handles loading the dataset, evaluating solutions, and computing metrics based on answer accuracy.</p> <p>Each GSM8K example has the following structure: {     \"id\": \"test-1\",      \"question\": \"the question\",      \"answer\": \"the answer\" }</p> <p>The benchmark evaluates answers by extracting the final numerical value and comparing it to the ground truth answer.</p> Source code in <code>evoagentx/benchmark/gsm8k.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/gsm8k\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.GSM8K.extract_last_number","title":"extract_last_number","text":"<pre><code>extract_last_number(text: str) -&gt; float\n</code></pre> <p>Extract the last number from a text.</p> Source code in <code>evoagentx/benchmark/gsm8k.py</code> <pre><code>def extract_last_number(self, text: str) -&gt; float:\n    \"\"\"\n    Extract the last number from a text.\n    \"\"\"\n    matches = regex.findall(r\"[-+]?\\d+(?:,\\d{3})*(?:\\.\\d+)?|\\d+\\.\\d+\", str(text))\n    if matches:\n        last_number = matches[-1].replace(\",\", \"\").strip()\n        try:\n            last_number = float(last_number)\n            return last_number\n        except ValueError:\n            return None\n    return None\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.AFlowGSM8K","title":"AFlowGSM8K","text":"<pre><code>AFlowGSM8K(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>GSM8K</code></p> <p>AFlow-specific implementation of GSM8K benchmark.</p> <p>This class extends the GSM8K benchmark with features specific to the AFlow framework, including loading from AFlow-formatted data files and supporting asynchronous evaluation for workflows.</p> <p>Attributes:</p> Name Type Description <code>path</code> <p>Path to the directory containing AFlow-formatted GSM8K files.</p> <code>mode</code> <p>Data loading mode (\"train\", \"dev\", \"test\", or \"all\").</p> <code>_train_data</code> <code>Optional[List[dict]]</code> <p>Training dataset loaded from AFlow format.</p> <code>_dev_data</code> <code>Optional[List[dict]]</code> <p>Development dataset loaded from AFlow format.</p> <code>_test_data</code> <code>Optional[List[dict]]</code> <p>Test dataset loaded from AFlow format.</p> Source code in <code>evoagentx/benchmark/gsm8k.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/aflow/gsm8k\")\n    super().__init__(path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.MBPP","title":"MBPP","text":"<pre><code>MBPP(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>CodingBenchmark</code></p> <p>Benchmark class for evaluating code generation on the MBPP dataset.</p> <p>MBPP (Mostly Basic Python Programming) is a collection of Python programming    problems designed to test a model's ability to generate functionally correct    code from natural language descriptions. This class handles loading the dataset,    evaluating solutions, and computing metrics such as pass@k.</p> <p>The original MBPP format is transformed to be compatible with the HumanEval   benchmark format, allowing for consistent evaluation infrastructure.</p> <p>Each MBPP example has the following structure:   {       \"task_id\" (int): 2,        \"prompt\" (str): \"Write a function to find the shared elements from the given two lists.\",       \"code\" (str): \"def similar_elements(test_tup1, test_tup2): res = tuple(set(test_tup1) &amp; set(test_tup2)) return (res) \",        \"test_imports\": []        \"test_list\" (List[str]): ['assert set(similar_elements((3, 4, 5, 6),(5, 7, 4, 10))) == set((4, 5))', 'assert set(similar_elements((1, 2, 3, 4),(5, 4, 3, 7))) == set((3, 4))', 'assert set(similar_elements((11, 12, 14, 13),(17, 15, 14, 13))) == set((13, 14))']   }</p> <p>Attributes:       k: An integer or list of integers specifying which pass@k metrics to compute</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1,**kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/mbpp\")\n    self.k = k \n    super().__init__(name=type(self).__name__, path=path, mode=mode, timeout=timeout, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.MBPP.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The unit test code(s).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The unit test code(s).\n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n\n    results = []\n    for solution in prediction:\n        solution_states = []\n        for label_data in label:\n            task_id = label_data[\"task_id\"]\n            prompt = self.get_example_by_id(task_id)[\"prompt\"]\n            unit_test = label_data[\"test\"]\n            entry_point = label_data[\"entry_point\"]\n            state, message = self.check_solution(\n                task_id=task_id, \n                solution=prompt + \"\\n\" + solution,\n                test=unit_test, \n                entry_point=entry_point\n            )\n            if state != self.SUCCESS:\n                break \n            solution_states.append(state)\n        results.append(len(solution_states)==len(label) and all(state==self.SUCCESS for state in solution_states))\n\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n    pass_at_k = self.compute_pass_at_k(results, k_list)\n\n    return pass_at_k\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.AFlowMBPP","title":"AFlowMBPP","text":"<pre><code>AFlowMBPP(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>MBPP</code></p> <p>AFlow-specific implementation of MBPP benchmark.</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1,**kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/aflow/mbpp\")\n    super().__init__(path=path, mode=mode, timeout=timeout, k=k, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.AFlowMBPP.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The unit test code(s).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/mbpp.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The unit test code(s).\n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n\n    results = []\n    for solution in prediction:\n        solution_states = []\n        for label_data in label:\n            task_id = label_data[\"task_id\"]\n            prompt = self.get_example_by_id(task_id)[\"prompt\"]\n            unit_test = label_data[\"test\"]\n            entry_point = label_data[\"entry_point\"]\n            state, message = self.check_solution(\n                task_id=task_id, \n                solution=prompt + \"\\n\" + solution,\n                test=unit_test, \n                entry_point=entry_point,\n                use_entrypoint_as_input=False\n            )\n            if state != self.SUCCESS:\n                break \n            solution_states.append(state)\n        results.append(len(solution_states)==len(label) and all(state==self.SUCCESS for state in solution_states))\n\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n    pass_at_k = self.compute_pass_at_k(results, k_list)\n\n    return pass_at_k\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.MATH","title":"MATH","text":"<pre><code>MATH(path: str = None, mode: str = 'all', **kwargs)\n</code></pre> <p>               Bases: <code>Benchmark</code></p> <p>Benchmark class for evaluating mathematical reasoning on the MATH dataset.</p> <p>MATH is a dataset of challenging competition mathematics problems, spanning various difficulty levels and subject areas. This class handles loading the dataset, extracting answers, evaluating solutions through symbolic and numerical comparisons, and computing accuracy metrics.</p> <p>The dataset includes problems across 7 subject areas (Algebra, Geometry, etc.) and 5 difficulty levels. Each problem contains LaTeX-formatted questions and solutions.</p> <p>Each MATH example has the following structure: {     \"id\": \"test-1\",      \"problem\": \"the problem\",      \"solution\": \"the solution\",     \"level\": \"Level 1\", # \"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", \"Level ?\"     \"type\": \"Algebra\", # 'Geometry', 'Algebra', 'Intermediate Algebra', 'Counting &amp; Probability', 'Precalculus', 'Number Theory', 'Prealgebra' }</p> <p>The benchmark evaluates answers using symbolic math equality checking and numerical approximation to handle equivalent mathematical expressions.</p> Source code in <code>evoagentx/benchmark/math_benchmark.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/math\")\n    super().__init__(name=type(self).__name__, path=path, mode=mode, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.HumanEval","title":"HumanEval","text":"<pre><code>HumanEval(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>CodingBenchmark</code></p> <p>Benchmark class for evaluating code generation on HumanEval.</p> <pre><code>HumanEval is a collection of Python programming problems designed to test\na model's ability to generate functionally correct code from natural language\ndescriptions. This class handles loading the dataset, evaluating solutions,\nand computing metrics such as pass@k.\n\nEach HumanEval example has the following structure:\n{\n    \"task_id\": \"HumanEval/0\", \n    \"prompt\": \"from typing import List\n</code></pre> <p>def func_name(args, *kwargs) -&gt; return_type     \"function description\"</p> <p>\",          \"entry_point\": \"func_name\",         \"canonical_solution\": \"canonical solution (code)\",         \"test\": \"METADATA = {xxx}</p> <p>def check(candidate):  assert candidate(inputs) == output \"     }</p> <pre><code>Attributes:\n    k: An integer or list of integers specifying which pass@k metrics to compute\n</code></pre> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1, **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/humaneval\")\n    self.k = k \n    super().__init__(name=type(self).__name__, path=path, mode=mode, timeout=timeout, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.HumanEval.handle_special_cases","title":"handle_special_cases","text":"<pre><code>handle_special_cases(task_id: str, solution: str, test: str) -&gt; bool\n</code></pre> <p>Handle special cases for HumanEval.</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def handle_special_cases(self, task_id: str, solution: str, test: str) -&gt; bool:\n    \"\"\"\n    Handle special cases for HumanEval.\n    \"\"\"\n    if task_id == \"HumanEval/50\":\n        solution = (\n            '\\n\\ndef encode_shift(s: str):\\n    \"\"\"\\n    returns encoded string by shifting every character by 5 in the alphabet.\\n    \"\"\"\\n    return \"\".join([chr(((ord(ch) + 5 - ord(\"a\")) % 26) + ord(\"a\")) for ch in s])\\n\\n\\n'\n            + solution\n        )\n        return solution, test \n\n    return super().handle_special_cases(task_id=task_id, solution=solution, test=test)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.HumanEval.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The unit test code(s).</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The unit test code(s).\n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n\n    results = []\n    for solution in prediction:\n        solution_states = []\n        for label_data in label:\n            task_id = label_data[\"task_id\"]\n            prompt = self.get_example_by_id(task_id)[\"prompt\"]\n            unit_test = label_data[\"test\"]\n            entry_point = label_data[\"entry_point\"]\n            state, message = self.check_solution(\n                task_id=task_id, \n                solution=prompt + solution,\n                test=unit_test, \n                entry_point=entry_point\n            )\n            if state != self.SUCCESS:\n                break \n            solution_states.append(state)\n        results.append(len(solution_states)==len(label) and all(state==self.SUCCESS for state in solution_states))\n\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n    pass_at_k = self.compute_pass_at_k(results, k_list)\n\n    return pass_at_k\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.AFlowHumanEval","title":"AFlowHumanEval","text":"<pre><code>AFlowHumanEval(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, **kwargs)\n</code></pre> <p>               Bases: <code>HumanEval</code></p> <p>AFlow-specific implementation of HumanEval benchmark.</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def __init__(self, path: str = None, mode: str = \"all\", timeout: int = 60, k: Union[int, list] = 1, **kwargs):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/aflow/humaneval\")\n    super().__init__(path=path, mode=mode, timeout=timeout, k=k, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.AFlowHumanEval.extract_test_cases_with_entry_point","title":"extract_test_cases_with_entry_point","text":"<pre><code>extract_test_cases_with_entry_point(entry_point: str)\n</code></pre> <p>Extract test cases with the given entry point.</p> Source code in <code>evoagentx/benchmark/humaneval.py</code> <pre><code>def extract_test_cases_with_entry_point(self, entry_point: str):\n    \"\"\"\n    Extract test cases with the given entry point.\n    \"\"\"\n\n    hardcoded_cases = {\n        \"find_zero\": \"\",\n        \"decode_cyclic\": \"\",\n        \"decode_shift\": \"\",\n        \"by_length\": \"\",\n        \"add\": \"\",\n        \"triangle_area\": \"\",\n        \"correct_bracketing\": \"\",\n        \"solve\": \"\",\n        \"sum_squares\": \"\",\n        \"starts_one_ends\": \"\",\n    }\n    if entry_point in hardcoded_cases:\n        return hardcoded_cases[entry_point]\n\n    for case in self._test_cases:\n        if case[\"entry_point\"] == entry_point:\n            return case[\"test\"]\n\n    return None\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.LiveCodeBench","title":"LiveCodeBench","text":"<pre><code>LiveCodeBench(path: str = None, mode: str = 'all', timeout: int = 60, k: Union[int, list] = 1, num_process: int = 6, scenario: str = 'code_generation', version: str = 'release_latest', start_date: str = None, end_date: str = None, use_cot_for_execution: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>CodingBenchmark</code></p> <p>Benchmark class for evaluating LLM capabilities on real-world programming tasks.</p> <p>LiveCodeBench provides a framework for evaluating different scenarios of code-related tasks: 1. Code Generation: generating code from problem descriptions 2. Test Output Prediction: predicting test outputs given test code 3. Code Execution: generating code that executes correctly</p> <p>The benchmark supports different evaluation modes, metrics, and can be customized with various parameters like timeouts, sample dates, and processing options.</p> <p>Attributes:</p> Name Type Description <code>k</code> <p>An integer or list of integers specifying which pass@k metrics to compute</p> <code>version</code> <p>Release version of the dataset to use</p> <code>num_process</code> <p>Number of processes to use for evaluation</p> <code>start_date</code> <p>Filter problems to those after this date</p> <code>end_date</code> <p>Filter problems to those before this date</p> <code>scenario</code> <p>Type of programming task to evaluate (\"code_generation\",        \"test_output_prediction\", or \"code_execution\")</p> <code>use_cot_for_execution</code> <p>Whether to use chain-of-thought processing for code execution</p> Source code in <code>evoagentx/benchmark/livecodebench.py</code> <pre><code>def __init__(\n    self, \n    path: str = None, \n    mode: str = \"all\", \n    timeout: int = 60, \n    k: Union[int, list] = 1, \n    num_process: int = 6, \n    scenario: str = \"code_generation\", \n    version: str = \"release_latest\", \n    start_date: str = None, \n    end_date: str = None, \n    use_cot_for_execution: bool = False, \n    **kwargs\n):\n    path = os.path.expanduser(path or \"~/.evoagentx/data/livecodebench\")\n    self.k = k \n    self.version = version\n    self.num_process = num_process\n    self.start_date = start_date\n    self.end_date = end_date\n    self.scenario = scenario \n    self.use_cot_for_execution = use_cot_for_execution\n    assert scenario in VALID_SCENARIO, f\"Invalid scenario: {scenario}. Available choices: {VALID_SCENARIO}.\" \n    super().__init__(name=type(self).__name__, path=path, mode=mode, timeout=timeout, **kwargs)\n</code></pre>"},{"location":"api/benchmark.html#evoagentx.benchmark.LiveCodeBench.evaluate","title":"evaluate","text":"<pre><code>evaluate(prediction: Any, label: Any) -&gt; dict\n</code></pre> <p>Evaluate the solution code.</p> <p>Parameters:</p> Name Type Description Default <code>prediction</code> <code>str | List[str]</code> <p>The solution code(s).</p> required <code>label</code> <code>dict | List[dict]</code> <p>The test cases and expected outputs. </p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The evaluation metrics (pass@k).</p> Source code in <code>evoagentx/benchmark/livecodebench.py</code> <pre><code>def evaluate(self, prediction: Any, label: Any) -&gt; dict:\n    \"\"\"\n    Evaluate the solution code.\n\n    Args:\n        prediction (str | List[str]): The solution code(s).\n        label (dict | List[dict]): The test cases and expected outputs. \n\n    Returns:\n        dict: The evaluation metrics (pass@k).\n    \"\"\"\n    prediction, label = self._check_evaluation_inputs(prediction, label)\n    k_list = [self.k] if isinstance(self.k, int) else self.k\n\n    if self.scenario == \"code_generation\":\n        solutions: List[str] = [extract_code_blocks(pred)[0] for pred in prediction]\n        metrics, results, metadatas = codegen_metrics(\n            samples_list=label, # label is already a list \n            generations_list=[solutions], # for a single example. \n            k_list=k_list, \n            num_process_evaluate=self.num_process,\n            timeout=self.timeout\n        )\n\n    elif self.scenario == \"test_output_prediction\":\n        pred_outputs = [extract_test_output_code(pred) for pred in prediction]\n        metrics, results = test_output_metrics(\n            samples=label, \n            generations=[pred_outputs], \n            k_list=k_list, \n        )\n    elif self.scenario == \"code_execution\":\n        pred_outputs = [extract_execution_code(pred, self.use_cot_for_execution) for pred in prediction]\n        metrics, results = code_execution_metrics(\n            samples=label, \n            generations=[pred_outputs], \n        )\n    else:\n        raise ValueError(f\"Invalid scenario: {self.scenario}. Available choices: {VALID_SCENARIO}.\")\n\n    pass_at_k = {f\"pass@{k}\": float(metrics[f\"pass@{k}\"]) for k in k_list}\n    return pass_at_k\n</code></pre>"},{"location":"api/core.html","title":"\ud83e\udde0 Core","text":""},{"location":"api/core.html#evoagentx.core","title":"evoagentx.core","text":""},{"location":"api/core.html#evoagentx.core.BaseConfig","title":"BaseConfig","text":"<pre><code>BaseConfig(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base configuration class that serves as parent for all configuration classes.</p> <p>A config should inherit BaseConfig and specify the attributes and their types.  Otherwise this will be an empty config.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseConfig.save","title":"save","text":"<pre><code>save(path: str, **kwargs) -&gt; str\n</code></pre> <p>Save configuration to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save the configuration</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to save_module method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file was saved</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def save(self, path: str, **kwargs)-&gt; str:\n\n    \"\"\"Save configuration to the specified path.\n\n    Args:\n        path: The file path to save the configuration\n        **kwargs (Any): Additional keyword arguments passed to save_module method\n\n    Returns:\n        str: The path where the file was saved\n    \"\"\"\n    return super().save_module(path, **kwargs)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseConfig.get_config_params","title":"get_config_params","text":"<pre><code>get_config_params() -&gt; List[str]\n</code></pre> <p>Get a list of configuration parameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of configuration parameter names, excluding 'class_name'</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_config_params(self) -&gt; List[str]:\n    \"\"\"Get a list of configuration parameters.\n\n    Returns:\n        List[str]: List of configuration parameter names, excluding 'class_name'\n    \"\"\"\n    config_params = list(type(self).model_fields.keys())\n    config_params.remove(\"class_name\")\n    return config_params\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseConfig.get_set_params","title":"get_set_params","text":"<pre><code>get_set_params(ignore: List[str] = []) -&gt; dict\n</code></pre> <p>Get a dictionary of explicitly set parameters.</p> <p>Parameters:</p> Name Type Description Default <code>ignore</code> <code>List[str]</code> <p>List of parameter names to ignore</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_set_params(self, ignore: List[str] = []) -&gt; dict:\n    \"\"\"Get a dictionary of explicitly set parameters.\n\n    Args:\n        ignore: List of parameter names to ignore\n\n    Returns:\n        dict: Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters\n    \"\"\"\n    explicitly_set_fields = {field: getattr(self, field) for field in self.model_fields_set}\n    if self.kwargs:\n        explicitly_set_fields.update(self.kwargs)\n    for field in ignore:\n        explicitly_set_fields.pop(field, None)\n    explicitly_set_fields.pop(\"class_name\", None)\n    return explicitly_set_fields\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Message","title":"Message","text":"<pre><code>Message(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>the base class for message. </p> <p>Attributes:</p> Name Type Description <code>content</code> <code>Any</code> <p>the content of the message, need to implement str() function. </p> <code>agent</code> <code>str</code> <p>the sender of the message, normally set as the agent name.</p> <code>action</code> <code>str</code> <p>the trigger of the message, normally set as the action name.</p> <code>prompt</code> <code>str</code> <p>the prompt used to obtain the generated text. </p> <code>next_actions</code> <code>List[str]</code> <p>the following actions. </p> <code>msg_type</code> <code>str</code> <p>the type of the message, such as \"request\", \"response\", \"command\" etc. </p> <code>wf_goal</code> <code>str</code> <p>the goal of the whole workflow. </p> <code>wf_task</code> <code>str</code> <p>the name of a task in the workflow, i.e., the <code>name</code> of a WorkFlowNode instance. </p> <code>wf_task_desc</code> <code>str</code> <p>the description of a task in the workflow, i.e., the <code>description</code> of a WorkFlowNode instance.</p> <code>message_id</code> <code>str</code> <p>the unique identifier of the message. </p> <code>timestamp</code> <code>str</code> <p>the timestame of the message.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Message.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the Message to a dictionary for saving.</p> Source code in <code>evoagentx/core/message.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the Message to a dictionary for saving. \n    \"\"\"\n    data = super().to_dict(exclude_none=exclude_none, ignore=ignore, **kwargs) \n    if self.msg_type:\n        data[\"msg_type\"] = self.msg_type.value\n    return data \n</code></pre>"},{"location":"api/core.html#evoagentx.core.Message.sort_by_timestamp","title":"sort_by_timestamp  <code>classmethod</code>","text":"<pre><code>sort_by_timestamp(messages: List[Message], reverse: bool = False) -&gt; List[Message]\n</code></pre> <p>sort the messages based on the timestamp. </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>the messages to be sorted. </p> required <code>reverse</code> <code>bool</code> <p>If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.</p> <code>False</code> Source code in <code>evoagentx/core/message.py</code> <pre><code>@classmethod\ndef sort_by_timestamp(cls, messages: List['Message'], reverse: bool = False) -&gt; List['Message']:\n    \"\"\"\n    sort the messages based on the timestamp. \n\n    Args: \n        messages (List[Message]): the messages to be sorted. \n        reverse (bool): If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.\n    \"\"\"\n    messages.sort(key=lambda msg: datetime.strptime(msg.timestamp, \"%Y-%m-%d %H:%M:%S\"), reverse=reverse)\n    return messages\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Message.sort","title":"sort  <code>classmethod</code>","text":"<pre><code>sort(messages: List[Message], key: Optional[Callable[[Message], Any]] = None, reverse: bool = False) -&gt; List[Message]\n</code></pre> <p>sort the messages using key or timestamp (by default). </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[Message]</code> <p>the messages to be sorted. </p> required <code>key</code> <code>Optional[Callable[[Message], Any]]</code> <p>the function used to sort messages. </p> <code>None</code> <code>reverse</code> <code>bool</code> <p>If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.</p> <code>False</code> Source code in <code>evoagentx/core/message.py</code> <pre><code>@classmethod\ndef sort(cls, messages: List['Message'], key: Optional[Callable[['Message'], Any]] = None, reverse: bool = False) -&gt; List['Message']:\n    \"\"\"\n    sort the messages using key or timestamp (by default). \n\n    Args:\n        messages (List[Message]): the messages to be sorted. \n        key (Optional[Callable[['Message'], Any]]): the function used to sort messages. \n        reverse (bool): If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.\n    \"\"\"\n    if key is None:\n        return cls.sort_by_timestamp(messages, reverse=reverse)\n    messages.sort(key=key, reverse=reverse)\n    return messages\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Message.merge","title":"merge  <code>classmethod</code>","text":"<pre><code>merge(messages: List[List[Message]], sort: bool = False, key: Optional[Callable[[Message], Any]] = None, reverse: bool = False) -&gt; List[Message]\n</code></pre> <p>merge different message list. </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[List[Message]]</code> <p>the message lists to be merged. </p> required <code>sort</code> <code>bool</code> <p>whether to sort the merged messages.</p> <code>False</code> <code>key</code> <code>Optional[Callable[[Message], Any]]</code> <p>the function used to sort messages. </p> <code>None</code> <code>reverse</code> <code>bool</code> <p>If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.</p> <code>False</code> Source code in <code>evoagentx/core/message.py</code> <pre><code>@classmethod\ndef merge(cls, messages: List[List['Message']], sort: bool=False, key: Optional[Callable[['Message'], Any]] = None, reverse: bool=False) -&gt; List['Message']:\n    \"\"\"\n    merge different message list. \n\n    Args:\n        messages (List[List[Message]]): the message lists to be merged. \n        sort (bool): whether to sort the merged messages.\n        key (Optional[Callable[['Message'], Any]]): the function used to sort messages. \n        reverse (bool): If True, sort the messages in descending order. Otherwise, sort the messages in ascending order.\n    \"\"\"\n    merged_messages = sum(messages, [])\n    if sort:\n        merged_messages = cls.sort(merged_messages, key=key, reverse=reverse)\n    return merged_messages\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Parser","title":"Parser","text":"<pre><code>Parser(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Parser.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(content: str, **kwargs)\n</code></pre> <p>the method used to parse text into a Parser object. Use Parser.from_str to parse input by default.  Args:     content: The content to parse     **kwargs: Additional keyword arguments Returns:     Parser: The parsed Parser object</p> Source code in <code>evoagentx/core/parser.py</code> <pre><code>@classmethod\ndef parse(cls, content: str, **kwargs):\n    \"\"\"\n    the method used to parse text into a Parser object. Use Parser.from_str to parse input by default. \n    Args:\n        content: The content to parse\n        **kwargs: Additional keyword arguments\n    Returns:\n        Parser: The parsed Parser object\n    \"\"\"\n    return cls.from_str(content, **kwargs)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.Parser.save","title":"save","text":"<pre><code>save(path: str, **kwargs) -&gt; str\n</code></pre> <p>Save the Parser object to a file.</p> Source code in <code>evoagentx/core/parser.py</code> <pre><code>def save(self, path: str, **kwargs)-&gt; str:\n    \"\"\"\n    Save the Parser object to a file.\n    \"\"\"\n    super().save_module(path, **kwargs)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule","title":"BaseModule","text":"<pre><code>BaseModule(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModel</code></p> <p>Base module class that serves as the foundation for all modules in the EvoAgentX framework.</p> <p>This class provides serialization/deserialization capabilities, supports creating instances from dictionaries, JSON, or files, and exporting instances to these formats.</p> <p>Attributes:</p> Name Type Description <code>class_name</code> <code>str</code> <p>The class name, defaults to None but is automatically set during subclass initialization</p> <code>model_config</code> <p>Pydantic model configuration that controls type matching and behavior</p> <p>Initializes a BaseModule instance.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Keyword arguments used to initialize the instance</p> <code>{}</code> <p>Raises:</p> Type Description <code>ValidationError</code> <p>When parameter validation fails</p> <code>Exception</code> <p>When other errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.kwargs","title":"kwargs  <code>property</code>","text":"<pre><code>kwargs: dict\n</code></pre> <p>Returns the extra fields of the model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing all extra keyword arguments</p>"},{"location":"api/core.html#evoagentx.core.BaseModule.__init_subclass__","title":"__init_subclass__","text":"<pre><code>__init_subclass__(**kwargs)\n</code></pre> <p>Subclass initialization method that automatically sets the class_name attribute.</p> <p>Parameters:</p> Name Type Description Default <code>cls</code> <code>Type</code> <p>The subclass being initialized</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init_subclass__(cls, **kwargs):\n    \"\"\"\n    Subclass initialization method that automatically sets the class_name attribute.\n\n    Args:\n        cls (Type): The subclass being initialized\n        **kwargs (Any): Additional keyword arguments\n    \"\"\"\n    super().__init_subclass__(**kwargs)\n    cls.class_name = cls.__name__\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Module initialization method that subclasses can override to provide additional initialization logic.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def init_module(self):\n    \"\"\"\n    Module initialization method that subclasses can override to provide additional initialization logic.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of the object.</p> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>String representation of the object</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the object.\n\n    Returns:\n        str: String representation of the object\n    \"\"\"\n    return self.to_str()\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any], **kwargs) -&gt; BaseModule\n</code></pre> <p>Instantiate the BaseModule from a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary containing instance data</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include log to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>Exception</code> <p>When errors occur during initialization</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Instantiate the BaseModule from a dictionary.\n\n    Args:\n        data: Dictionary containing instance data\n        **kwargs (Any): Additional keyword arguments, can include log to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        Exception: When errors occur during initialization\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    with exception_buffer() as buffer:\n        try:\n            class_name = data.get(\"class_name\", None)\n            if class_name:\n                cls = MODULE_REGISTRY.get_module(class_name)\n            module = cls._create_instance(data)\n            # module = cls.model_validate(data)\n            if len(buffer.exceptions) &gt; 0:\n                error_message = get_base_module_init_error_message(cls, data, buffer.exceptions)\n                if use_logger:\n                    logger.error(error_message)\n                raise Exception(get_error_message(buffer.exceptions))\n        finally:\n            pass\n    return module\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.from_json","title":"from_json  <code>classmethod</code>","text":"<pre><code>from_json(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a JSON string.</p> <p>This method uses yaml.safe_load to parse the JSON string into a Python object, which supports more flexible parsing than standard json.loads (including handling single quotes, trailing commas, etc). The parsed data is then passed to from_dict to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>JSON string</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input is not a valid JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_json(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a JSON string.\n\n    This method uses yaml.safe_load to parse the JSON string into a Python object,\n    which supports more flexible parsing than standard json.loads (including handling\n    single quotes, trailing commas, etc). The parsed data is then passed to from_dict\n    to create the instance.\n\n    Args:\n        content: JSON string\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input is not a valid JSON string\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    try:\n        data = yaml.safe_load(content)\n    except Exception:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    if not isinstance(data, (list, dict)):\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_json is not a valid JSON string.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return cls.from_dict(data, log=use_logger)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.from_str","title":"from_str  <code>classmethod</code>","text":"<pre><code>from_str(content: str, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a string that may contain JSON.</p> <p>This method is more forgiving than <code>from_json</code> as it can extract valid JSON objects embedded within larger text. It uses <code>parse_json_from_text</code> to extract  all potential JSON strings from the input text, then tries to create an instance  from each extracted JSON string until successful.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>Text that may contain JSON strings</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the input does not contain valid JSON strings or the JSON is incompatible with the class</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_str(cls, content: str, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a string that may contain JSON.\n\n    This method is more forgiving than `from_json` as it can extract valid JSON\n    objects embedded within larger text. It uses `parse_json_from_text` to extract \n    all potential JSON strings from the input text, then tries to create an instance \n    from each extracted JSON string until successful.\n\n    Args:\n        content: Text that may contain JSON strings\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the input does not contain valid JSON strings or the JSON is incompatible with the class\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n\n    extracted_json_list = parse_json_from_text(content)\n    if len(extracted_json_list) == 0:\n        error_message = f\"The input to {cls.__name__}.from_str does not contain any valid JSON str.\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    module = None\n    for json_str in extracted_json_list:\n        try:\n            module = cls.from_json(json_str, log=False)\n        except Exception:\n            continue\n        break\n\n    if module is None:\n        error_message = f\"Can not instantiate {cls.__name__}. The input to {cls.__name__}.from_str either does not contain a valide JSON str, or the JSON str is incomplete or incompatable (incorrect variables or types) with {cls.__name__}.\"\n        error_message += f\"\\nInput:\\n{content}\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    return module\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, **kwargs) -&gt; dict\n</code></pre> <p>Load the values for a module from a file.</p> <p>By default, it opens the specified file and uses <code>yaml.safe_load</code> to parse its contents  into a Python object (typically a dictionary).</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The JSON object instantiated from the file</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod \ndef load_module(cls, path: str, **kwargs) -&gt; dict:\n    \"\"\"\n    Load the values for a module from a file.\n\n    By default, it opens the specified file and uses `yaml.safe_load` to parse its contents \n    into a Python object (typically a dictionary).\n\n    Args:\n        path: The path of the file\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: The JSON object instantiated from the file\n    \"\"\"\n    with open(path, mode=\"r\", encoding=\"utf-8\") as file:\n        content = yaml.safe_load(file.read())\n    return content\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.from_file","title":"from_file  <code>classmethod</code>","text":"<pre><code>from_file(path: str, load_function: Callable = None, **kwargs) -&gt; BaseModule\n</code></pre> <p>Construct the BaseModule from a file.</p> <p>This method reads and parses a file into a data structure, then creates a module instance from that data. It first verifies that the file exists, then uses either the provided <code>load_function</code> or the default <code>load_module</code> method to read and parse the file content, and finally calls <code>from_dict</code> to create the instance.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path of the file</p> required <code>load_function</code> <code>Callable</code> <p>The function used to load the data, takes a file path as input and returns a JSON object</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments, can include <code>log</code> to control logging output</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>BaseModule</code> <code>BaseModule</code> <p>The created module instance</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>When the file does not exist</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>@classmethod\ndef from_file(cls, path: str, load_function: Callable=None, **kwargs) -&gt; \"BaseModule\":\n    \"\"\"\n    Construct the BaseModule from a file.\n\n    This method reads and parses a file into a data structure, then creates\n    a module instance from that data. It first verifies that the file exists,\n    then uses either the provided `load_function` or the default `load_module`\n    method to read and parse the file content, and finally calls `from_dict`\n    to create the instance.\n\n    Args:\n        path: The path of the file\n        load_function: The function used to load the data, takes a file path as input and returns a JSON object\n        **kwargs (Any): Additional keyword arguments, can include `log` to control logging output\n\n    Returns:\n        BaseModule: The created module instance\n\n    Raises:\n        ValueError: When the file does not exist\n    \"\"\"\n    use_logger = kwargs.get(\"log\", True)\n    if not os.path.exists(path):\n        error_message = f\"File \\\"{path}\\\" does not exist!\"\n        if use_logger:\n            logger.error(error_message)\n        raise ValueError(error_message)\n\n    function = load_function or cls.load_module\n    content = function(path, **kwargs)\n    module = cls.from_dict(content, log=use_logger)\n\n    return module\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.to_dict","title":"to_dict","text":"<pre><code>to_dict(exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict\n</code></pre> <p>Convert the BaseModule to a dictionary.</p> <p>Parameters:</p> Name Type Description Default <code>exclude_none</code> <code>bool</code> <p>Whether to exclude fields with None values</p> <code>True</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary containing the object data</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_dict(self, exclude_none: bool = True, ignore: List[str] = [], **kwargs) -&gt; dict:\n    \"\"\"\n    Convert the BaseModule to a dictionary.\n\n    Args:\n        exclude_none: Whether to exclude fields with None values\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        dict: Dictionary containing the object data\n    \"\"\"\n    data = {}\n    for field_name, _ in type(self).model_fields.items():\n        if field_name in ignore:\n            continue\n        field_value = getattr(self, field_name, None)\n        if exclude_none and field_value is None:\n            continue\n        if isinstance(field_value, BaseModule):\n            data[field_name] = field_value.to_dict(exclude_none=exclude_none, ignore=ignore)\n        elif isinstance(field_value, list):\n            data[field_name] = [\n                item.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(item, BaseModule) else item\n                for item in field_value\n            ]\n        elif isinstance(field_value, dict):\n            data[field_name] = {\n                key: value.to_dict(exclude_none=exclude_none, ignore=ignore) if isinstance(value, BaseModule) else value\n                for key, value in field_value.items()\n            }\n        else:\n            data[field_name] = field_value\n\n    return data\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.to_json","title":"to_json","text":"<pre><code>to_json(use_indent: bool = False, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a JSON string.</p> <p>Parameters:</p> Name Type Description Default <code>use_indent</code> <code>bool</code> <p>Whether to use indentation</p> <code>False</code> <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The JSON string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_json(self, use_indent: bool=False, ignore: List[str] = [], **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a JSON string.\n\n    Args:\n        use_indent: Whether to use indentation\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The JSON string\n    \"\"\"\n    if use_indent:\n        kwargs[\"indent\"] = kwargs.get(\"indent\", 4)\n    else:\n        kwargs.pop(\"indent\", None)\n    if kwargs.get(\"default\", None) is None:\n        kwargs[\"default\"] = custom_serializer\n    data = self.to_dict(exclude_none=True)\n    for ignore_field in ignore:\n        data.pop(ignore_field, None)\n    return json.dumps(data, **kwargs)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.to_str","title":"to_str","text":"<pre><code>to_str(**kwargs) -&gt; str\n</code></pre> <p>Convert the BaseModule to a string. Use .to_json to output JSON string by default.</p> <p>Parameters:</p> Name Type Description Default <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The string</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def to_str(self, **kwargs) -&gt; str:\n    \"\"\"\n    Convert the BaseModule to a string. Use .to_json to output JSON string by default.\n\n    Args:\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The string\n    \"\"\"\n    return self.to_json(use_indent=False)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs) -&gt; str\n</code></pre> <p>Save the BaseModule to a file.</p> <p>This method will set non-serializable objects to None by default. If you want to save non-serializable objects, override this method. Remember to also override the <code>load_module</code> function to ensure the loaded object can be correctly parsed by <code>cls.from_dict</code>.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the file</p> required <code>ignore</code> <code>List[str]</code> <p>List of field names to ignore</p> <code>[]</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file is saved, same as the input path</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs)-&gt; str:\n    \"\"\"\n    Save the BaseModule to a file.\n\n    This method will set non-serializable objects to None by default.\n    If you want to save non-serializable objects, override this method.\n    Remember to also override the `load_module` function to ensure the loaded\n    object can be correctly parsed by `cls.from_dict`.\n\n    Args:\n        path: The path to save the file\n        ignore: List of field names to ignore\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The path where the file is saved, same as the input path\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    return save_json(self.to_json(use_indent=True, default=lambda x: None, ignore=ignore), path=path)\n</code></pre>"},{"location":"api/core.html#evoagentx.core.BaseModule.deepcopy","title":"deepcopy","text":"<pre><code>deepcopy()\n</code></pre> <p>Deep copy the module.</p> <p>This is a tweak to the default python deepcopy that only deep copies <code>self.parameters()</code>, and for other attributes, we just do the shallow copy.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def deepcopy(self):\n    \"\"\"Deep copy the module.\n\n    This is a tweak to the default python deepcopy that only deep copies `self.parameters()`, and for other\n    attributes, we just do the shallow copy.\n    \"\"\"\n    try:\n        # If the instance itself is copyable, we can just deep copy it.\n        # Otherwise we will have to create a new instance and copy over the attributes one by one.\n        return copy.deepcopy(self)\n    except Exception:\n        pass\n\n    # Create an empty instance.\n    new_instance = self.__class__.__new__(self.__class__)\n    # Set attribuetes of the copied instance.\n    for attr, value in self.__dict__.items():\n        if isinstance(value, BaseModule):\n            setattr(new_instance, attr, value.deepcopy())\n        else:\n            try:\n                # Try to deep copy the attribute\n                setattr(new_instance, attr, copy.deepcopy(value))\n            except Exception:\n                logging.warning(\n                    f\"Failed to deep copy attribute '{attr}' of {self.__class__.__name__}, \"\n                    \"falling back to shallow copy or reference copy.\"\n                )\n                try:\n                    # Fallback to shallow copy if deep copy fails\n                    setattr(new_instance, attr, copy.copy(value))\n                except Exception:\n                    # If even the shallow copy fails, we just copy over the reference.\n                    setattr(new_instance, attr, value)\n\n    return new_instance\n</code></pre>"},{"location":"api/core.html#evoagentx.core.ParseFunctionRegistry","title":"ParseFunctionRegistry","text":"<pre><code>ParseFunctionRegistry()\n</code></pre> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def __init__(self):\n    self.functions = {}\n</code></pre>"},{"location":"api/core.html#evoagentx.core.ParseFunctionRegistry.register","title":"register","text":"<pre><code>register(func_name: str, func)\n</code></pre> <p>Register a function with a given name.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name to register the function under</p> required <code>func</code> <code>Callable</code> <p>The function to register</p> required <p>Raises:</p> Type Description <code>ValueError</code> <p>If a function with the same name is already registered</p> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def register(self, func_name: str, func):\n    \"\"\"Register a function with a given name.\n\n    Args:\n        func_name: The name to register the function under\n        func (Callable): The function to register\n\n    Raises:\n        ValueError: If a function with the same name is already registered\n    \"\"\"\n    if func_name in self.functions:\n        raise ValueError(f\"Function name '{func_name}' is already registered!\")\n    self.functions[func_name] = func\n</code></pre>"},{"location":"api/core.html#evoagentx.core.ParseFunctionRegistry.get_function","title":"get_function","text":"<pre><code>get_function(func_name: str) -&gt; callable\n</code></pre> <p>Get a registered function by name.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name of the function to retrieve</p> required <p>Returns:</p> Name Type Description <code>Callable</code> <code>callable</code> <p>The registered function</p> <p>Raises:</p> Type Description <code>KeyError</code> <p>If no function with the given name is registered</p> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def get_function(self, func_name: str) -&gt; callable:\n    \"\"\"Get a registered function by name.\n\n    Args:\n        func_name: The name of the function to retrieve\n\n    Returns:\n        Callable: The registered function\n\n    Raises:\n        KeyError: If no function with the given name is registered\n    \"\"\"\n    if func_name not in self.functions:\n        available_funcs = list(self.functions.keys())\n        raise KeyError(f\"Function '{func_name}' not found! Available functions: {available_funcs}\")\n    return self.functions[func_name]\n</code></pre>"},{"location":"api/core.html#evoagentx.core.ParseFunctionRegistry.has_function","title":"has_function","text":"<pre><code>has_function(func_name: str) -&gt; bool\n</code></pre> <p>Check if a function name is registered.</p> <p>Parameters:</p> Name Type Description Default <code>func_name</code> <code>str</code> <p>The name to check</p> required <p>Returns:</p> Type Description <code>bool</code> <p>True if the function name is registered, False otherwise</p> Source code in <code>evoagentx/core/registry.py</code> <pre><code>def has_function(self, func_name: str) -&gt; bool:\n    \"\"\"Check if a function name is registered.\n\n    Args:\n        func_name: The name to check\n\n    Returns:\n        True if the function name is registered, False otherwise\n    \"\"\"\n    return func_name in self.functions\n</code></pre>"},{"location":"api/evaluators.html","title":"\ud83e\uddd1\u200d\u2696\ufe0f Evaluators","text":""},{"location":"api/evaluators.html#evoagentx.evaluators","title":"evoagentx.evaluators","text":""},{"location":"api/evaluators.html#evoagentx.evaluators.Evaluator","title":"Evaluator","text":"<pre><code>Evaluator(llm: BaseLLM, num_workers: int = 1, agent_manager: Optional[AgentManager] = None, collate_func: Optional[Callable] = None, output_postprocess_func: Optional[Callable] = None, verbose: Optional[bool] = None, **kwargs)\n</code></pre> <p>A class for evaluating the performance of a workflow.</p> <p>Initialize the Evaluator.</p> <p>Parameters:</p> Name Type Description Default <code>llm</code> <code>BaseLLM</code> <p>The LLM to use for evaluation.</p> required <code>num_workers</code> <code>int</code> <p>The number of parallel workers to use for evaluation. Default is 1. </p> <code>1</code> <code>agent_manager</code> <code>AgentManager</code> <p>The agent manager used to construct the workflow. Only used when the workflow graph is a WorkFlowGraph.</p> <code>None</code> <code>collate_func</code> <code>Callable</code> <p>A function to collate the benchmark data.  It receives a single example from the benchmark and the output (which should be a dictionary) will serve as inputs to the <code>execute</code> function of an WorkFlow (or ActionGraph) instance.  Note that the keys in the collated output should match the inputs of the workflow. The default is a lambda function that returns the example itself. </p> <code>None</code> <code>output_postprocess_func</code> <code>Callable</code> <p>A function to postprocess the output of the workflow.  It receives the output of an WorkFlow instance (str) or an ActionGraph instance (dict) as input  and the output will be passed to the <code>evaluate</code> function of the benchmark.  The default is a lambda function that returns the output itself.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the evaluation progress.</p> <code>None</code> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def __init__(\n    self, \n    llm: BaseLLM,\n    num_workers: int = 1, \n    agent_manager: Optional[AgentManager] = None,\n    collate_func: Optional[Callable] = None, \n    output_postprocess_func: Optional[Callable] = None, \n    verbose: Optional[bool] = None, \n    **kwargs\n):\n    \"\"\"\n    Initialize the Evaluator.\n\n    Args:\n        llm (BaseLLM): The LLM to use for evaluation.\n        num_workers (int): The number of parallel workers to use for evaluation. Default is 1. \n        agent_manager (AgentManager, optional): The agent manager used to construct the workflow. Only used when the workflow graph is a WorkFlowGraph.\n        collate_func (Callable, optional): A function to collate the benchmark data. \n            It receives a single example from the benchmark and the output (which should be a dictionary) will serve as inputs  \n            to the `execute` function of an WorkFlow (or ActionGraph) instance. \n            Note that the keys in the collated output should match the inputs of the workflow.\n            The default is a lambda function that returns the example itself. \n        output_postprocess_func (Callable, optional): A function to postprocess the output of the workflow. \n            It receives the output of an WorkFlow instance (str) or an ActionGraph instance (dict) as input \n            and the output will be passed to the `evaluate` function of the benchmark. \n            The default is a lambda function that returns the output itself.\n        verbose (bool, optional): Whether to print the evaluation progress.\n    \"\"\"\n    self.llm = llm\n    self.num_workers = num_workers\n    self.agent_manager = agent_manager\n    self._thread_agent_managers = {}\n    self.collate_func = collate_func or (lambda x: x)\n    self.output_postprocess_func = output_postprocess_func or (lambda x: x)\n    self.verbose = verbose\n    # {example_id: {\"prediction\": Any, \"label\": Any, \"metrics\": dict, \"trajectory\" (WorkFlowGraph only): List[Message]}}\n    self._evaluation_records = {}\n    self.kwargs = kwargs\n</code></pre>"},{"location":"api/evaluators.html#evoagentx.evaluators.Evaluator.evaluate","title":"evaluate","text":"<pre><code>evaluate(graph: Union[WorkFlowGraph, ActionGraph], benchmark: Benchmark, eval_mode: str = 'test', indices: Optional[List[int]] = None, sample_k: Optional[int] = None, seed: Optional[int] = None, verbose: Optional[bool] = None, update_agents: Optional[bool] = False, **kwargs) -&gt; dict\n</code></pre> <p>Evaluate the performance of the workflow on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>WorkFlowGraph or ActionGraph</code> <p>The workflow to evaluate.</p> required <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'test'</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the evaluation progress. If not provided, the <code>self.verbose</code> will be used.</p> <code>None</code> <code>update_agents</code> <code>bool</code> <p>Whether to update the agents in the agent manager. Only used when the workflow graph is a WorkFlowGraph.</p> <code>False</code> <p>Returns:     dict: The average metrics of the workflow evaluation.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def evaluate(\n    self, \n    graph: Union[WorkFlowGraph, ActionGraph],\n    benchmark: Benchmark, \n    eval_mode: str = \"test\", \n    indices: Optional[List[int]] = None, \n    sample_k: Optional[int] = None, \n    seed: Optional[int] = None, \n    verbose: Optional[bool] = None,\n    update_agents: Optional[bool] = False,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Evaluate the performance of the workflow on the benchmark.\n\n    Args:\n        graph (WorkFlowGraph or ActionGraph): The workflow to evaluate.\n        benchmark (Benchmark): The benchmark to evaluate the workflow on.\n        eval_mode (str): which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n        verbose (bool, optional): Whether to print the evaluation progress. If not provided, the `self.verbose` will be used.\n        update_agents (bool, optional): Whether to update the agents in the agent manager. Only used when the workflow graph is a WorkFlowGraph.\n    Returns:\n        dict: The average metrics of the workflow evaluation.\n    \"\"\"\n    # clear the evaluation records\n    self._evaluation_records.clear()\n\n    # update the agents in the agent manager\n    if isinstance(graph, WorkFlowGraph) and update_agents:\n        if self.agent_manager is None:\n            raise ValueError(f\"`agent_manager` is not provided in {type(self).__name__}. Please provide an agent manager when evaluating a WorkFlowGraph.\")\n        self.agent_manager.update_agents_from_workflow(workflow_graph=graph, llm_config=self.llm.config, **kwargs)\n\n    data = self._get_eval_data(benchmark=benchmark, eval_mode=eval_mode, indices=indices, sample_k=sample_k, seed=seed)\n    results = self._evaluate_graph(graph=graph, data=data, benchmark=benchmark, verbose=verbose, **kwargs)\n    return results\n</code></pre>"},{"location":"api/evaluators.html#evoagentx.evaluators.Evaluator.get_example_evaluation_record","title":"get_example_evaluation_record","text":"<pre><code>get_example_evaluation_record(benchmark: Benchmark, example: Any) -&gt; Optional[dict]\n</code></pre> <p>Get the evaluation record for a given example.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def get_example_evaluation_record(self, benchmark: Benchmark, example: Any) -&gt; Optional[dict]:\n    \"\"\"\n    Get the evaluation record for a given example.\n    \"\"\"\n    example_id = benchmark.get_id(example=example)\n    return self._evaluation_records.get(example_id, None)\n</code></pre>"},{"location":"api/evaluators.html#evoagentx.evaluators.Evaluator.get_evaluation_record_by_id","title":"get_evaluation_record_by_id","text":"<pre><code>get_evaluation_record_by_id(benchmark: Benchmark, example_id: str, eval_mode: str = 'test') -&gt; Optional[dict]\n</code></pre> <p>Get the evaluation record for a given example id.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def get_evaluation_record_by_id(self, benchmark: Benchmark, example_id: str, eval_mode: str = \"test\") -&gt; Optional[dict]:\n    \"\"\"\n    Get the evaluation record for a given example id.\n    \"\"\"\n    example = benchmark.get_example_by_id(example_id=example_id, mode=eval_mode)\n    return self.get_example_evaluation_record(benchmark=benchmark, example=example)\n</code></pre>"},{"location":"api/evaluators.html#evoagentx.evaluators.Evaluator.get_all_evaluation_records","title":"get_all_evaluation_records","text":"<pre><code>get_all_evaluation_records() -&gt; dict\n</code></pre> <p>Get all the evaluation records.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>def get_all_evaluation_records(self) -&gt; dict:\n    \"\"\"\n    Get all the evaluation records.\n    \"\"\"\n    return self._evaluation_records.copy()\n</code></pre>"},{"location":"api/evaluators.html#evoagentx.evaluators.Evaluator.async_evaluate","title":"async_evaluate  <code>async</code>","text":"<pre><code>async_evaluate(graph: Union[WorkFlowGraph, ActionGraph], benchmark: Benchmark, eval_mode: str = 'test', indices: Optional[List[int]] = None, sample_k: Optional[int] = None, seed: Optional[int] = None, verbose: Optional[bool] = None, **kwargs) -&gt; dict\n</code></pre> <p>Asynchronously evaluate the performance of the workflow on the benchmark.</p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>WorkFlowGraph or ActionGraph</code> <p>The workflow to evaluate.</p> required <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'test'</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <code>verbose</code> <code>bool</code> <p>Whether to print the evaluation progress. If not provided, the <code>self.verbose</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The average metrics of the workflow evaluation.</p> Source code in <code>evoagentx/evaluators/evaluator.py</code> <pre><code>async def async_evaluate(\n    self, \n    graph: Union[WorkFlowGraph, ActionGraph],\n    benchmark: Benchmark, \n    eval_mode: str = \"test\", \n    indices: Optional[List[int]] = None, \n    sample_k: Optional[int] = None, \n    seed: Optional[int] = None, \n    verbose: Optional[bool] = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Asynchronously evaluate the performance of the workflow on the benchmark.\n\n    Args:\n        graph (WorkFlowGraph or ActionGraph): The workflow to evaluate.\n        benchmark (Benchmark): The benchmark to evaluate the workflow on.\n        eval_mode (str): which split of the benchmark to evaluate the workflow on. Choices: [\"test\", \"dev\", \"train\"].\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n        verbose (bool, optional): Whether to print the evaluation progress. If not provided, the `self.verbose` will be used.\n\n    Returns:\n        dict: The average metrics of the workflow evaluation.\n    \"\"\"\n    # clear the evaluation records\n    self._evaluation_records.clear()\n    data = self._get_eval_data(benchmark=benchmark, eval_mode=eval_mode, indices=indices, sample_k=sample_k, seed=seed)\n\n    if not data:\n        logger.warning(\"No data to evaluate. Return an empty dictionary.\")\n        return {}\n\n    verbose = verbose if verbose is not None else self.verbose\n\n    # Create a semaphore to limit concurrent executions\n    sem = asyncio.Semaphore(self.num_workers)\n\n    async def process_with_semaphore(example):\n        async with sem:\n            try:\n                return await self._async_evaluate_single_example(\n                    graph=graph, \n                    example=example, \n                    benchmark=benchmark, \n                    **kwargs\n                )\n            except Exception as e:\n                logger.warning(f\"Async evaluation failed for example with semaphore: {str(e)}\")\n                return None\n\n    # Create tasks for concurrent execution with semaphore\n    tasks = [process_with_semaphore(example) for example in data]\n\n    # Execute all tasks with progress bar if verbose\n    if verbose:\n        results = await tqdm_asyncio.gather(\n            *tasks,\n            desc=f\"Evaluating {benchmark.name}\",\n            total=len(data)\n        )\n    else:\n        results = await asyncio.gather(*tasks)\n\n    return self._calculate_average_score(results)\n</code></pre>"},{"location":"api/memory.html","title":"\ud83d\udd87\ufe0f Memory","text":""},{"location":"api/memory.html#evoagentx.memory","title":"evoagentx.memory","text":""},{"location":"api/memory.html#evoagentx.memory.BaseMemory","title":"BaseMemory","text":"<pre><code>BaseMemory(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base class for memory implementations in the EvoAgentX framework.</p> <p>BaseMemory provides core functionality for storing, retrieving, and  filtering messages. It maintains a chronological list of messages while  also providing indices for efficient retrieval by action or workflow goal.</p> <p>Attributes:</p> Name Type Description <code>messages</code> <code>List[Message]</code> <p>List of stored Message objects.</p> <code>memory_id</code> <code>str</code> <p>Unique identifier for this memory instance.</p> <code>timestamp</code> <code>str</code> <p>Creation timestamp of this memory instance.</p> <code>capacity</code> <code>Optional[PositiveInt]</code> <p>Maximum number of messages that can be stored, or None for unlimited.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.size","title":"size  <code>property</code>","text":"<pre><code>size: int\n</code></pre> <p>Returns the current number of messages in memory.</p> <p>Returns:</p> Name Type Description <code>int</code> <code>int</code> <p>Number of messages currently stored.</p>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Initialize memory indices.</p> <p>Creates default dictionaries for indexing messages by action and workflow goal.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def init_module(self):\n    \"\"\"Initialize memory indices.\n\n    Creates default dictionaries for indexing messages by action and workflow goal.\n    \"\"\"\n    self._by_action = defaultdict(list)\n    self._by_wf_goal = defaultdict(list)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.clear","title":"clear","text":"<pre><code>clear()\n</code></pre> <p>Clear all messages from memory.</p> <p>Removes all messages and resets all indices.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def clear(self):\n    \"\"\"Clear all messages from memory.\n\n    Removes all messages and resets all indices.\n    \"\"\"\n    self.messages.clear()\n    self._by_action.clear()\n    self._by_wf_goal.clear()\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.remove_message","title":"remove_message","text":"<pre><code>remove_message(message: Message)\n</code></pre> <p>Remove a single message from memory.</p> <p>Removes the specified message from the main message list and all indices. If the message is not found in memory, no action is taken.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>The message to be removed. The message will be removed from     self.messages, self._by_action, and self._by_wf_goal.</p> required Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def remove_message(self, message: Message):\n    \"\"\"Remove a single message from memory.\n\n    Removes the specified message from the main message list and all indices.\n    If the message is not found in memory, no action is taken.\n\n    Args:\n        message: The message to be removed. The message will be removed from \n               self.messages, self._by_action, and self._by_wf_goal.\n    \"\"\"\n    if not message:\n        return\n    if message not in self.messages:\n        return\n    safe_remove(self.messages, message)\n    if self._by_action and not message.action:\n        safe_remove(self._by_action[message.action], message)\n    if self._by_wf_goal and not message.wf_goal:\n        safe_remove(self._by_wf_goal[message.wf_goal], message)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.add_message","title":"add_message","text":"<pre><code>add_message(message: Message)\n</code></pre> <p>Store a single message in memory.</p> <p>Adds the message to the main list and relevant indices if it's not already stored.</p> <p>Parameters:</p> Name Type Description Default <code>message</code> <code>Message</code> <p>the message to be stored.</p> required Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def add_message(self, message: Message):\n    \"\"\"Store a single message in memory.\n\n    Adds the message to the main list and relevant indices if it's not already stored.\n\n    Args:\n        message (Message): the message to be stored. \n    \"\"\"\n    if not message:\n        return\n    if message in self.messages:\n        return\n    self.messages.append(message)\n    if self._by_action and not message.action:\n        self._by_action[message.action].append(message)\n    if self._by_wf_goal and not message.wf_goal:\n        self._by_wf_goal[message.wf_goal].append(message)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.add_messages","title":"add_messages","text":"<pre><code>add_messages(messages: Union[Message, List[Message]], **kwargs)\n</code></pre> <p>store (a) message(s) to the memory. </p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>Union[Message, List[Message]]</code> <p>the input messages can be a single message or a list of message.</p> required Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def add_messages(self, messages: Union[Message, List[Message]], **kwargs):\n    \"\"\"\n    store (a) message(s) to the memory. \n\n    Args:\n        messages (Union[Message, List[Message]]): the input messages can be a single message or a list of message.\n    \"\"\"\n    if not isinstance(messages, list):\n        messages = [messages]\n    for message in messages:\n        self.add_message(message)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.get","title":"get","text":"<pre><code>get(n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>Retrieve recent messages from memory.</p> <p>Returns the most recent messages, up to the specified limit.</p> <p>Parameters:</p> Name Type Description Default <code>n</code> <code>int</code> <p>The maximum number of messages to return. If None, returns all messages.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused in base implementation).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of Message objects, ordered from oldest to newest.</p> <p>Raises:</p> Type Description <code>AssertionError</code> <p>If n is negative.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get(self, n: int=None, **kwargs) -&gt; List[Message]:\n    \"\"\"Retrieve recent messages from memory.\n\n    Returns the most recent messages, up to the specified limit.\n\n    Args: \n        n: The maximum number of messages to return. If None, returns all messages.\n        **kwargs (Any): Additional parameters (unused in base implementation).\n\n    Returns:\n        A list of Message objects, ordered from oldest to newest.\n\n    Raises:\n        AssertionError: If n is negative.\n    \"\"\"\n    assert n is None or n&gt;=0, \"n must be None or a positive int\"\n    messages = self.messages if n is None else self.messages[-n:]\n    return messages\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.get_by_type","title":"get_by_type","text":"<pre><code>get_by_type(data: Dict[str, list], key: str, n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>Retrieve a list of Message objects from a given data dictionary <code>data</code> based on a specified type <code>key</code>.</p> <p>This function looks up the value associated with <code>key</code> in the <code>data</code> dictionary, which should be a list of messages. It then returns a subset of these messages according to the specified parameters. If <code>n</code> is provided, it limits the number of messages returned; otherwise, it may return the entire list. Additional keyword arguments (**kwargs) can be used to further filter or process the resulting messages.</p> <p>Parameters:</p> Name Type Description Default <code>data</code> <code>Dict[str, list]</code> <p>A dictionary where keys are type strings and values are lists of messages.</p> required <code>key</code> <code>str</code> <p>The key in <code>data</code> identifying the specific list of messages to retrieve.</p> required <code>n</code> <code>int</code> <p>The maximum number of messages to return. If not provided, all messages under the given <code>key</code> may be returned.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters for filtering or processing the messages.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>List[Message]: A list of messages corresponding to the given <code>key</code>, possibly filtered or truncated according to <code>n</code> and other provided keyword arguments.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get_by_type(self, data: Dict[str, list], key: str, n: int = None, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    Retrieve a list of Message objects from a given data dictionary `data` based on a specified type `key`.\n\n    This function looks up the value associated with `key` in the `data` dictionary, which should be a list of messages. It then returns a subset of these messages according to the specified parameters.\n    If `n` is provided, it limits the number of messages returned; otherwise, it may return the entire list. Additional keyword arguments (**kwargs) can be used to further filter or process the resulting messages.\n\n    Args:\n        data (Dict[str, list]): A dictionary where keys are type strings and values are lists of messages.\n        key (str): The key in `data` identifying the specific list of messages to retrieve.\n        n (int, optional): The maximum number of messages to return. If not provided, all messages under the given `key` may be returned.\n        **kwargs (Any): Additional parameters for filtering or processing the messages.\n\n    Returns:\n        List[Message]: A list of messages corresponding to the given `key`, possibly filtered or truncated according to `n` and other provided keyword arguments.\n    \"\"\"\n    if not data or key not in data:\n        return []\n    assert n is None or n&gt;=0, \"n must be None or a positive int\"\n    messages = data[key] if n is None else data[key][-n:]\n    return messages\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.get_by_action","title":"get_by_action","text":"<pre><code>get_by_action(actions: Union[str, List[str]], n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>return messages triggered by <code>actions</code> in the memory. </p> <p>Parameters:</p> Name Type Description Default <code>actions</code> <code>Union[str, List[str]]</code> <p>A single action name or list of action names to filter by.</p> required <code>n</code> <code>int</code> <p>Maximum number of messages to return per action. If None, returns all matching messages.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused in base implementation).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of Message objects, sorted by timestamp.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get_by_action(self, actions: Union[str, List[str]], n: int=None, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    return messages triggered by `actions` in the memory. \n\n    Args:\n        actions: A single action name or list of action names to filter by.\n        n: Maximum number of messages to return per action. If None, returns all matching messages.\n        **kwargs (Any): Additional parameters (unused in base implementation).\n\n    Returns:\n        A list of Message objects, sorted by timestamp.\n    \"\"\"\n    if isinstance(actions, str):\n        actions = [actions]\n    messages = []\n    for action in actions:\n        messages.extend(self.get_by_type(self._by_action, key=action, n=n, **kwargs))\n    messages = Message.sort_by_timestamp(messages)\n    return messages\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.BaseMemory.get_by_wf_goal","title":"get_by_wf_goal","text":"<pre><code>get_by_wf_goal(wf_goals: Union[str, List[str]], n: int = None, **kwargs) -&gt; List[Message]\n</code></pre> <p>return messages related to <code>wf_goals</code> in the memory. </p> <p>Parameters:</p> Name Type Description Default <code>wf_goals</code> <code>Union[str, List[str]]</code> <p>A single workflow goal or list of workflow goals to filter by.</p> required <code>n</code> <code>int</code> <p>Maximum number of messages to return per workflow goal. If None, returns all matching messages.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional parameters (unused in base implementation).</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[Message]</code> <p>A list of Message objects, sorted by timestamp.</p> Source code in <code>evoagentx/memory/memory.py</code> <pre><code>def get_by_wf_goal(self, wf_goals: Union[str, List[str]], n: int=None, **kwargs) -&gt; List[Message]:\n    \"\"\"\n    return messages related to `wf_goals` in the memory. \n\n    Args:\n        wf_goals: A single workflow goal or list of workflow goals to filter by.\n        n: Maximum number of messages to return per workflow goal. If None, returns all matching messages.\n        **kwargs (Any): Additional parameters (unused in base implementation).\n\n    Returns:\n        A list of Message objects, sorted by timestamp.\n    \"\"\"\n    if isinstance(wf_goals, str):\n        wf_goals = [wf_goals]\n    messages = []\n    for wf_goal in wf_goals:\n        messages.append(self.get_by_type(self._by_wf_goal, key=wf_goal, n=n, **kwargs))\n    messages = Message.sort_by_timestamp(messages)\n    return messages\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.ShortTermMemory","title":"ShortTermMemory","text":"<pre><code>ShortTermMemory(**kwargs)\n</code></pre> <p>               Bases: <code>BaseMemory</code></p> <p>Short-term memory implementation.</p> <p>This class extends BaseMemory to represent a temporary, short-term memory storage. In the current implementation, it inherits all functionality from BaseMemory without modifications, but it provides a semantic distinction for different memory usage patterns in the framework.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.LongTermMemory","title":"LongTermMemory","text":"<pre><code>LongTermMemory(**kwargs)\n</code></pre> <p>               Bases: <code>BaseMemory</code></p> <p>Responsible for the management of raw data for long-term storage.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/memory.html#evoagentx.memory.MemoryManager","title":"MemoryManager","text":"<pre><code>MemoryManager(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>The Memory Manager is responsible for organizing and managing LongTerm Memory's data at a higher level. It gets data from LongTermMemory, then it processes the data, store the data in LongTermMemory,  and store the LongTermMemory through StorageHandler.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/models.html","title":"\ud83e\uddec Models","text":""},{"location":"api/models.html#evoagentx.models","title":"evoagentx.models","text":""},{"location":"api/models.html#evoagentx.models.LLMOutputParser","title":"LLMOutputParser","text":"<pre><code>LLMOutputParser(**kwargs)\n</code></pre> <p>               Bases: <code>Parser</code></p> <p>A basic parser for LLM-generated content.</p> <p>This parser stores the raw text generated by an LLM in the <code>.content</code> attribute and provides methods to extract structured data from this text using different parsing strategies.</p> <p>Attributes:</p> Name Type Description <code>content</code> <code>str</code> <p>The raw text generated by the LLM.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.get_attrs","title":"get_attrs  <code>classmethod</code>","text":"<pre><code>get_attrs(return_type: bool = False) -&gt; List[Union[str, tuple]]\n</code></pre> <p>Returns the attributes of the LLMOutputParser class.</p> <p>Excludes [\"class_name\", \"content\"] by default.</p> <p>Parameters:</p> Name Type Description Default <code>return_type</code> <code>bool</code> <p>Whether to return the type of the attributes along with their names.</p> <code>False</code> <p>Returns:</p> Type Description <code>List[Union[str, tuple]]</code> <p>If <code>return_type</code> is True, returns a list of tuples where each tuple contains </p> <code>List[Union[str, tuple]]</code> <p>the attribute name and its type. Otherwise, returns a list of attribute names.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef get_attrs(cls, return_type: bool = False) -&gt; List[Union[str, tuple]]:\n    \"\"\"Returns the attributes of the LLMOutputParser class.\n\n    Excludes [\"class_name\", \"content\"] by default.\n\n    Args:\n        return_type: Whether to return the type of the attributes along with their names.\n\n    Returns:\n        If `return_type` is True, returns a list of tuples where each tuple contains \n        the attribute name and its type. Otherwise, returns a list of attribute names.\n    \"\"\"\n    attrs = [] \n    exclude_attrs = [\"class_name\", \"content\"]\n    for field, field_info in cls.model_fields.items():\n        if field not in exclude_attrs:\n            if return_type:\n                field_type = get_type_name(field_info.annotation)\n                attrs.append((field, field_type))\n            else:\n                attrs.append(field)\n    return attrs\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.get_attr_descriptions","title":"get_attr_descriptions  <code>classmethod</code>","text":"<pre><code>get_attr_descriptions() -&gt; dict\n</code></pre> <p>Returns the attributes and their descriptions.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary mapping attribute names to their descriptions.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef get_attr_descriptions(cls) -&gt; dict:\n    \"\"\"Returns the attributes and their descriptions.\n\n    Returns:\n        A dictionary mapping attribute names to their descriptions.\n    \"\"\"\n    attrs = cls.get_attrs()\n    results = {} \n    for field_name, field_info in cls.model_fields.items():\n        if field_name not in attrs:\n            continue\n        field_desc = field_info.description if field_info.description is not None else \"None\"\n        results[field_name] = field_desc\n    return results\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.get_content_data","title":"get_content_data  <code>classmethod</code>","text":"<pre><code>get_content_data(content: str, parse_mode: str = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; dict\n</code></pre> <p>Parses LLM-generated content into a dictionary.</p> <p>This method takes content from an LLM response and converts it to a structured dictionary based on the specified parsing mode.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The content to parse.</p> required <code>parse_mode</code> <code>str</code> <p>The mode to parse the content. Must be one of: - 'str': Assigns the raw text content to all attributes of the parser.  - 'json': Extracts and parses JSON objects from LLM output. It will return a dictionary parsed from the first valid JSON string. - 'xml': Parses content using XML tags. It will return a dictionary parsed from the XML tags. - 'title': Parses content with Markdown-style headings. - 'custom': Uses custom parsing logic. Requires providing <code>parse_func</code> parameter as a custom parsing function.</p> <code>'json'</code> <code>parse_func</code> <code>Optional[Callable]</code> <p>The function to parse the content, only valid when parse_mode is 'custom'.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parsing function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>dict</code> <p>The parsed content as a dictionary.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parse_mode is invalid or if parse_func is not provided when parse_mode is 'custom'.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef get_content_data(cls, content: str, parse_mode: str = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; dict:\n    \"\"\"Parses LLM-generated content into a dictionary.\n\n    This method takes content from an LLM response and converts it to a structured\n    dictionary based on the specified parsing mode.\n\n    Args:\n        content: The content to parse.\n        parse_mode: The mode to parse the content. Must be one of:\n            - 'str': Assigns the raw text content to all attributes of the parser. \n            - 'json': Extracts and parses JSON objects from LLM output. It will return a dictionary parsed from the first valid JSON string.\n            - 'xml': Parses content using XML tags. It will return a dictionary parsed from the XML tags.\n            - 'title': Parses content with Markdown-style headings.\n            - 'custom': Uses custom parsing logic. Requires providing `parse_func` parameter as a custom parsing function.\n        parse_func: The function to parse the content, only valid when parse_mode is 'custom'.\n        **kwargs (Any): Additional arguments passed to the parsing function.\n\n    Returns:\n        The parsed content as a dictionary.\n\n    Raises:\n        ValueError: If parse_mode is invalid or if parse_func is not provided when parse_mode is 'custom'.\n    \"\"\"\n    attrs = cls.get_attrs()\n    if len(attrs) &lt;= 0:\n        return {} \n\n    if parse_mode == \"str\":\n        parse_func = cls._parse_str_content\n    elif parse_mode == \"json\":\n        parse_func = cls._parse_json_content\n    elif parse_mode == \"xml\":\n        parse_func = cls._parse_xml_content\n    elif parse_mode == \"title\":\n        parse_func = cls._parse_title_content\n    elif parse_mode == \"custom\":\n        if parse_func is None:\n            raise ValueError(\"`parse_func` must be provided when `parse_mode` is 'custom'.\")\n        # obtain the function inputs\n        signature = inspect.signature(parse_func)\n        if \"content\" not in signature.parameters:\n            raise ValueError(\"`parse_func` must have an input argument `content`.\")\n\n        func_args = {}\n        func_args[\"content\"] = content\n        for param_name, param in signature.parameters.items():\n            if param_name == \"content\":\n                continue  # Already set\n            if param_name in kwargs:\n                func_args[param_name] = kwargs[param_name]\n        data = parse_func(**func_args)\n        if not isinstance(data, dict):\n            raise ValueError(f\"The output of `parse_func` must be a dictionary, but found {type(data)}.\")\n        return data\n    else:\n        raise ValueError(f\"Invalid value '{parse_mode}' detected for `parse_mode`. Available choices: {PARSER_VALID_MODE}\")\n    data = parse_func(content=content, **kwargs)\n    return data\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.parse","title":"parse  <code>classmethod</code>","text":"<pre><code>parse(content: str, parse_mode: str = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; LLMOutputParser\n</code></pre> <p>Parses LLM-generated text into a structured parser instance.</p> <p>This is the main method for creating parser instances from LLM output.</p> <p>Parameters:</p> Name Type Description Default <code>content</code> <code>str</code> <p>The text generated by the LLM.</p> required <code>parse_mode</code> <code>str</code> <p>The mode to parse the content, must be one of: - 'str': Assigns the raw text content to all attributes of the parser.  - 'json': Extracts and parses JSON objects from LLM output. Uses the first valid JSON string to create an instance of LLMOutputParser. - 'xml': Parses content using XML tags. Uses the XML tags to create an instance of LLMOutputParser. - 'title': Parses content with Markdown-style headings. Uses the Markdown-style headings to create an instance of LLMOutputParser. The default title format is \"## {title}\", you can change it by providing <code>title_format</code> parameter, which should be a string that contains <code>{title}</code> placeholder.  - 'custom': Uses custom parsing logic. Requires providing <code>parse_func</code> parameter as a custom parsing function. The <code>parse_func</code> must have a parameter named <code>content</code> and return a dictionary where the keys are the attribute names and the values are the parsed data. </p> <code>'json'</code> <code>parse_func</code> <code>Optional[Callable]</code> <p>The function to parse the content, only valid when <code>parse_mode</code> is 'custom'.</p> <code>None</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to parsing functions, such as: - <code>title_format</code> for <code>parse_mode=\"title\"</code>.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMOutputParser</code> <p>An instance of LLMOutputParser containing the parsed data.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If parse_mode is invalid or if content is not a string.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@classmethod\ndef parse(cls, content: str, parse_mode: str = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; \"LLMOutputParser\":\n    \"\"\"Parses LLM-generated text into a structured parser instance.\n\n    This is the main method for creating parser instances from LLM output.\n\n    Args:\n        content: The text generated by the LLM.\n        parse_mode: The mode to parse the content, must be one of:\n            - 'str': Assigns the raw text content to all attributes of the parser. \n            - 'json': Extracts and parses JSON objects from LLM output. Uses the first valid JSON string to create an instance of LLMOutputParser.\n            - 'xml': Parses content using XML tags. Uses the XML tags to create an instance of LLMOutputParser.\n            - 'title': Parses content with Markdown-style headings. Uses the Markdown-style headings to create an instance of LLMOutputParser. The default title format is \"## {title}\", you can change it by providing `title_format` parameter, which should be a string that contains `{title}` placeholder. \n            - 'custom': Uses custom parsing logic. Requires providing `parse_func` parameter as a custom parsing function. The `parse_func` must have a parameter named `content` and return a dictionary where the keys are the attribute names and the values are the parsed data. \n        parse_func: The function to parse the content, only valid when `parse_mode` is 'custom'.\n        **kwargs (Any): Additional arguments passed to parsing functions, such as:\n            - `title_format` for `parse_mode=\"title\"`.\n\n    Returns:\n        An instance of LLMOutputParser containing the parsed data.\n\n    Raises:\n        ValueError: If parse_mode is invalid or if content is not a string.\n    \"\"\"\n    if parse_mode not in PARSER_VALID_MODE:\n        raise ValueError(f\"'{parse_mode}' is an invalid value for `parse_mode`. Available choices: {PARSER_VALID_MODE}.\")\n    if not isinstance(content, str):\n        raise ValueError(f\"The input to {cls.__name__}.parse should be a str, but found {type(content)}.\")\n    data = cls.get_content_data(content=content, parse_mode=parse_mode, parse_func=parse_func, **kwargs)\n    data.update({\"content\": content})\n    parser = cls.from_dict(data, **kwargs)\n    # parser.content = content\n    return parser\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.__str__","title":"__str__","text":"<pre><code>__str__() -&gt; str\n</code></pre> <p>Returns a string representation of the parser.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __str__(self) -&gt; str:\n    \"\"\"\n    Returns a string representation of the parser.\n    \"\"\"\n    return self.to_str()\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.to_str","title":"to_str","text":"<pre><code>to_str(**kwargs) -&gt; str\n</code></pre> <p>Converts the parser to a string.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def to_str(self, **kwargs) -&gt; str:\n    \"\"\"\n    Converts the parser to a string.\n    \"\"\"\n    return self.content\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LLMOutputParser.get_structured_data","title":"get_structured_data","text":"<pre><code>get_structured_data() -&gt; dict\n</code></pre> <p>Extracts structured data from the parser.</p> <p>Returns:</p> Type Description <code>dict</code> <p>A dictionary containing only the defined attributes and their values,</p> <code>dict</code> <p>excluding metadata like class_name.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def get_structured_data(self) -&gt; dict:\n    \"\"\"Extracts structured data from the parser.\n\n    Returns:\n        A dictionary containing only the defined attributes and their values,\n        excluding metadata like class_name.\n    \"\"\"\n    attrs = type(self).get_attrs()\n    data = self.to_dict(ignore=[\"class_name\"])\n    # structured_data = {attr: data[attr] for attr in attrs}\n    structured_data = {key: value for key, value in data.items() if key in attrs}\n    return structured_data\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseConfig","title":"BaseConfig","text":"<pre><code>BaseConfig(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base configuration class that serves as parent for all configuration classes.</p> <p>A config should inherit BaseConfig and specify the attributes and their types.  Otherwise this will be an empty config.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseConfig.save","title":"save","text":"<pre><code>save(path: str, **kwargs) -&gt; str\n</code></pre> <p>Save configuration to the specified path.</p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The file path to save the configuration</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments passed to save_module method</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The path where the file was saved</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def save(self, path: str, **kwargs)-&gt; str:\n\n    \"\"\"Save configuration to the specified path.\n\n    Args:\n        path: The file path to save the configuration\n        **kwargs (Any): Additional keyword arguments passed to save_module method\n\n    Returns:\n        str: The path where the file was saved\n    \"\"\"\n    return super().save_module(path, **kwargs)\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseConfig.get_config_params","title":"get_config_params","text":"<pre><code>get_config_params() -&gt; List[str]\n</code></pre> <p>Get a list of configuration parameters.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of configuration parameter names, excluding 'class_name'</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_config_params(self) -&gt; List[str]:\n    \"\"\"Get a list of configuration parameters.\n\n    Returns:\n        List[str]: List of configuration parameter names, excluding 'class_name'\n    \"\"\"\n    config_params = list(type(self).model_fields.keys())\n    config_params.remove(\"class_name\")\n    return config_params\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseConfig.get_set_params","title":"get_set_params","text":"<pre><code>get_set_params(ignore: List[str] = []) -&gt; dict\n</code></pre> <p>Get a dictionary of explicitly set parameters.</p> <p>Parameters:</p> Name Type Description Default <code>ignore</code> <code>List[str]</code> <p>List of parameter names to ignore</p> <code>[]</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters</p> Source code in <code>evoagentx/core/base_config.py</code> <pre><code>def get_set_params(self, ignore: List[str] = []) -&gt; dict:\n    \"\"\"Get a dictionary of explicitly set parameters.\n\n    Args:\n        ignore: List of parameter names to ignore\n\n    Returns:\n        dict: Dictionary of explicitly set parameters, excluding 'class_name' and ignored parameters\n    \"\"\"\n    explicitly_set_fields = {field: getattr(self, field) for field in self.model_fields_set}\n    if self.kwargs:\n        explicitly_set_fields.update(self.kwargs)\n    for field in ignore:\n        explicitly_set_fields.pop(field, None)\n    explicitly_set_fields.pop(\"class_name\", None)\n    return explicitly_set_fields\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LiteLLM","title":"LiteLLM","text":"<pre><code>LiteLLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>OpenAILLM</code></p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LiteLLM.init_model","title":"init_model","text":"<pre><code>init_model()\n</code></pre> <p>Initialize the model based on the configuration.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>def init_model(self):\n    \"\"\"\n    Initialize the model based on the configuration.\n    \"\"\"\n    # Check if llm_type is correct\n    if self.config.llm_type != \"LiteLLM\":\n        raise ValueError(\"llm_type must be 'LiteLLM'\")\n\n    # Set model and extract the company name\n    self.model = self.config.model\n    self.api_base = self.config.api_base  # save api_base\n    self.api_key = self.config.api_key\n    # company = self.model.split(\"/\")[0] if \"/\" in self.model else \"openai\"\n    company = infer_litellm_company_from_model(self.model)\n\n    if self.config.is_local or company == \"local\":  # update support local model\n        if not self.api_base:\n            raise ValueError(\"api_base is required for local models in LiteLLMConfig\")\n        # local llm doesn't need API key\n        litellm.api_base = self.api_base  # set litellm global api_base\n        litellm.api_key = self.api_key\n    else:\n        # Set environment variables based on the company\n        if company == \"openai\":\n            if not self.config.openai_key:\n                raise ValueError(\"OpenAI API key is required for OpenAI models. You should set `openai_key` in LiteLLMConfig\")\n            os.environ[\"OPENAI_API_KEY\"] = self.config.openai_key\n        elif company == \"azure\":\n            if not self.config.azure_key or not self.config.azure_endpoint:\n                raise ValueError(\"Azure OpenAI key and endpoint are required for Azure models. You should set `azure_key` and `azure_endpoint` in LiteLLMConfig\")\n            os.environ[\"AZURE_API_KEY\"] = self.config.azure_key\n            os.environ[\"AZURE_API_BASE\"] = self.config.azure_endpoint\n            if self.config.api_version:\n                os.environ[\"AZURE_API_VERSION\"] = self.config.api_version\n        elif company == \"deepseek\":\n            if not self.config.deepseek_key:\n                raise ValueError(\"DeepSeek API key is required for DeepSeek models. You should set `deepseek_key` in LiteLLMConfig\")\n            os.environ[\"DEEPSEEK_API_KEY\"] = self.config.deepseek_key\n        elif company == \"anthropic\":\n            if not self.config.anthropic_key:\n                raise ValueError(\"Anthropic API key is required for Anthropic models. You should set `anthropic_key` in LiteLLMConfig\")\n            os.environ[\"ANTHROPIC_API_KEY\"] = self.config.anthropic_key\n        elif company == \"gemini\":\n            if not self.config.gemini_key:\n                raise ValueError(\"Gemini API key is required for Gemini models. You should set `gemini_key` in LiteLLMConfig\")\n            os.environ[\"GEMINI_API_KEY\"] = self.config.gemini_key \n        elif company == \"meta_llama\":\n            if not self.config.meta_llama_key:\n                raise ValueError(\"Meta Llama API key is required for Meta Llama models. You should set `meta_llama_key` in LiteLLMConfig\")\n            os.environ[\"LLAMA_API_KEY\"] = self.config.meta_llama_key\n        elif company == \"openrouter\":\n            if not self.config.openrouter_key:\n                raise ValueError(\"OpenRouter API key is required for OpenRouter models. You should set `openrouter_key` in LiteLLMConfig. You can also set `openrouter_base` in LiteLLMConfig to use a custom base URL [optional]\")\n            os.environ[\"OPENROUTER_API_KEY\"] = self.config.openrouter_key\n            os.environ[\"OPENROUTER_API_BASE\"] = self.config.openrouter_base # [optional]\n        elif company == \"perplexity\":\n            if not self.config.perplexity_key:\n                raise ValueError(\"Perplexity API key is required for Perplexity models. You should set `perplexity_key` in LiteLLMConfig\")\n            os.environ[\"PERPLEXITYAI_API_KEY\"] = self.config.perplexity_key\n        elif company == \"groq\":\n            if not self.config.groq_key:\n                raise ValueError(\"Groq API key is required for Groq models. You should set `groq_key` in LiteLLMConfig\")\n            os.environ[\"GROQ_API_KEY\"] = self.config.groq_key\n        else:\n            raise ValueError(f\"Unsupported company: {company}\")\n\n    self._default_ignore_fields = [\n        \"llm_type\", \"output_response\", \"openai_key\", \"deepseek_key\", \"anthropic_key\", \n        \"gemini_key\", \"meta_llama_key\", \"openrouter_key\", \"openrouter_base\", \"perplexity_key\", \n        \"groq_key\", \"api_base\", \"is_local\", \"azure_endpoint\", \"azure_key\", \"api_version\", \"api_key\"\n    ] # parameters in LiteLLMConfig that are not LiteLLM models' input parameters \n</code></pre>"},{"location":"api/models.html#evoagentx.models.LiteLLM.single_generate","title":"single_generate","text":"<pre><code>single_generate(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generate a single response using the completion function.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>A list of dictionaries representing the conversation history.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters to be passed to the <code>completion</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing the model's response.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef single_generate(self, messages: List[dict], **kwargs) -&gt; str:\n\n    \"\"\"\n    Generate a single response using the completion function.\n\n    Args: \n        messages (List[dict]): A list of dictionaries representing the conversation history.\n        **kwargs (Any): Additional parameters to be passed to the `completion` function.\n\n    Returns: \n        str: A string containing the model's response.\n    \"\"\"\n    stream = kwargs[\"stream\"] if \"stream\" in kwargs else self.config.stream\n    output_response = kwargs[\"output_response\"] if \"output_response\" in kwargs else self.config.output_response\n\n    try:\n        completion_params = self.get_completion_params(**kwargs)\n        company = infer_litellm_company_from_model(self.model)\n        if self.config.is_local or company == \"local\":  # update save api_base for local model\n            completion_params[\"api_base\"] = self.api_base\n        elif company == \"azure\":  # Add Azure OpenAI specific parameters\n            completion_params[\"api_base\"] = self.config.azure_endpoint\n            completion_params[\"api_version\"] = self.config.api_version\n            completion_params[\"api_key\"] = self.config.azure_key\n        response = completion(messages=messages, **completion_params)\n        if stream:\n            output = self.get_stream_output(response, output_response=output_response)\n            cost = self._stream_cost(messages=messages, output=output)\n        else:\n            output: str = self.get_completion_output(response=response, output_response=output_response)\n            cost = self._completion_cost(response=response)\n        self._update_cost(cost=cost)\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during single_generate: {str(e)}\")\n\n    return output\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LiteLLM.batch_generate","title":"batch_generate","text":"<pre><code>batch_generate(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Generate responses for a batch of messages.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>A list of message lists, where each sublist represents a conversation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters to be passed to the <code>completion</code> function.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: A list of responses for each conversation.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>def batch_generate(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"\n    Generate responses for a batch of messages.\n\n    Args: \n        batch_messages (List[List[dict]]): A list of message lists, where each sublist represents a conversation.\n        **kwargs (Any): Additional parameters to be passed to the `completion` function.\n\n    Returns: \n        List[str]: A list of responses for each conversation.\n    \"\"\"\n    results = []\n    for messages in batch_messages:\n        response = self.single_generate(messages, **kwargs)\n        results.append(response)\n    return results\n</code></pre>"},{"location":"api/models.html#evoagentx.models.LiteLLM.single_generate_async","title":"single_generate_async  <code>async</code>","text":"<pre><code>single_generate_async(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generate a single response using the async completion function.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>A list of dictionaries representing the conversation history.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional parameters to be passed to the <code>completion</code> function.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>A string containing the model's response.</p> Source code in <code>evoagentx/models/litellm_model.py</code> <pre><code>async def single_generate_async(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"\n    Generate a single response using the async completion function.\n\n    Args: \n        messages (List[dict]): A list of dictionaries representing the conversation history.\n        **kwargs (Any): Additional parameters to be passed to the `completion` function.\n\n    Returns: \n        str: A string containing the model's response.\n    \"\"\"\n    stream = kwargs[\"stream\"] if \"stream\" in kwargs else self.config.stream\n    output_response = kwargs[\"output_response\"] if \"output_response\" in kwargs else self.config.output_response\n\n    try:\n        completion_params = self.get_completion_params(**kwargs)\n        company = infer_litellm_company_from_model(self.model)\n        if self.config.is_local or company == \"local\":  # add api base for local model\n            completion_params[\"api_base\"] = self.api_base\n        elif company == \"azure\":  # Add Azure OpenAI specific parameters\n            completion_params[\"api_base\"] = self.config.azure_endpoint\n            completion_params[\"api_version\"] = self.config.api_version\n            completion_params[\"api_key\"] = self.config.azure_key\n        response = await acompletion(messages=messages, **completion_params)\n        if stream:\n            if hasattr(response, \"__aiter__\"):\n                output = await self.get_stream_output_async(response, output_response=output_response)\n            else:\n                output = self.get_stream_output(response, output_response=output_response)\n            cost = self._stream_cost(messages=messages, output=output)\n        else:\n            output: str = self.get_completion_output(response=response, output_response=output_response)\n            cost = self._completion_cost(response=response)\n        self._update_cost(cost=cost)\n    except Exception as e:\n        raise RuntimeError(f\"Error during single_generate_async: {str(e)}\")\n\n    return output\n</code></pre>"},{"location":"api/models.html#evoagentx.models.OpenAILLM","title":"OpenAILLM","text":"<pre><code>OpenAILLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLLM</code></p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models.html#evoagentx.models.OpenAILLM.get_stream_output","title":"get_stream_output","text":"<pre><code>get_stream_output(response: Stream, output_response: bool = True) -&gt; str\n</code></pre> <p>Process stream response and return the complete output.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Stream</code> <p>The stream response from OpenAI</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response in real-time</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete output text</p> Source code in <code>evoagentx/models/openai_model.py</code> <pre><code>def get_stream_output(self, response: Stream, output_response: bool=True) -&gt; str:\n    \"\"\"\n    Process stream response and return the complete output.\n\n    Args:\n        response: The stream response from OpenAI\n        output_response: Whether to print the response in real-time\n\n    Returns:\n        str: The complete output text\n    \"\"\"\n    output = \"\"\n    for chunk in response:\n        content = chunk.choices[0].delta.content\n        if content:\n            if output_response:\n                print(content, end=\"\", flush=True)\n            output += content\n    if output_response:\n        print(\"\")\n    return output\n</code></pre>"},{"location":"api/models.html#evoagentx.models.OpenAILLM.get_stream_output_async","title":"get_stream_output_async  <code>async</code>","text":"<pre><code>get_stream_output_async(response, output_response: bool = False) -&gt; str\n</code></pre> <p>Process async stream response and return the complete output.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>AsyncIterator[ChatCompletionChunk]</code> <p>The async stream response from OpenAI</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response in real-time</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete output text</p> Source code in <code>evoagentx/models/openai_model.py</code> <pre><code>async def get_stream_output_async(self, response, output_response: bool = False) -&gt; str:\n    \"\"\"\n    Process async stream response and return the complete output.\n\n    Args:\n        response (AsyncIterator[ChatCompletionChunk]): The async stream response from OpenAI\n        output_response (bool): Whether to print the response in real-time\n\n\n    Returns:\n        str: The complete output text\n    \"\"\"\n    output = \"\"\n    async for chunk in response:\n        content = chunk.choices[0].delta.content\n        if content:\n            if output_response:\n                print(content, end=\"\", flush=True)\n            output += content\n    if output_response:\n        print(\"\")\n    return output\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM","title":"BaseLLM","text":"<pre><code>BaseLLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>ABC</code></p> <p>Abstract base class for Large Language Model implementations.</p> <p>This class defines the interface that all LLM implementations must follow, providing methods for generating text, formatting messages, and parsing output.</p> <p>Attributes:</p> Name Type Description <code>config</code> <p>Configuration for the LLM.</p> <code>kwargs</code> <p>Additional keyword arguments provided during initialization.</p> <p>Initializes the LLM with configuration.</p> <p>Parameters:</p> Name Type Description Default <code>config</code> <code>LLMConfig</code> <p>Configuration object for the LLM.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments.</p> <code>{}</code> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.init_model","title":"init_model  <code>abstractmethod</code>","text":"<pre><code>init_model()\n</code></pre> <p>Initializes the underlying model.</p> <p>This method should be implemented by subclasses to set up the actual LLM.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef init_model(self):\n    \"\"\"Initializes the underlying model.\n\n    This method should be implemented by subclasses to set up the actual LLM.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.__deepcopy__","title":"__deepcopy__","text":"<pre><code>__deepcopy__(memo) -&gt; BaseLLM\n</code></pre> <p>Handles deep copying of the LLM instance.</p> <p>Returns the same instance when deepcopy is called, as LLM instances often cannot be meaningfully deep-copied.</p> <p>Parameters:</p> Name Type Description Default <code>memo</code> <code>Dict[int, Any]</code> <p>Memo dictionary used by the deepcopy process.</p> required <p>Returns:</p> Type Description <code>BaseLLM</code> <p>The same LLM instance.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __deepcopy__(self, memo) -&gt; \"BaseLLM\":\n    \"\"\"Handles deep copying of the LLM instance.\n\n    Returns the same instance when deepcopy is called, as LLM instances\n    often cannot be meaningfully deep-copied.\n\n    Args:\n        memo (Dict[int, Any]): Memo dictionary used by the deepcopy process.\n\n    Returns:\n        The same LLM instance.\n    \"\"\"\n    # return the same instance when deepcopy\n    memo[id(self)] = self\n    return self\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.formulate_messages","title":"formulate_messages  <code>abstractmethod</code>","text":"<pre><code>formulate_messages(prompts: List[str], system_messages: Optional[List[str]] = None) -&gt; List[List[dict]]\n</code></pre> <p>Converts input prompts into the chat format compatible with different LLMs.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[str]</code> <p>A list of user prompts that need to be converted.</p> required <code>system_messages</code> <code>Optional[List[str]]</code> <p>An optional list of system messages that provide instructions or context to the model.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>A list of message lists, where each inner list contains messages in the chat format required by LLMs.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef formulate_messages(self, prompts: List[str], system_messages: Optional[List[str]] = None) -&gt; List[List[dict]]:\n    \"\"\"Converts input prompts into the chat format compatible with different LLMs.\n\n    Args:\n        prompts: A list of user prompts that need to be converted.\n        system_messages: An optional list of system messages that provide instructions or context to the model.\n\n    Returns:\n        A list of message lists, where each inner list contains messages in the chat format required by LLMs. \n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.single_generate","title":"single_generate  <code>abstractmethod</code>","text":"<pre><code>single_generate(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generates LLM output for a single set of messages.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>The input messages to the LLM in chat format.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated output text from the LLM.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef single_generate(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"Generates LLM output for a single set of messages.\n\n    Args:\n        messages: The input messages to the LLM in chat format.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        The generated output text from the LLM.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.batch_generate","title":"batch_generate  <code>abstractmethod</code>","text":"<pre><code>batch_generate(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Generates outputs for a batch of message sets.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>A list of message lists, where each inner list contains messages for a single generation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of generated outputs from the LLM, one for each input message set.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>@abstractmethod\ndef batch_generate(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"Generates outputs for a batch of message sets.\n\n    Args: \n        batch_messages: A list of message lists, where each inner list contains messages for a single generation.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        A list of generated outputs from the LLM, one for each input message set.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.single_generate_async","title":"single_generate_async  <code>async</code>","text":"<pre><code>single_generate_async(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Asynchronously generates LLM output for a single set of messages.</p> <p>This default implementation wraps the synchronous method in an async executor. Subclasses should override this for true async implementation if supported.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>The input messages to the LLM in chat format.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>str</code> <p>The generated output text from the LLM.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>async def single_generate_async(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"Asynchronously generates LLM output for a single set of messages.\n\n    This default implementation wraps the synchronous method in an async executor.\n    Subclasses should override this for true async implementation if supported.\n\n    Args:\n        messages: The input messages to the LLM in chat format.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        The generated output text from the LLM.\n    \"\"\"\n    # Default implementation for backward compatibility\n    loop = asyncio.get_event_loop()\n    result = await loop.run_in_executor(None, self.single_generate, messages, **kwargs)\n    return result\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.batch_generate_async","title":"batch_generate_async  <code>async</code>","text":"<pre><code>batch_generate_async(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Asynchronously generates outputs for a batch of message sets.</p> <p>This default implementation runs each generation as a separate async task. Subclasses should override this for more efficient async batching if supported.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>A list of message lists, where each inner list contains messages for a single generation.</p> required <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments for generation settings.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>A list of generated outputs from the LLM, one for each input message set.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>async def batch_generate_async(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"Asynchronously generates outputs for a batch of message sets.\n\n    This default implementation runs each generation as a separate async task.\n    Subclasses should override this for more efficient async batching if supported.\n\n    Args: \n        batch_messages: A list of message lists, where each inner list contains messages for a single generation.\n        **kwargs (Any): Additional keyword arguments for generation settings.\n\n    Returns:\n        A list of generated outputs from the LLM, one for each input message set.\n    \"\"\"\n    # Default implementation for backward compatibility\n    tasks = [self.single_generate_async(messages, **kwargs) for messages in batch_messages]\n    return await asyncio.gather(*tasks)\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.parse_generated_text","title":"parse_generated_text","text":"<pre><code>parse_generated_text(text: str, parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; LLMOutputParser\n</code></pre> <p>Parses generated text into a structured output using a parser.</p> <p>Parameters:</p> Name Type Description Default <code>text</code> <code>str</code> <p>The text generated by the LLM.</p> required <code>parser</code> <code>Optional[Type[LLMOutputParser]]</code> <p>An LLMOutputParser class to use for parsing. If None, the default LLMOutputParser is used.</p> <code>None</code> <code>parse_mode</code> <code>Optional[str]</code> <p>The mode to use for parsing, must be the <code>parse_mode</code> supported by the <code>parser</code>. </p> <code>'json'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parser.</p> <code>{}</code> <p>Returns:</p> Type Description <code>LLMOutputParser</code> <p>An LLMOutputParser instance containing the parsed data.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def parse_generated_text(self, text: str, parser: Optional[Type[LLMOutputParser]]=None, parse_mode: Optional[str] = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; LLMOutputParser:\n    \"\"\"Parses generated text into a structured output using a parser.\n\n    Args: \n        text: The text generated by the LLM.\n        parser: An LLMOutputParser class to use for parsing. If None, the default LLMOutputParser is used.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        **kwargs (Any): Additional arguments passed to the parser.\n\n    Returns:\n        An LLMOutputParser instance containing the parsed data.\n    \"\"\"\n    if not parser:\n        parser = LLMOutputParser\n    return parser.parse(text, parse_mode=parse_mode, parse_func=parse_func)\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.parse_generated_texts","title":"parse_generated_texts","text":"<pre><code>parse_generated_texts(texts: List[str], parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; List[LLMOutputParser]\n</code></pre> <p>Parses multiple generated texts into structured outputs.</p> <p>Parameters:</p> Name Type Description Default <code>texts</code> <code>List[str]</code> <p>A list of texts generated by the LLM.</p> required <code>parser</code> <code>Optional[Type[LLMOutputParser]]</code> <p>An LLMOutputParser class to use for parsing.</p> <code>None</code> <code>parse_mode</code> <code>Optional[str]</code> <p>The mode to use for parsing, must be the <code>parse_mode</code> supported by the <code>parser</code>. </p> <code>'json'</code> <code>**kwargs</code> <code>Any</code> <p>Additional arguments passed to the parser.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[LLMOutputParser]</code> <p>A list of LLMOutputParser instances containing the parsed data.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def parse_generated_texts(self, texts: List[str], parser: Optional[Type[LLMOutputParser]]=None, parse_mode: Optional[str] = \"json\", parse_func: Optional[Callable] = None, **kwargs) -&gt; List[LLMOutputParser]:\n    \"\"\"Parses multiple generated texts into structured outputs.\n\n    Args:\n        texts: A list of texts generated by the LLM.\n        parser: An LLMOutputParser class to use for parsing.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        **kwargs (Any): Additional arguments passed to the parser.\n\n    Returns:\n        A list of LLMOutputParser instances containing the parsed data.\n    \"\"\"\n    parsed_results = [self.parse_generated_text(text=text, parser=parser, parse_mode=parse_mode, parse_func=parse_func, **kwargs) for text in texts]\n    return parsed_results\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.generate","title":"generate","text":"<pre><code>generate(prompt: Optional[Union[str, List[str]]] = None, system_message: Optional[Union[str, List[str]]] = None, messages: Optional[Union[List[dict], List[List[dict]]]] = None, parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]\n</code></pre> <p>Generates LLM output(s) and parses the result(s).</p> <p>This is the main method for generating text with the LLM. It handles both single and batch generation, and automatically parses the outputs.</p> <p>Parameters:</p> Name Type Description Default <code>prompt</code> <code>Optional[Union[str, List[str]]]</code> <p>Input prompt(s) to the LLM.</p> <code>None</code> <code>system_message</code> <code>Optional[Union[str, List[str]]]</code> <p>System message(s) for the LLM.</p> <code>None</code> <code>messages</code> <code>Optional[Union[List[dict], List[List[dict]]]]</code> <p>Chat message(s) for the LLM, already in the required format (either <code>prompt</code> or <code>messages</code> must be provided).</p> <code>None</code> <code>parser</code> <code>Optional[Type[LLMOutputParser]]</code> <p>Parser class to use for processing the output.</p> <code>None</code> <code>parse_mode</code> <code>Optional[str]</code> <p>The mode to use for parsing, must be the <code>parse_mode</code> supported by the <code>parser</code>. </p> <code>'json'</code> <code>**kwargs</code> <code>Any</code> <p>Additional generation configuration parameters.</p> <code>{}</code> <p>Returns:</p> Type Description <code>Union[LLMOutputParser, List[LLMOutputParser]]</code> <p>For single generation: An LLMOutputParser instance.</p> <code>Union[LLMOutputParser, List[LLMOutputParser]]</code> <p>For batch generation: A list of LLMOutputParser instances.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the input format is invalid.</p> Note <p>Either prompt or messages must be provided. If both or neither is provided, an error will be raised.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"Generates LLM output(s) and parses the result(s).\n\n    This is the main method for generating text with the LLM. It handles both\n    single and batch generation, and automatically parses the outputs.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        **kwargs (Any): Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n\n    Raises:\n        ValueError: If the input format is invalid.\n\n    Note:\n        Either prompt or messages must be provided. If both or neither is provided,\n        an error will be raised.\n    \"\"\"\n    prepared_messages, single_generate = self._prepare_messages(prompt, system_message, messages)\n    if not prepared_messages:  # Handle empty messages case\n        return []\n\n    generated_texts = self.batch_generate(batch_messages=prepared_messages, **kwargs)\n    parsed_outputs = self.parse_generated_texts(texts=generated_texts, parser=parser, parse_mode=parse_mode, parse_func=parse_func, **kwargs)\n    return parsed_outputs[0] if single_generate else parsed_outputs\n</code></pre>"},{"location":"api/models.html#evoagentx.models.BaseLLM.async_generate","title":"async_generate  <code>async</code>","text":"<pre><code>async_generate(prompt: Optional[Union[str, List[str]]] = None, system_message: Optional[Union[str, List[str]]] = None, messages: Optional[Union[List[dict], List[List[dict]]]] = None, parser: Optional[Type[LLMOutputParser]] = None, parse_mode: Optional[str] = 'json', parse_func: Optional[Callable] = None, **kwargs) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]\n</code></pre> <p>Asynchronously generates LLM output(s) and parses the result(s).</p> <p>This is the async version of the generate method. It works identically but performs the generation asynchronously.</p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>async def async_generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"Asynchronously generates LLM output(s) and parses the result(s).\n\n    This is the async version of the generate method. It works identically but\n    performs the generation asynchronously.\n    \"\"\"\n    prepared_messages, single_generate = self._prepare_messages(prompt, system_message, messages)\n    if not prepared_messages:  # Handle empty messages case\n        return []\n\n    generated_texts = await self.batch_generate_async(batch_messages=prepared_messages, **kwargs)\n    parsed_outputs = self.parse_generated_texts(texts=generated_texts, parser=parser, parse_mode=parse_mode, parse_func=parse_func, **kwargs)\n    return parsed_outputs[0] if single_generate else parsed_outputs\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM","title":"AliyunLLM","text":"<pre><code>AliyunLLM(config: LLMConfig, **kwargs)\n</code></pre> <p>               Bases: <code>BaseLLM</code></p> Source code in <code>evoagentx/models/base_model.py</code> <pre><code>def __init__(self, config: LLMConfig, **kwargs):\n    \"\"\"Initializes the LLM with configuration.\n\n    Args:\n        config: Configuration object for the LLM.\n        **kwargs (Any): Additional keyword arguments.\n    \"\"\"\n    self.config = config\n    self.kwargs = kwargs\n    self.init_model()\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.init_model","title":"init_model","text":"<pre><code>init_model()\n</code></pre> <p>Initialize the DashScope Generation client.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def init_model(self):\n    \"\"\"\n    Initialize the DashScope Generation client.\n    \"\"\"\n    config: AliyunLLMConfig = self.config\n    if not config.aliyun_api_key:\n        raise ValueError(\"Aliyun API key is required. You should set `aliyun_api_key` in AliyunLLMConfig\")\n\n    #  API key\n    os.environ[\"DASHSCOPE_API_KEY\"] = config.aliyun_api_key\n    dashscope.api_key = config.aliyun_api_key\n\n    # model\n    self._client = Generation()\n    self._default_ignore_fields = [\n        \"llm_type\", \"output_response\", \"aliyun_api_key\", \"aliyun_access_key_id\",\n        \"aliyun_access_key_secret\", \"model_name\"\n    ]\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.formulate_messages","title":"formulate_messages","text":"<pre><code>formulate_messages(prompts: List[str], system_messages: Optional[List[str]] = None) -&gt; List[List[dict]]\n</code></pre> <p>Format messages for the Aliyun model.</p> <p>Parameters:</p> Name Type Description Default <code>prompts</code> <code>List[str]</code> <p>List of user prompts.</p> required <code>system_messages</code> <code>Optional[List[str]]</code> <p>Optional list of system messages.</p> <code>None</code> <p>Returns:</p> Type Description <code>List[List[dict]]</code> <p>List[List[dict]]: Formatted messages for the model.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def formulate_messages(self, prompts: List[str], system_messages: Optional[List[str]] = None) -&gt; List[List[dict]]:\n    \"\"\"\n    Format messages for the Aliyun model.\n\n    Args:\n        prompts (List[str]): List of user prompts.\n        system_messages (Optional[List[str]]): Optional list of system messages.\n\n    Returns:\n        List[List[dict]]: Formatted messages for the model.\n    \"\"\"\n    if system_messages:\n        assert len(prompts) == len(system_messages), f\"the number of prompts ({len(prompts)}) is different from the number of system_messages ({len(system_messages)})\"\n    else:\n        system_messages = [None] * len(prompts)\n\n    messages_list = []\n    for prompt, system_message in zip(prompts, system_messages):\n        messages = []\n        if system_message:\n            messages.append({\"role\": \"system\", \"content\": system_message})\n        messages.append({\"role\": \"user\", \"content\": prompt})\n        messages_list.append(messages)\n    return messages_list\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.update_completion_params","title":"update_completion_params","text":"<pre><code>update_completion_params(params1: dict, params2: dict) -&gt; dict\n</code></pre> <p>Update completion parameters with new values.</p> <p>Parameters:</p> Name Type Description Default <code>params1</code> <code>dict</code> <p>Base parameters.</p> required <code>params2</code> <code>dict</code> <p>New parameters to update with.</p> required <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>Updated parameters.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def update_completion_params(self, params1: dict, params2: dict) -&gt; dict:\n    \"\"\"\n    Update completion parameters with new values.\n\n    Args:\n        params1 (dict): Base parameters.\n        params2 (dict): New parameters to update with.\n\n    Returns:\n        dict: Updated parameters.\n    \"\"\"\n    config_params: list = self.config.get_config_params()\n    for key, value in params2.items():\n        if key in self._default_ignore_fields:\n            continue\n        if key not in config_params:\n            continue\n        params1[key] = value\n    return params1\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.get_completion_params","title":"get_completion_params","text":"<pre><code>get_completion_params(**kwargs)\n</code></pre> <p>Get completion parameters for the model.</p> <p>Returns:</p> Name Type Description <code>dict</code> <p>Parameters for model completion.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def get_completion_params(self, **kwargs):\n    \"\"\"\n    Get completion parameters for the model.\n\n    Returns:\n        dict: Parameters for model completion.\n    \"\"\"\n    completion_params = self.config.get_set_params(ignore=self._default_ignore_fields)\n    completion_params = self.update_completion_params(completion_params, kwargs)\n    completion_params[\"model\"] = self.config.model\n    return completion_params\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.get_stream_output","title":"get_stream_output","text":"<pre><code>get_stream_output(response: Any, output_response: bool = True) -&gt; str\n</code></pre> <p>Process streaming response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The streaming response from the model.</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete response text.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def get_stream_output(self, response: Any, output_response: bool = True) -&gt; str:\n    \"\"\"\n    Process streaming response from the model.\n\n    Args:\n        response: The streaming response from the model.\n        output_response (bool): Whether to print the response.\n\n    Returns:\n        str: The complete response text.\n    \"\"\"\n    output = \"\"\n    try:\n        for chunk in response:\n            if not hasattr(chunk, 'output') or chunk.output is None:\n                error_msg = getattr(chunk, 'message', 'Invalid chunk format from model')\n                raise ValueError(f\"Model stream chunk error: {error_msg}\")\n            if hasattr(chunk.output, 'text'):\n                content = chunk.output.text\n            elif hasattr(chunk.output, 'choices') and chunk.output.choices:\n                content = chunk.output.choices[0].message.content\n            else:\n                continue\n            if content:\n                if output_response:\n                    print(content, end=\"\", flush=True)\n                output += content\n    except Exception as e:\n        print(f\"Error processing stream: {str(e)}\")\n        if not output:\n            raise RuntimeError(f\"Failed to process stream response: {str(e)}\")\n    if output_response:\n        print(\"\")\n    return output\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.get_stream_output_async","title":"get_stream_output_async  <code>async</code>","text":"<pre><code>get_stream_output_async(response: Any, output_response: bool = False) -&gt; str\n</code></pre> <p>Process streaming response asynchronously.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The streaming response from the model.</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response.</p> <code>False</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete response text.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>async def get_stream_output_async(self, response: Any, output_response: bool = False) -&gt; str:\n    \"\"\"\n    Process streaming response asynchronously.\n\n    Args:\n        response: The streaming response from the model.\n        output_response (bool): Whether to print the response.\n\n    Returns:\n        str: The complete response text.\n    \"\"\"\n    output = \"\"\n    try:\n        async for chunk in response:\n            if not hasattr(chunk, 'output') or chunk.output is None:\n                error_msg = getattr(chunk, 'message', 'Invalid chunk format from model')\n                raise ValueError(f\"Model stream chunk error: {error_msg}\")\n            if hasattr(chunk.output, 'text'):\n                content = chunk.output.text\n            elif hasattr(chunk.output, 'choices') and chunk.output.choices:\n                content = chunk.output.choices[0].message.content\n            else:\n                continue\n            if content:\n                if output_response:\n                    print(content, end=\"\", flush=True)\n                output += content\n    except Exception as e:\n        print(f\"Error processing async stream: {str(e)}\")\n        if not output:\n            raise RuntimeError(f\"Failed to process async stream response: {str(e)}\")\n    if output_response:\n        print(\"\")\n    return output\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.get_completion_output","title":"get_completion_output","text":"<pre><code>get_completion_output(response: Any, output_response: bool = True) -&gt; str\n</code></pre> <p>Process non-streaming response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>response</code> <code>Any</code> <p>The response from the model.</p> required <code>output_response</code> <code>bool</code> <p>Whether to print the response.</p> <code>True</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The complete response text.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def get_completion_output(self, response: Any, output_response: bool = True) -&gt; str:\n    \"\"\"\n    Process non-streaming response from the model.\n\n    Args:\n        response: The response from the model.\n        output_response (bool): Whether to print the response.\n\n    Returns:\n        str: The complete response text.\n    \"\"\"\n    try:\n        if not hasattr(response, 'output') or response.output is None:\n            error_msg = getattr(response, 'message', 'Invalid response format from model')\n            raise ValueError(f\"Model response error: {error_msg}\")\n\n        if hasattr(response.output, 'text'):\n            output = response.output.text\n        elif hasattr(response.output, 'choices') and response.output.choices:\n            output = response.output.choices[0].message.content\n        else:\n            raise ValueError(\"Unexpected response format\")\n\n        if output_response:\n            print(output)\n        return output\n    except Exception as e:\n        raise RuntimeError(f\"Error processing completion response: {str(e)}\")\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.single_generate","title":"single_generate","text":"<pre><code>single_generate(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Generate a single response from the model.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>The conversation history.</p> required <code>**kwargs</code> <p>Additional parameters for generation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated response.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>@retry(wait=wait_random_exponential(min=1, max=60), stop=stop_after_attempt(5))\ndef single_generate(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"\n    Generate a single response from the model.\n\n    Args:\n        messages (List[dict]): The conversation history.\n        **kwargs: Additional parameters for generation.\n\n    Returns:\n        str: The generated response.\n    \"\"\"\n    stream = kwargs.get(\"stream\", self.config.stream)\n    output_response = kwargs.get(\"output_response\", self.config.output_response)\n\n    try:\n        completion_params = self.get_completion_params(**kwargs)\n        response = self._client.call(messages=messages, **completion_params)\n\n        if response is None:\n            raise RuntimeError(\"Received empty response from model\")\n\n        if stream:\n            output = self.get_stream_output(response, output_response=output_response)\n            cost = self._stream_cost(response)\n        else:\n            output = self.get_completion_output(response=response, output_response=output_response)\n            cost = self._completion_cost(response)\n        self._update_cost(cost=cost)\n        return output\n    except Exception as e:\n        raise RuntimeError(f\"Error during single_generate of AliyunLLM: {str(e)}\")\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.batch_generate","title":"batch_generate","text":"<pre><code>batch_generate(batch_messages: List[List[dict]], **kwargs) -&gt; List[str]\n</code></pre> <p>Generate responses for a batch of messages.</p> <p>Parameters:</p> Name Type Description Default <code>batch_messages</code> <code>List[List[dict]]</code> <p>List of conversation histories.</p> required <code>**kwargs</code> <p>Additional parameters for generation.</p> <code>{}</code> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of generated responses.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>def batch_generate(self, batch_messages: List[List[dict]], **kwargs) -&gt; List[str]:\n    \"\"\"\n    Generate responses for a batch of messages.\n\n    Args:\n        batch_messages (List[List[dict]]): List of conversation histories.\n        **kwargs: Additional parameters for generation.\n\n    Returns:\n        List[str]: List of generated responses.\n    \"\"\"\n    if not isinstance(batch_messages, list) or not batch_messages:\n        raise ValueError(\"batch_messages must be a non-empty list of message lists\")\n    return [self.single_generate(messages=one_messages, **kwargs) for one_messages in batch_messages]\n</code></pre>"},{"location":"api/models.html#evoagentx.models.AliyunLLM.single_generate_async","title":"single_generate_async  <code>async</code>","text":"<pre><code>single_generate_async(messages: List[dict], **kwargs) -&gt; str\n</code></pre> <p>Asynchronously generate a single response.</p> <p>Parameters:</p> Name Type Description Default <code>messages</code> <code>List[dict]</code> <p>The conversation history.</p> required <code>**kwargs</code> <p>Additional parameters for the generation.</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The generated response.</p> Source code in <code>evoagentx/models/aliyun_model.py</code> <pre><code>async def single_generate_async(self, messages: List[dict], **kwargs) -&gt; str:\n    \"\"\"\n    Asynchronously generate a single response.\n\n    Args:\n        messages (List[dict]): The conversation history.\n        **kwargs: Additional parameters for the generation.\n\n    Returns:\n        str: The generated response.\n    \"\"\"\n    stream = kwargs.get(\"stream\", self.config.stream)\n    output_response = kwargs.get(\"output_response\", self.config.output_response)\n\n    try:\n        completion_params = self.get_completion_params(**kwargs)\n        loop = asyncio.get_event_loop()\n        response = await loop.run_in_executor(\n            None,\n            lambda: self._client.call(messages=messages, **completion_params)\n        )\n\n        if stream:\n            output = await self.get_stream_output_async(response, output_response=output_response)\n            cost = self._stream_cost(response)\n        else:\n            output = self.get_completion_output(response=response, output_response=output_response)\n            cost = self._completion_cost(response)\n\n        self._update_cost(cost=cost)\n        return output\n\n    except Exception as e:\n        raise RuntimeError(f\"Error during single_generate_async of AliyunLLM: {str(e)}\")\n</code></pre>"},{"location":"api/models.html#evoagentx.models.atomic_method","title":"atomic_method","text":"<pre><code>atomic_method(func)\n</code></pre> <p>threading safe decorator for class methods.  If there are self._lock in the instance, it will use the lock. Otherwise, use nullcontext for execution.</p> Source code in <code>evoagentx/core/decorators.py</code> <pre><code>def atomic_method(func):\n    \"\"\"\n    threading safe decorator for class methods. \n    If there are self._lock in the instance, it will use the lock. Otherwise, use nullcontext for execution.\n    \"\"\"\n    @wraps(func)\n    def wrapper(self, *args, **kwargs):\n        context = getattr(self, \"_lock\", nullcontext())\n        with context:\n            return func(self, *args, **kwargs)\n    return wrapper\n</code></pre>"},{"location":"api/optimizers.html","title":"\ud83e\uddee Optimizers","text":""},{"location":"api/optimizers.html#evoagentx.optimizers","title":"evoagentx.optimizers","text":""},{"location":"api/optimizers.html#evoagentx.optimizers.SEWOptimizer","title":"SEWOptimizer","text":"<pre><code>SEWOptimizer(**kwargs)\n</code></pre> <p>               Bases: <code>Optimizer</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.SEWOptimizer.step","title":"step","text":"<pre><code>step(**kwargs) -&gt; Union[SequentialWorkFlowGraph, ActionGraph]\n</code></pre> <p>Take a step of optimization and return the optimized graph.</p> Source code in <code>evoagentx/optimizers/sew_optimizer.py</code> <pre><code>def step(self, **kwargs) -&gt; Union[SequentialWorkFlowGraph, ActionGraph]:\n    \"\"\"\n    Take a step of optimization and return the optimized graph.\n    \"\"\"\n    graph = self._select_graph_with_highest_score(return_metrics=False)\n    if isinstance(graph, SequentialWorkFlowGraph):\n        new_graph = self._workflow_graph_step(graph)\n    elif isinstance(graph, ActionGraph):\n        new_graph = self._action_graph_step(graph)\n    else:\n        raise ValueError(f\"Invalid graph type: {type(graph)}. The graph should be an instance of `WorkFlowGraph` or `ActionGraph`.\")\n    return new_graph\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.SEWOptimizer.evaluate","title":"evaluate","text":"<pre><code>evaluate(dataset: Benchmark, eval_mode: str = 'test', graph: Optional[Union[SequentialWorkFlowGraph, ActionGraph]] = None, indices: Optional[List[int]] = None, sample_k: Optional[int] = None, **kwargs) -&gt; dict\n</code></pre> <p>Evaluate the workflow. If <code>graph</code> is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. </p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>The dataset to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'test'</code> <code>graph</code> <code>Union[WorkFlowGraph, ActionGraph]</code> <p>The graph to evaluate. If not provided, use the graph in the optimizer.</p> <code>None</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The metrics of the workflow evaluation.</p> Source code in <code>evoagentx/optimizers/sew_optimizer.py</code> <pre><code>def evaluate(\n    self, \n    dataset: Benchmark, \n    eval_mode: str = \"test\", \n    graph: Optional[Union[SequentialWorkFlowGraph, ActionGraph]] = None,\n    indices: Optional[List[int]] = None,\n    sample_k: Optional[int] = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"\n    Evaluate the workflow. If `graph` is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. \n\n    Args:\n        dataset (Benchmark): The dataset to evaluate the workflow on.\n        eval_mode (str): The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].\n        graph (Union[WorkFlowGraph, ActionGraph], optional): The graph to evaluate. If not provided, use the graph in the optimizer.\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n\n    Returns:\n        dict: The metrics of the workflow evaluation.\n    \"\"\"\n    graph = graph if graph is not None else self.graph\n    metrics_list = []\n    for i in range(self.eval_rounds):\n        eval_info = [\n            f\"[{type(graph).__name__}]\", \n            f\"Evaluation round {i+1}/{self.eval_rounds}\", \n            f\"Mode: {eval_mode}\"\n        ]\n        if indices is not None:\n            eval_info.append(f\"Indices: {len(indices)} samples\")\n        if sample_k is not None:\n            eval_info.append(f\"Sample size: {sample_k}\")\n        logger.info(\" | \".join(eval_info))\n        metrics = self.evaluator.evaluate(\n            graph=graph, \n            benchmark=dataset, \n            eval_mode=eval_mode, \n            indices=indices, \n            sample_k=sample_k,\n            **kwargs\n        )\n        metrics_list.append(metrics)\n    avg_metrics = self.evaluator._calculate_average_score(metrics_list)\n\n    return avg_metrics\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.SEWOptimizer.save","title":"save","text":"<pre><code>save(path: str, ignore: List[str] = [])\n</code></pre> <p>Save the (optimized) workflow graph to a file. </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the workflow graph.</p> required <code>ignore</code> <code>List[str]</code> <p>The keys to ignore when saving the workflow graph.</p> <code>[]</code> Source code in <code>evoagentx/optimizers/sew_optimizer.py</code> <pre><code>def save(self, path: str, ignore: List[str] = []):\n    \"\"\"\n    Save the (optimized) workflow graph to a file. \n\n    Args:\n        path (str): The path to save the workflow graph.\n        ignore (List[str]): The keys to ignore when saving the workflow graph.\n    \"\"\"\n    self.graph.save_module(path, ignore=ignore)\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.AFlowOptimizer","title":"AFlowOptimizer","text":"<pre><code>AFlowOptimizer(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>AFlow Optimizer for workflow optimization.</p> <p>This optimizer iteratively improves workflows through multiple rounds of optimization using large language models. It evaluates workflow performance, identifies improvement opportunities, and applies optimizations based on experience and convergence metrics.</p> <p>Attributes:</p> Name Type Description <code>question_type</code> <code>str</code> <p>Type of task to optimize for (e.g., qa, match, code)</p> <code>graph_path</code> <code>str</code> <p>Path to the workflow graph directory (must contain graph.py and prompt.py)</p> <code>optimized_path</code> <code>str</code> <p>Path to save optimized workflows (defaults to graph_path)</p> <code>initial_round</code> <code>int</code> <p>Starting round number for optimization</p> <code>optimizer_llm</code> <code>BaseLLM</code> <p>LLM used for generating optimizations</p> <code>executor_llm</code> <code>BaseLLM</code> <p>LLM used for executing the workflow</p> <code>operators</code> <code>List[str]</code> <p>List of operators available for optimization</p> <code>sample</code> <code>int</code> <p>Number of rounds to sample from for optimization</p> <code>max_rounds</code> <code>int</code> <p>Maximum number of optimization rounds to perform</p> <code>validation_rounds</code> <code>int</code> <p>Number of validation runs per optimization round</p> <code>eval_rounds</code> <code>int</code> <p>Number of evaluation runs for test mode</p> <code>check_convergence</code> <code>bool</code> <p>Whether to check for optimization convergence</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.AFlowOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(benchmark: Benchmark)\n</code></pre> <p>Run the optimization process on the workflow.</p> <p>Performs multiple rounds of optimization, evaluating each round against the benchmark and checking for convergence. Continues until convergence is detected or the maximum number of rounds is reached.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate the workflow against</p> required Source code in <code>evoagentx/optimizers/aflow_optimizer.py</code> <pre><code>def optimize(self, benchmark: Benchmark):\n    \"\"\"Run the optimization process on the workflow.\n\n    Performs multiple rounds of optimization, evaluating each round against\n    the benchmark and checking for convergence. Continues until convergence\n    is detected or the maximum number of rounds is reached.\n\n    Args:\n        benchmark: The benchmark to evaluate the workflow against\n    \"\"\"\n    self.benchmark = benchmark\n    for _ in range(self.max_rounds):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        score = loop.run_until_complete(self._execute_with_retry(self._optimize_graph))\n        self.round += 1\n        logger.info(f\"Score for round {self.round}: {score}\")\n        if self._check_convergence():\n            break\n        if self.round &gt;= self.max_rounds:\n            logger.info(f\"Max rounds reached: {self.max_rounds}, stopping optimization.\")\n            break\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.AFlowOptimizer.test","title":"test","text":"<pre><code>test(benchmark: Benchmark, test_rounds: List[int] = None)\n</code></pre> <p>Run the test evaluation on optimized workflows.</p> <p>Evaluates specified rounds (or the best round if none specified) against the benchmark multiple times and logs the results.</p> <p>Parameters:</p> Name Type Description Default <code>benchmark</code> <code>Benchmark</code> <p>The benchmark to evaluate against</p> required <code>test_rounds</code> <code>List[int]</code> <p>Specific round numbers to test, or None to use the best round</p> <code>None</code> Source code in <code>evoagentx/optimizers/aflow_optimizer.py</code> <pre><code>def test(self, benchmark: Benchmark, test_rounds: List[int] = None):\n    \"\"\"Run the test evaluation on optimized workflows.\n\n    Evaluates specified rounds (or the best round if none specified) against\n    the benchmark multiple times and logs the results.\n\n    Args:\n        benchmark: The benchmark to evaluate against\n        test_rounds: Specific round numbers to test, or None to use the best round\n    \"\"\"\n    self.benchmark = benchmark\n    if test_rounds is None:\n        best_round = self._load_best_round()\n        logger.info(f\"No test rounds provided, using best round: {best_round}\")\n        test_rounds = [best_round]\n    for _ in tqdm(range(self.eval_rounds)):\n        loop = asyncio.new_event_loop()\n        asyncio.set_event_loop(loop)\n        loop.run_until_complete(self._run_test(test_rounds))\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer","title":"TextGradOptimizer","text":"<pre><code>TextGradOptimizer(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Uses TextGrad to optimize agents' system prompts and instructions in a multi-agent workflow. For more information on TextGrad, see https://github.com/zou-group/textgrad.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: Benchmark, use_answers: bool = True, seed: Optional[int] = None) -&gt; None\n</code></pre> <p>Optimizes self.graph using <code>dataset</code>.</p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>The dataset to use for optimization.</p> required <code>use_answers</code> <code>bool</code> <p>Whether to use the answers (labels) in the training set for optimization. If False, <code>dataset</code>'s training set does not need to have answers. If <code>eval_every_n_steps</code> is set to None, we can optimize the workflow without any labeled data.</p> <code>True</code> <code>seed</code> <code>Optional[int]</code> <p>The random seed to use for shuffling the data.</p> <code>None</code> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def optimize(self, dataset: Benchmark, use_answers: bool = True, seed: Optional[int] = None) -&gt; None:\n    \"\"\"Optimizes self.graph using `dataset`.\n\n    Args:\n        dataset (Benchmark): The dataset to use for optimization.\n        use_answers (bool): Whether to use the answers (labels) in the training set for optimization.\n            If False, `dataset`'s training set does not need to have answers.\n            If `eval_every_n_steps` is set to None, we can optimize the workflow without any labeled data.\n        seed (Optional[int]): The random seed to use for shuffling the data.\n    \"\"\"\n    self._init_textgrad(dataset, use_answers)\n\n    def iterator() -&gt; Iterator[Tuple[List[dict[str, str]],  Optional[List[Union[str, dict[str, str]]]]]]:\n        epoch = 0\n        while True:\n            # Shuffle train data every epoch\n            effective_seed = seed + epoch if seed is not None else None\n            train_data = dataset.get_train_data(sample_k=len(dataset._train_data), seed=effective_seed)\n            for i in range(0, len(train_data), self.batch_size):\n                batch = train_data[i:i + self.batch_size]\n                inputs = [self.evaluator.collate_func(x) for x in batch]\n                if use_answers:\n                    labels = dataset.get_labels(batch)\n                else:\n                    labels = None\n                yield inputs, labels\n            epoch += 1\n\n    data_iterator = iterator()\n\n    for step in tqdm(range(self.max_steps)):\n        inputs, labels = next(data_iterator)\n        self.step(inputs, labels, dataset, use_answers)\n\n        if self.eval_every_n_steps is not None and (step + 1) % self.eval_every_n_steps == 0:\n            logger.info(f\"Evaluating the workflow at step {step+1} ...\")\n            with suppress_logger_info():\n                metrics = self.evaluate(dataset, **self.eval_config)\n            self.log_snapshot(self.graph, metrics)\n            logger.info(f\"Step {step+1} metrics: {metrics}\")\n\n            # If rollback is enabled, keep track of the best snapshot\n            if self.rollback:\n                if len(self._snapshot) == 1:\n                    best_snapshot = self._snapshot[-1]\n                    best_average_score = np.mean(list(metrics.values()))\n                else:\n                    current_average_score = np.mean(list(metrics.values()))\n\n                    if current_average_score &gt;= best_average_score:\n                        # If the current average score is better than the best average score, update the best snapshot\n                        best_snapshot = self._snapshot[-1]\n                        best_average_score = current_average_score\n                    else:\n                        # If the current average score is worse than the best average score, roll back to the best snapshot\n                        logger.info(f\"Metrics are worse than the best snapshot which has {best_snapshot['metrics']}. Rolling back to the best snapshot.\")\n                        best_graph = WorkFlowGraph.from_dict(best_snapshot[\"graph\"])\n                        self.graph = best_graph\n                        self._create_textgrad_agents()\n\n        if self.save_interval is not None and (step + 1) % self.save_interval == 0:\n            logger.info(f\"Saving the workflow at step {step+1} ...\")\n            self.save(os.path.join(self.save_path, f\"{dataset.name}_textgrad_step_{step+1}.json\"))\n\n    logger.info(f\"Reached the maximum number of steps {self.max_steps}. Optimization has finished.\")\n    self.save(os.path.join(self.save_path, f\"{dataset.name}_textgrad_final.json\"))\n\n    # Saves the best graph\n    if len(self._snapshot) &gt; 0:\n        best_graph = self._select_graph_with_highest_score()\n        self.save(os.path.join(self.save_path, f\"{dataset.name}_textgrad_best.json\"), graph=best_graph)\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.step","title":"step","text":"<pre><code>step(inputs: list[dict[str, str]], labels: Optional[list[Union[str, dict[str, str]]]], dataset: Benchmark, use_answers: bool = True) -&gt; None\n</code></pre> <p>Performs one optimization step using a batch of data.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def step(\n    self, \n    inputs: list[dict[str, str]], \n    labels: Optional[list[Union[str, dict[str, str]]]], \n    dataset: Benchmark, \n    use_answers: bool = True\n) -&gt; None:\n    \"\"\"Performs one optimization step using a batch of data.\"\"\"\n\n    losses = []\n    logger.info(\"Executing workflow...\")\n\n    if use_answers:\n        if labels is None:\n            raise ValueError(\"Labels must be provided if `use_answers` is True.\")\n\n        for input, label in zip(inputs, labels, strict=True):\n            output = self.forward(input)\n            if isinstance(label, str):\n                label = Variable(label, requires_grad=False, role_description=\"correct answer for the query\")\n            elif isinstance(label, dict):\n                if not isinstance(dataset, CodingBenchmark):\n                    raise ValueError(\"Label must be a string for non-coding benchmarks.\")\n                end_node_name = self.graph.find_end_nodes()[0]\n                end_node = self.graph.get_node(end_node_name)\n                output_name = end_node.outputs[0].name\n                code = output.parsed_outputs[output_name]\n                label = self._format_code_label(code, label, dataset)\n                label = Variable(label, requires_grad=False, role_description=\"the task, the test result, and the correct code\")\n            loss = self.loss_fn([output, label])\n            losses.append(loss)\n    else:\n        for input in inputs:\n            output = self.forward(input)\n            loss = self.loss_fn(output)\n            losses.append(loss)\n\n    total_loss = tg.sum(losses)\n    logger.info(\"Computing gradients...\")\n    total_loss.backward(self.optimizer_engine)\n    logger.info(\"Updating agents...\")\n    self.textgrad_optimizer.step()\n    self.textgrad_optimizer.zero_grad()\n    self._update_workflow_graph()\n    logger.info(\"Agents updated\")\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.forward","title":"forward","text":"<pre><code>forward(inputs: dict[str, str]) -&gt; Variable\n</code></pre> <p>Returns the final output from the workflow.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def forward(self, inputs: dict[str, str]) -&gt; Variable:\n    \"\"\"Returns the final output from the workflow.\"\"\"\n    self._visited_nodes = set()\n    end_node = self.graph.find_end_nodes()[0]\n    input_variables = self._initial_inputs_to_variables(inputs)\n    output = self._compute_node(end_node, input_variables)\n    return output\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.evaluate","title":"evaluate","text":"<pre><code>evaluate(dataset: Benchmark, eval_mode: str = 'dev', graph: Optional[WorkFlowGraph] = None, indices: Optional[List[int]] = None, sample_k: Optional[int] = None, **kwargs) -&gt; dict\n</code></pre> <p>Evaluate the workflow. If <code>graph</code> is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. </p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>The dataset to evaluate the workflow on.</p> required <code>eval_mode</code> <code>str</code> <p>The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].</p> <code>'dev'</code> <code>graph</code> <code>WorkFlowGraph</code> <p>The graph to evaluate. If not provided, use the graph in the optimizer.</p> <code>None</code> <code>indices</code> <code>List[int]</code> <p>The indices of the data to evaluate the workflow on.</p> <code>None</code> <code>sample_k</code> <code>int</code> <p>The number of data to evaluate the workflow on. If provided, a random sample of size <code>sample_k</code> will be used.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>The metrics of the workflow evaluation.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def evaluate(\n    self, \n    dataset: Benchmark, \n    eval_mode: str = \"dev\", \n    graph: Optional[WorkFlowGraph] = None,\n    indices: Optional[List[int]] = None,\n    sample_k: Optional[int] = None,\n    **kwargs\n) -&gt; dict:\n    \"\"\"Evaluate the workflow. If `graph` is provided, use the provided graph for evaluation. Otherwise, use the graph in the optimizer. \n\n    Args:\n        dataset (Benchmark): The dataset to evaluate the workflow on.\n        eval_mode (str): The evaluation mode. Choices: [\"test\", \"dev\", \"train\"].\n        graph (WorkFlowGraph, optional): The graph to evaluate. If not provided, use the graph in the optimizer.\n        indices (List[int], optional): The indices of the data to evaluate the workflow on.\n        sample_k (int, optional): The number of data to evaluate the workflow on. If provided, a random sample of size `sample_k` will be used.\n\n    Returns:\n        dict: The metrics of the workflow evaluation.\n    \"\"\"\n    if graph is None:\n        graph = self.graph\n\n    metrics_list = []\n    for i in range(self.eval_rounds):\n        eval_info = [\n            f\"[{type(graph).__name__}]\", \n            f\"Evaluation round {i+1}/{self.eval_rounds}\", \n            f\"Mode: {eval_mode}\"\n        ]\n        if indices is not None:\n            eval_info.append(f\"Indices: {len(indices)} samples\")\n        if sample_k is not None:\n            eval_info.append(f\"Sample size: {sample_k}\")\n        logger.info(\" | \".join(eval_info))\n        metrics = self.evaluator.evaluate(\n            graph=graph, \n            benchmark=dataset, \n            eval_mode=eval_mode, \n            indices=indices, \n            sample_k=sample_k,\n            update_agents=True, \n            **kwargs\n        )\n        metrics_list.append(metrics)\n    avg_metrics = self.evaluator._calculate_average_score(metrics_list)\n\n    return avg_metrics\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.save","title":"save","text":"<pre><code>save(path: str, graph: Optional[WorkFlowGraph] = None, ignore: List[str] = []) -&gt; None\n</code></pre> <p>Save the workflow graph containing the optimized prompts to a file. </p> <p>Parameters:</p> Name Type Description Default <code>path</code> <code>str</code> <p>The path to save the workflow graph.</p> required <code>graph</code> <code>WorkFlowGraph</code> <p>The graph to save. If not provided, use the graph in the optimizer.</p> <code>None</code> <code>ignore</code> <code>List[str]</code> <p>The keys to ignore when saving the workflow graph.</p> <code>[]</code> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def save(self, path: str, graph: Optional[WorkFlowGraph] = None, ignore: List[str] = []) -&gt; None:\n    \"\"\"Save the workflow graph containing the optimized prompts to a file. \n\n    Args:\n        path (str): The path to save the workflow graph.\n        graph (WorkFlowGraph, optional): The graph to save. If not provided, use the graph in the optimizer.\n        ignore (List[str]): The keys to ignore when saving the workflow graph.\n    \"\"\"\n    if graph is None:\n        graph = self.graph\n    graph.save_module(path, ignore=ignore)\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.log_snapshot","title":"log_snapshot","text":"<pre><code>log_snapshot(graph: WorkFlowGraph, metrics: dict) -&gt; None\n</code></pre> <p>Log the snapshot of the workflow.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def log_snapshot(self, graph: WorkFlowGraph, metrics: dict) -&gt; None:\n    \"\"\"Log the snapshot of the workflow.\"\"\"\n    self._snapshot.append(\n        {\n            \"index\": len(self._snapshot),\n            \"graph\": deepcopy(graph.get_config()),\n            \"metrics\": metrics,\n        }\n    )\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.TextGradOptimizer.restore_best_graph","title":"restore_best_graph","text":"<pre><code>restore_best_graph() -&gt; None\n</code></pre> <p>Restore the best graph from the snapshot and set it to <code>self.graph</code>.</p> Source code in <code>evoagentx/optimizers/textgrad_optimizer.py</code> <pre><code>def restore_best_graph(self) -&gt; None:\n    \"\"\"Restore the best graph from the snapshot and set it to `self.graph`.\"\"\"\n    if len(self._snapshot) == 0:\n        logger.info(\"No snapshot found. No graph to restore.\")\n        return\n\n    best_graph, best_metrics = self._select_graph_with_highest_score(return_metrics=True)\n    self.graph = best_graph\n    logger.info(f\"Restored the best graph from snapshot with metrics {best_metrics}\")\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.MiproOptimizer","title":"MiproOptimizer","text":"<pre><code>MiproOptimizer(registry: ParamRegistry, program: Callable, optimizer_llm: BaseLLM, evaluator: Optional[Callable] = None, eval_rounds: Optional[int] = 1, metric_threshold: Optional[float] = None, max_bootstrapped_demos: int = 4, max_labeled_demos: int = 4, auto: Optional[Literal['light', 'medium', 'heavy']] = 'medium', max_steps: int = None, num_candidates: Optional[int] = None, num_threads: Optional[int] = None, max_errors: int = 10, seed: int = 9, init_temperature: float = 0.5, track_stats: bool = True, save_path: Optional[str] = None, minibatch: bool = True, minibatch_size: int = 35, minibatch_full_eval_steps: int = 5, program_aware_proposer: bool = True, data_aware_proposer: bool = True, view_data_batch_size: int = 10, tip_aware_proposer: bool = True, fewshot_aware_proposer: bool = True, requires_permission_to_run: bool = False, provide_traceback: Optional[bool] = None, verbose: bool = False, **kwargs)\n</code></pre> <p>               Bases: <code>BaseOptimizer</code>, <code>MIPROv2</code></p> <p>Base MiproOptimizer class that supports plug-and-play usage. </p> <p>Parameters:</p> Name Type Description Default <code>registry</code> <code>ParamRegistry</code> <p>a ParamRegistry object that contains the parameters to optimize. </p> required <code>program</code> <code>Callable</code> <p>a program to optimize. Must be a callable object with save(path) and load(path) methods.</p> required <code>optimizer_llm</code> <code>BaseLLM</code> <p>a language model to use for optimization. </p> required <code>evaluator</code> <code>Optional[Callable]</code> <p>a function that evaluates the performance of the program.  Required to have a <code>__call__(program, evalset, *kwargs) -&gt; float</code> method that receives a program and a list of  examples from a benchmark's train/dev/test set and return a float score. Must also have a <code>metric(example, prediction) -&gt; float</code>  method that evaluates a single example. If not provided, will construct a default evaluator using the benchmark's evaluate method.</p> <code>None</code> <code>eval_rounds</code> <code>Optional[int]</code> <p>number of rounds to evaluate the program. Defaults to 1. </p> <code>1</code> <code>metric_threshold</code> <code>Optional[float]</code> <p>threshold for the metric score. If provided, only examples with scores above this threshold will be used as demonstrations.  If not provided, examples with scores above 0 will be used as demonstrations. </p> <code>None</code> <code>max_bootstrapped_demos</code> <code>int</code> <p>maximum number of bootstrapped demonstrations to use. Defaults to 4.</p> <code>4</code> <code>max_labeled_demos</code> <code>int</code> <p>maximum number of labeled demonstrations to use. Defaults to 4.</p> <code>4</code> <code>auto</code> <code>Optional[Literal['light', 'medium', 'heavy']]</code> <p>automatic configuration mode. If set, will override num_candidates and max_steps.  \"light\": n=6, val_size=100; \"medium\": n=12, val_size=300; \"heavy\": n=18, val_size=1000. Defaults to \"medium\".</p> <code>'medium'</code> <code>max_steps</code> <code>int</code> <p>maximum number of optimization steps. Required if auto is None.</p> <code>None</code> <code>num_candidates</code> <code>Optional[int]</code> <p>number of candidates to generate for each optimization step. Required if auto is None.</p> <code>None</code> <code>num_threads</code> <code>Optional[int]</code> <p>number of threads to use for parallel evaluation. If None, will use single thread. Only used if evaluator is not provided. </p> <code>None</code> <code>max_errors</code> <code>int</code> <p>maximum number of errors allowed during evaluation before stopping. Defaults to 10.</p> <code>10</code> <code>seed</code> <code>int</code> <p>random seed for reproducibility. Defaults to 9.</p> <code>9</code> <code>init_temperature</code> <code>float</code> <p>initial temperature for instruction generation. Defaults to 0.5.</p> <code>0.5</code> <code>track_stats</code> <code>bool</code> <p>whether to track optimization statistics. Defaults to True.</p> <code>True</code> <code>save_path</code> <code>Optional[str]</code> <p>path to save optimization results. If None, results will not be saved.</p> <code>None</code> <code>minibatch</code> <code>bool</code> <p>whether to use minibatch evaluation during optimization. Defaults to True.</p> <code>True</code> <code>minibatch_size</code> <code>int</code> <p>size of minibatch for evaluation. Defaults to 35.</p> <code>35</code> <code>minibatch_full_eval_steps</code> <code>int</code> <p>number of minibatch steps between full evaluations. Defaults to 5.</p> <code>5</code> <code>program_aware_proposer</code> <code>bool</code> <p>whether to use program-aware instruction proposer. Defaults to True.</p> <code>True</code> <code>data_aware_proposer</code> <code>bool</code> <p>whether to use data-aware instruction proposer. Defaults to True.</p> <code>True</code> <code>view_data_batch_size</code> <code>int</code> <p>batch size for viewing data during instruction proposal. Defaults to 10.</p> <code>10</code> <code>tip_aware_proposer</code> <code>bool</code> <p>whether to use tip-aware instruction proposer. Defaults to True.</p> <code>True</code> <code>fewshot_aware_proposer</code> <code>bool</code> <p>whether to use fewshot-aware instruction proposer. Defaults to True.</p> <code>True</code> <code>requires_permission_to_run</code> <code>bool</code> <p>whether to require user permission before running optimization. Defaults to False.</p> <code>False</code> <code>provide_traceback</code> <code>Optional[bool]</code> <p>whether to provide traceback for evaluation errors. If None, will use default setting.</p> <code>None</code> <code>**kwargs</code> <p>additional keyword arguments to pass to the evaluator.</p> <code>{}</code> <p>Raises:</p> Type Description <code>TypeError</code> <p>If program is not callable or evaluator doesn't return float</p> <code>ValueError</code> <p>If program doesn't have required methods (save and load) or if evaluator doesn't have required methods</p> Source code in <code>evoagentx/optimizers/mipro_optimizer.py</code> <pre><code>def __init__(\n    self,\n    registry: ParamRegistry,\n    program: Callable,\n    optimizer_llm: BaseLLM,\n    evaluator: Optional[Callable] = None,\n    eval_rounds: Optional[int] = 1, \n    metric_threshold: Optional[float] = None,\n    max_bootstrapped_demos: int = 4, \n    max_labeled_demos: int = 4, \n    auto: Optional[Literal[\"light\", \"medium\", \"heavy\"]] = \"medium\", \n    max_steps: int = None, \n    num_candidates: Optional[int] = None, \n    num_threads: Optional[int] = None, \n    max_errors: int = 10, \n    seed: int = 9, \n    init_temperature: float = 0.5, \n    track_stats: bool = True, \n    save_path: Optional[str] = None,  \n    minibatch: bool = True, \n    minibatch_size: int = 35, \n    minibatch_full_eval_steps: int = 5, \n    program_aware_proposer: bool = True,\n    data_aware_proposer: bool = True,\n    view_data_batch_size: int = 10,\n    tip_aware_proposer: bool = True,\n    fewshot_aware_proposer: bool = True,\n    requires_permission_to_run: bool = False,\n    provide_traceback: Optional[bool] = None,\n    verbose: bool = False, \n    **kwargs\n):\n    \"\"\"\n    Base MiproOptimizer class that supports plug-and-play usage. \n\n    Args: \n        registry (ParamRegistry): a ParamRegistry object that contains the parameters to optimize. \n        program (Callable): a program to optimize. Must be a callable object with save(path) and load(path) methods.\n        optimizer_llm (BaseLLM): a language model to use for optimization. \n        evaluator (Optional[Callable]): a function that evaluates the performance of the program. \n            Required to have a `__call__(program, evalset, *kwargs) -&gt; float` method that receives a program and a list of \n            examples from a benchmark's train/dev/test set and return a float score. Must also have a `metric(example, prediction) -&gt; float` \n            method that evaluates a single example. If not provided, will construct a default evaluator using the benchmark's evaluate method.\n        eval_rounds (Optional[int]): number of rounds to evaluate the program. Defaults to 1. \n        metric_threshold (Optional[float]): threshold for the metric score. If provided, only examples with scores above this threshold will be used as demonstrations. \n            If not provided, examples with scores above 0 will be used as demonstrations. \n        max_bootstrapped_demos (int): maximum number of bootstrapped demonstrations to use. Defaults to 4.\n        max_labeled_demos (int): maximum number of labeled demonstrations to use. Defaults to 4.\n        auto (Optional[Literal[\"light\", \"medium\", \"heavy\"]]): automatic configuration mode. If set, will override num_candidates and max_steps. \n            \"light\": n=6, val_size=100; \"medium\": n=12, val_size=300; \"heavy\": n=18, val_size=1000. Defaults to \"medium\".\n        max_steps (int): maximum number of optimization steps. Required if auto is None.\n        num_candidates (Optional[int]): number of candidates to generate for each optimization step. Required if auto is None.\n        num_threads (Optional[int]): number of threads to use for parallel evaluation. If None, will use single thread. Only used if evaluator is not provided. \n        max_errors (int): maximum number of errors allowed during evaluation before stopping. Defaults to 10.\n        seed (int): random seed for reproducibility. Defaults to 9.\n        init_temperature (float): initial temperature for instruction generation. Defaults to 0.5.\n        track_stats (bool): whether to track optimization statistics. Defaults to True.\n        save_path (Optional[str]): path to save optimization results. If None, results will not be saved.\n        minibatch (bool): whether to use minibatch evaluation during optimization. Defaults to True.\n        minibatch_size (int): size of minibatch for evaluation. Defaults to 35.\n        minibatch_full_eval_steps (int): number of minibatch steps between full evaluations. Defaults to 5.\n        program_aware_proposer (bool): whether to use program-aware instruction proposer. Defaults to True.\n        data_aware_proposer (bool): whether to use data-aware instruction proposer. Defaults to True.\n        view_data_batch_size (int): batch size for viewing data during instruction proposal. Defaults to 10.\n        tip_aware_proposer (bool): whether to use tip-aware instruction proposer. Defaults to True.\n        fewshot_aware_proposer (bool): whether to use fewshot-aware instruction proposer. Defaults to True.\n        requires_permission_to_run (bool): whether to require user permission before running optimization. Defaults to False.\n        provide_traceback (Optional[bool]): whether to provide traceback for evaluation errors. If None, will use default setting.\n        **kwargs: additional keyword arguments to pass to the evaluator.\n\n    Raises:\n        TypeError: If program is not callable or evaluator doesn't return float\n        ValueError: If program doesn't have required methods (save and load) or if evaluator doesn't have required methods\n    \"\"\"\n\n    # initialize base optimizer\n    BaseOptimizer.__init__(self, registry=registry, program=program, evaluator=evaluator)\n\n    # convert the registry and program to dspy-compatible module\n    self._validate_program(program=program)\n    self.model = self._convert_to_dspy_module(registry, program)\n    self.optimizer_llm = MiproLMWrapper(optimizer_llm)\n    dspy.configure(lm=self.optimizer_llm)\n    self.task_model = dspy.settings.lm \n    self.prompt_model = dspy.settings.lm \n    self.metric_threshold = metric_threshold\n    self.metric_name = None \n    self.teacher_settings = {\"use_teacher\": True} \n\n    # Validate 'auto' parameter\n    allowed_modes = {None, \"light\", \"medium\", \"heavy\"}\n    if auto not in allowed_modes:\n        raise ValueError(f\"Invalid value for auto: {auto}. Must be one of {allowed_modes}.\")\n    self.auto = auto\n    self.num_fewshot_candidates = num_candidates\n    self.num_instruct_candidates = num_candidates\n    self.num_candidates = num_candidates\n    self.init_temperature = init_temperature\n    self.max_bootstrapped_demos = max_bootstrapped_demos\n    self.max_labeled_demos = max_labeled_demos\n    self.max_steps = max_steps\n    self.num_threads = num_threads\n    self.max_errors = max_errors\n\n    self.track_stats = track_stats\n    self.eval_rounds = eval_rounds \n    self.save_path = save_path\n    self.prompt_model_total_calls = 0\n    self.total_calls = 0\n    self.seed = seed\n    self.rng = None\n\n    self.minibatch = minibatch \n    self.minibatch_size = minibatch_size \n    self.minibatch_full_eval_steps = minibatch_full_eval_steps \n    self.program_aware_proposer = program_aware_proposer \n    self.data_aware_proposer = data_aware_proposer \n    self.view_data_batch_size = view_data_batch_size \n    self.tip_aware_proposer = tip_aware_proposer \n    self.fewshot_aware_proposer = fewshot_aware_proposer \n    self.requires_permission_to_run = requires_permission_to_run \n    self.provide_traceback = provide_traceback \n    self.verbose = verbose\n    self.kwargs = kwargs \n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.MiproOptimizer.optimize","title":"optimize","text":"<pre><code>optimize(dataset: Benchmark, metric_name: Optional[str] = None, **kwargs)\n</code></pre> <p>Optimize the program using the Mipro algorithm. </p> <p>Parameters:</p> Name Type Description Default <code>dataset</code> <code>Benchmark</code> <p>a Benchmark object that contains the training and validation data. </p> required <code>metric_name</code> <code>Optional[str]</code> <p>the name of the metric to use for optimization. Only used when <code>self.evaluator</code> is not provided.  In this case, the evaluator will be constructed using the <code>evaluate</code> method (return a dictionary of scores) in the benchmark,  and the metric specified by <code>metric_name</code> will be used for optimization. If not provided, the average of all scores returned by the evaluator will be used.  If <code>self.evaluator</code> is provided, this argument will be ignored. </p> <code>None</code> <code>**kwargs</code> <p>additional keyword arguments to pass to the evaluator.</p> <code>{}</code> Source code in <code>evoagentx/optimizers/mipro_optimizer.py</code> <pre><code>def optimize(self, dataset: Benchmark, metric_name: Optional[str] = None, **kwargs):\n\n    \"\"\"\n    Optimize the program using the Mipro algorithm. \n\n    Args:\n        dataset (Benchmark): a Benchmark object that contains the training and validation data. \n        metric_name (Optional[str]): the name of the metric to use for optimization. Only used when `self.evaluator` is not provided. \n            In this case, the evaluator will be constructed using the `evaluate` method (return a dictionary of scores) in the benchmark, \n            and the metric specified by `metric_name` will be used for optimization. If not provided, the average of all scores returned by the evaluator will be used. \n            If `self.evaluator` is provided, this argument will be ignored. \n        **kwargs: additional keyword arguments to pass to the evaluator. \n    \"\"\"\n\n    zeroshot_opt = (self.max_bootstrapped_demos == 0) and (self.max_labeled_demos == 0)\n    student = self.model\n    num_trials = self.max_steps\n    minibatch = self.minibatch\n    self.metric_name = metric_name\n\n    # If auto is None, and num_trials is not provided (but num_candidates is), raise an error that suggests a good num_trials value\n    if self.auto is None and (self.num_candidates is not None and num_trials is None):\n        raise ValueError(f\"If auto is None, max_steps must also be provided. Given num_candidates={self.num_candidates}, we'd recommend setting max_steps to ~{self._set_num_trials_from_num_candidates(self.model, zeroshot_opt, self.num_candidates)}.\")\n\n    # If auto is None, and num_candidates or num_trials is None, raise an error\n    if self.auto is None and (self.num_candidates is None or num_trials is None):\n        raise ValueError(\"If auto is None, num_candidates must also be provided.\")\n\n    # If auto is provided, and either num_candidates or num_trials is not None, raise an error\n    if self.auto is not None and (self.num_candidates is not None or num_trials is not None):\n        raise ValueError(\"If auto is not None, num_candidates and max_steps cannot be set, since they would be overrided by the auto settings. Please either set auto to None, or do not specify num_candidates and max_steps.\")\n\n    # Set random seeds\n    seed = self.seed\n    self._set_random_seeds(seed)\n\n    # Set training &amp; validation sets\n    trainset, valset = self._set_and_validate_datasets(dataset=dataset)\n\n    # Set hyperparameters based on run mode (if set)\n    num_trials, valset, minibatch = self._set_hyperparams_from_run_mode(\n        student, num_trials, minibatch, zeroshot_opt, valset\n    )\n\n    if self.auto: \n        self._print_auto_run_settings(num_trials, minibatch, valset)\n\n    if minibatch and self.minibatch_size &gt; len(valset):\n        raise ValueError(f\"Minibatch size cannot exceed the size of the valset. Valset size: {len(valset)}.\")\n\n    # # Estimate LM calls and get user confirmation\n    if self.requires_permission_to_run:\n        if not self._get_user_confirmation(\n            student,\n            num_trials,\n            minibatch,\n            self.minibatch_size,\n            self.minibatch_full_eval_steps,\n            valset,\n            self.program_aware_proposer,\n        ):\n            logger.info(\"Compilation aborted by the user.\")\n            return student  # Return the original student program\n\n    program = student.deepcopy()\n\n    # check the evaluator (If None, will construct a default evaluator using the `evaluate` method in the benchmark) and wrap it with runtime checks\n    evaluator = self._validate_evaluator(evaluator=self.evaluator, benchmark=dataset, metric_name=metric_name)\n    self.metric = evaluator.metric\n\n    # Step 1: Bootstrap few-shot examples \n    demo_candidates = self._bootstrap_fewshot_examples(program, trainset, seed, teacher=None)\n\n    # Step 2: Propose instruction candidates \n    with suppress_cost_logging():\n        instruction_candidates = self._propose_instructions(\n            program,\n            trainset,\n            demo_candidates,\n            self.view_data_batch_size,\n            self.program_aware_proposer,\n            self.data_aware_proposer,\n            self.tip_aware_proposer,\n            self.fewshot_aware_proposer,\n        )\n\n    # Step 3: Find optimal prompt parameters \n    with suppress_cost_logging():\n        best_program = self._optimize_prompt_parameters(\n            program,\n            instruction_candidates,\n            demo_candidates,\n            evaluator,\n            valset,\n            num_trials,\n            minibatch,\n            self.minibatch_size,\n            self.minibatch_full_eval_steps,\n            seed,\n        )\n\n    if self.save_path:\n        os.makedirs(self.save_path, exist_ok=True)\n        self.best_program_path = os.path.join(self.save_path, \"best_program.json\")\n        best_program.save(self.best_program_path)\n\n    # reset the self.model. After optimization, the model will be reset to the original state.\n    # This is necessary to avoid the model being modified by the optimization process. \n    # Use self.restore_best_program() to restore the best program. \n    self.model.reset()\n</code></pre>"},{"location":"api/optimizers.html#evoagentx.optimizers.WorkFlowMiproOptimizer","title":"WorkFlowMiproOptimizer","text":"<pre><code>WorkFlowMiproOptimizer(graph: WorkFlowGraph, evaluator: Evaluator, optimizer_llm: Optional[BaseLLM] = None, **kwargs)\n</code></pre> <p>               Bases: <code>MiproOptimizer</code></p> <p>MiproOptimizer tailored for workflow graphs. </p> <p>Parameters:</p> Name Type Description Default <code>graph</code> <code>WorkFlowGraph</code> <p>the workflow graph to optimize.</p> required <code>evaluator</code> <code>Evaluator</code> <p>the evaluator to use for the optimization.</p> required <code>optimizer_llm</code> <code>BaseLLM</code> <p>the LLM to use for the optimization. If None, will use the LLM model in the evaluator.</p> <code>None</code> <code>**kwargs</code> <p>additional keyword arguments to pass to the MiproOptimizer. Available options: - metric_threshold (Optional[int]): threshold for the metric score. If provided, only examples with scores above this threshold will be used as demonstrations. - max_bootstrapped_demos (int): maximum number of bootstrapped demonstrations to use. Defaults to 4. - max_labeled_demos (int): maximum number of labeled demonstrations to use. Defaults to 4. - auto (Optional[Literal[\"light\", \"medium\", \"heavy\"]]): automatic configuration mode. If set, will override num_candidates and max_steps.      \"light\": n=6, val_size=100; \"medium\": n=12, val_size=300; \"heavy\": n=18, val_size=1000. Defaults to \"medium\". - max_steps (int): maximum number of optimization steps. Required if auto is None. - num_candidates (Optional[int]): number of candidates to generate for each optimization step. Required if auto is None. - num_threads (Optional[int]): number of threads to use for parallel evaluation. If None, will use single thread. - max_errors (int): maximum number of errors allowed during evaluation before stopping. Defaults to 10. - seed (int): random seed for reproducibility. Defaults to 9. - init_temperature (float): initial temperature for instruction generation. Defaults to 0.5. - track_stats (bool): whether to track optimization statistics. Defaults to True. - save_path (Optional[str]): path to save optimization results. If None, results will not be saved. - minibatch (bool): whether to use minibatch evaluation during optimization. Defaults to True. - minibatch_size (int): size of minibatch for evaluation. Defaults to 35. - minibatch_full_eval_steps (int): number of minibatch steps between full evaluations. Defaults to 5. - program_aware_proposer (bool): whether to use program-aware instruction proposer. Defaults to True. - data_aware_proposer (bool): whether to use data-aware instruction proposer. Defaults to True. - view_data_batch_size (int): batch size for viewing data during instruction proposal. Defaults to 10. - tip_aware_proposer (bool): whether to use tip-aware instruction proposer. Defaults to True. - fewshot_aware_proposer (bool): whether to use fewshot-aware instruction proposer. Defaults to True. - requires_permission_to_run (bool): whether to require user permission before running optimization. Defaults to False. - provide_traceback (Optional[bool]): whether to provide traceback for evaluation errors. If None, will use default setting.</p> <code>{}</code> Source code in <code>evoagentx/optimizers/mipro_optimizer.py</code> <pre><code>def __init__(\n    self, \n    graph: WorkFlowGraph,\n    evaluator: Evaluator, \n    optimizer_llm: Optional[BaseLLM] = None, \n    **kwargs, \n):\n    \"\"\"\n    MiproOptimizer tailored for workflow graphs. \n\n    Args:\n        graph (WorkFlowGraph): the workflow graph to optimize.\n        evaluator (Evaluator): the evaluator to use for the optimization.\n        optimizer_llm (BaseLLM): the LLM to use for the optimization. If None, will use the LLM model in the evaluator.\n        **kwargs: additional keyword arguments to pass to the MiproOptimizer. Available options:\n            - metric_threshold (Optional[int]): threshold for the metric score. If provided, only examples with scores above this threshold will be used as demonstrations.\n            - max_bootstrapped_demos (int): maximum number of bootstrapped demonstrations to use. Defaults to 4.\n            - max_labeled_demos (int): maximum number of labeled demonstrations to use. Defaults to 4.\n            - auto (Optional[Literal[\"light\", \"medium\", \"heavy\"]]): automatic configuration mode. If set, will override num_candidates and max_steps. \n                \"light\": n=6, val_size=100; \"medium\": n=12, val_size=300; \"heavy\": n=18, val_size=1000. Defaults to \"medium\".\n            - max_steps (int): maximum number of optimization steps. Required if auto is None.\n            - num_candidates (Optional[int]): number of candidates to generate for each optimization step. Required if auto is None.\n            - num_threads (Optional[int]): number of threads to use for parallel evaluation. If None, will use single thread.\n            - max_errors (int): maximum number of errors allowed during evaluation before stopping. Defaults to 10.\n            - seed (int): random seed for reproducibility. Defaults to 9.\n            - init_temperature (float): initial temperature for instruction generation. Defaults to 0.5.\n            - track_stats (bool): whether to track optimization statistics. Defaults to True.\n            - save_path (Optional[str]): path to save optimization results. If None, results will not be saved.\n            - minibatch (bool): whether to use minibatch evaluation during optimization. Defaults to True.\n            - minibatch_size (int): size of minibatch for evaluation. Defaults to 35.\n            - minibatch_full_eval_steps (int): number of minibatch steps between full evaluations. Defaults to 5.\n            - program_aware_proposer (bool): whether to use program-aware instruction proposer. Defaults to True.\n            - data_aware_proposer (bool): whether to use data-aware instruction proposer. Defaults to True.\n            - view_data_batch_size (int): batch size for viewing data during instruction proposal. Defaults to 10.\n            - tip_aware_proposer (bool): whether to use tip-aware instruction proposer. Defaults to True.\n            - fewshot_aware_proposer (bool): whether to use fewshot-aware instruction proposer. Defaults to True.\n            - requires_permission_to_run (bool): whether to require user permission before running optimization. Defaults to False.\n            - provide_traceback (Optional[bool]): whether to provide traceback for evaluation errors. If None, will use default setting.\n    \"\"\"\n\n    # check if the graph is compatible with the WorkFlowMipro optimizer.\n    graph = self._validate_graph_compatibility(graph=graph)\n\n    # convert the workflow graph to a callable program  \n    workflow_graph_program = WorkFlowGraphProgram(\n        graph=graph, \n        agent_manager=evaluator.agent_manager, \n        executor_llm=evaluator.llm, \n        collate_func=evaluator.collate_func, \n        output_postprocess_func=evaluator.output_postprocess_func, \n    )\n\n    # register optimizable parameters \n    registry = self._register_optimizable_parameters(program=workflow_graph_program)\n\n    super().__init__(\n        registry=registry, \n        program=workflow_graph_program, \n        optimizer_llm=optimizer_llm or evaluator.llm, \n        evaluator=evaluator,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/storages.html","title":"\ud83d\udcbe Storages","text":""},{"location":"api/storages.html#evoagentx.storages","title":"evoagentx.storages","text":""},{"location":"api/storages.html#evoagentx.storages.StorageHandler","title":"StorageHandler","text":"<pre><code>StorageHandler(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Implementation of a storage handler for managing various storage backends.</p> <p>StorageHandler provides an abstraction for reading and writing data (e.g., memory, agents, workflows). It supports multiple storage types, including database, vector, and graph storage, initialized via factories.</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.init_module","title":"init_module","text":"<pre><code>init_module()\n</code></pre> <p>Initialize all storage backends based on the provided configuration. Calls individual initialization methods for database, vector, and graph stores.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def init_module(self):\n    \"\"\"\n    Initialize all storage backends based on the provided configuration.\n    Calls individual initialization methods for database, vector, and graph stores.\n    \"\"\"\n    # Create the path\n    if (self.storageConfig.path is not None) or (self.storageConfig.path != \":memory:\") \\\n        or (not self.storageConfig.path):\n        os.makedirs(os.path.dirname(self.storageConfig.path), exist_ok=True)\n\n    self._init_db_store()\n    self._init_vector_store()\n    self._init_graph_store()\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.load","title":"load","text":"<pre><code>load(tables: Optional[List[str]] = None, *args, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load all data from the database storage.</p> <p>Attributes:</p> Name Type Description <code>tables</code> <code>Optional[List[str]]</code> <p>List of table names to load; if None, loads all tables.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Dict[str, str]]: A dictionary with table names as keys and lists of records as values. You should parse the values by yourself.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def load(self, tables: Optional[List[str]] = None, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load all data from the database storage.\n\n    Attributes:\n        tables (Optional[List[str]]): List of table names to load; if None, loads all tables.\n\n    Returns:\n        Dict[str, Dict[str, str]]: A dictionary with table names as keys and lists of records as values. You should parse the values by yourself.\n    \"\"\"\n    result = {}\n    table_info = self.storageDB.col_info()\n\n    if tables is None:\n        tables_to_load = [t.value for t in TableType]\n    else:\n        tables_to_load = tables\n\n    # Load data for each table\n    for table_name in tables_to_load:\n        table_data = []\n        # Check if the table exists\n        if any(t[\"table_name\"] == table_name for t in table_info):\n            cursor = self.storageDB.connection.cursor()\n            cursor.execute(f\"SELECT * FROM {table_name}\")\n            # Get column names from the columns dictionary\n            columns = next(t[\"columns\"].keys() for t in table_info if t[\"table_name\"] == table_name)\n            rows = cursor.fetchall()\n            table_data = [dict(zip(columns, row)) for row in rows]\n        result[table_name] = table_data\n\n    return result\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.save","title":"save","text":"<pre><code>save(data: Dict[str, Any], *args, **kwargs)\n</code></pre> <p>Save all provided data to the database storage.</p> <p>Attributes:</p> Name Type Description <code>data</code> <code>Dict[str, Any]</code> <p>Dictionary with table names as keys and lists of records to save.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If an unknown table name is provided.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def save(self, data: Dict[str, Any], *args, **kwargs):\n    \"\"\"\n    Save all provided data to the database storage.\n\n    Attributes:\n        data (Dict[str, Any]): Dictionary with table names as keys and lists of records to save.\n\n    Raises:\n        ValueError: If an unknown table name is provided.\n    \"\"\"\n    for table_name, records in data.items():\n        store_type = None\n        # Map table name to store_type\n        for st in TableType:\n            if st.value == table_name:\n                store_type = st\n                break\n        if store_type is None:\n            raise ValueError(f\"Unknown table: {table_name}\")\n        # Insert each record\n        for record in records:\n            self.storageDB.insert(metadata=record, store_type=store_type, table=table_name)\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.parse_result","title":"parse_result","text":"<pre><code>parse_result(results: Dict[str, str], store: Union[AgentStore, WorkflowStore, MemoryStore, HistoryStore]) -&gt; Dict[str, Any]\n</code></pre> <p>Parse database results, converting JSON strings to Python objects where applicable.</p> <p>Attributes:</p> Name Type Description <code>results</code> <code>Dict[str, str]</code> <p>Raw database results with column names as keys.</p> <code>store</code> <code>Union[AgentStore, WorkflowStore, MemoryStore, HistoryStore]</code> <p>Pydantic model for validation.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Parsed results with JSON strings deserialized to Python objects.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def parse_result(self, results: Dict[str, str], \n                 store: Union[AgentStore, WorkflowStore, MemoryStore, HistoryStore]) -&gt; Dict[str, Any]:\n    \"\"\"\n    Parse database results, converting JSON strings to Python objects where applicable.\n\n    Attributes:\n        results (Dict[str, str]): Raw database results with column names as keys.\n        store (Union[AgentStore, WorkflowStore, MemoryStore, HistoryStore]): Pydantic model for validation.\n\n    Returns:\n        Dict[str, Any]: Parsed results with JSON strings deserialized to Python objects.\n    \"\"\"\n    for k, v in store.model_fields.items():\n        if v.annotation not in [Optional[str], str]:\n            try:\n                results[k] = json.loads(results[k])\n            except (json.JSONDecodeError, KeyError, TypeError):\n                results[k] = results.get(k)\n    return results\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.load_memory","title":"load_memory","text":"<pre><code>load_memory(memory_id: str, table: Optional[str] = None, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single long-term memory data.</p> <p>Attributes:</p> Name Type Description <code>memory_id</code> <code>str</code> <p>The ID of the long-term memory.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'memory' if None.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The data that can be used to create a LongTermMemory instance.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def load_memory(self, memory_id: str, table: Optional[str]=None, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load a single long-term memory data.\n\n    Attributes:\n        memory_id (str): The ID of the long-term memory.\n        table (Optional[str]): The table name; defaults to 'memory' if None.\n\n    Returns:\n        Dict[str, Any]: The data that can be used to create a LongTermMemory instance.\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.save_memory","title":"save_memory","text":"<pre><code>save_memory(memory_data: Dict[str, Any], table: Optional[str] = None, **kwargs)\n</code></pre> <p>Save or update a single memory.</p> <p>Attributes:</p> Name Type Description <code>memory_data</code> <code>Dict[str, Any]</code> <p>The long-term memory's data.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'memory' if None.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def save_memory(self, memory_data: Dict[str, Any], table: Optional[str]=None, **kwargs):\n    \"\"\"\n    Save or update a single memory.\n\n    Attributes:\n        memory_data (Dict[str, Any]): The long-term memory's data.\n        table (Optional[str]): The table name; defaults to 'memory' if None.\n\n    \"\"\"\n    pass\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.load_agent","title":"load_agent","text":"<pre><code>load_agent(agent_name: str, table: Optional[str] = None, *args, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single agent's data.</p> <p>Attributes:</p> Name Type Description <code>agent_name</code> <code>str</code> <p>The unique name of the agent to retrieve.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'agent' if None.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The data that can be used to create an Agent instance, or None if not found.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def load_agent(self, agent_name: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load a single agent's data.\n\n    Attributes:\n        agent_name (str): The unique name of the agent to retrieve.\n        table (Optional[str]): The table name; defaults to 'agent' if None.\n\n    Returns:\n        Dict[str, Any]: The data that can be used to create an Agent instance, or None if not found.\n    \"\"\"\n    table = table or TableType.store_agent.value\n    result = self.storageDB.get_by_id(agent_name, store_type=\"agent\", table=table)\n    # Parse the result to convert JSON strings to Python objects\n    if result is not None:\n        result = self.parse_result(result, AgentStore)\n    return result\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.remove_agent","title":"remove_agent","text":"<pre><code>remove_agent(agent_name: str, table: Optional[str] = None, *args, **kwargs)\n</code></pre> <p>Remove an agent from storage if the agent exists.</p> <p>Attributes:</p> Name Type Description <code>agent_name</code> <code>str</code> <p>The name of the agent to be deleted.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'agent' if None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If the agent does not exist in the specified table.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def remove_agent(self, agent_name: str, table: Optional[str]=None, *args, **kwargs):\n    \"\"\"\n    Remove an agent from storage if the agent exists.\n\n    Attributes:\n        agent_name (str): The name of the agent to be deleted.\n        table (Optional[str]): The table name; defaults to 'agent' if None.\n\n    Raises:\n        ValueError: If the agent does not exist in the specified table.\n    \"\"\"\n    table = table or TableType.store_agent.value\n    success = self.storageDB.delete(agent_name, store_type=\"agent\", table=table)\n    if not success:\n        raise ValueError(f\"Agent with name {agent_name} not found in table {table}\")\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.save_agent","title":"save_agent","text":"<pre><code>save_agent(agent_data: Dict[str, Any], table: Optional[str] = None, *args, **kwargs)\n</code></pre> <p>Save or update a single agent's data.</p> <p>Attributes:</p> Name Type Description <code>agent_data</code> <code>Dict[str, Any]</code> <p>The agent's data, must include 'name' and 'content' keys.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'agent' if None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'name' field is missing or if Pydantic validation fails.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def save_agent(self, agent_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs):\n    \"\"\"\n    Save or update a single agent's data.\n\n    Attributes:\n        agent_data (Dict[str, Any]): The agent's data, must include 'name' and 'content' keys.\n        table (Optional[str]): The table name; defaults to 'agent' if None.\n\n    Raises:\n        ValueError: If 'name' field is missing or if Pydantic validation fails.\n    \"\"\"\n    table = table or TableType.store_agent.value\n    agent_name = agent_data.get(\"name\")\n    if not agent_name:\n        raise ValueError(\"Agent data must include a 'name' field\")\n\n    existing = self.storageDB.get_by_id(agent_name, store_type=\"agent\", table=table)\n    if existing:\n        self.storageDB.update(agent_name, new_metadata=agent_data, store_type=\"agent\", table=table)\n    else:\n        self.storageDB.insert(metadata=agent_data, store_type=\"agent\", table=table)\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.load_workflow","title":"load_workflow","text":"<pre><code>load_workflow(workflow_id: str, table: Optional[str] = None, *args, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single workflow's data.</p> <p>Attributes:</p> Name Type Description <code>workflow_id</code> <code>str</code> <p>The ID of the workflow.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'workflow' if None.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The data that can be used to create a WorkFlow instance, or None if not found.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def load_workflow(self, workflow_id: str, table: Optional[str] = None, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load a single workflow's data.\n\n    Attributes:\n        workflow_id (str): The ID of the workflow.\n        table (Optional[str]): The table name; defaults to 'workflow' if None.\n\n    Returns:\n        Dict[str, Any]: The data that can be used to create a WorkFlow instance, or None if not found.\n    \"\"\"\n    table = table or TableType.store_workflow.value\n    result = self.storageDB.get_by_id(workflow_id, store_type=\"workflow\", table=table)\n    # Parse the result to convert JSON strings to Python objects\n    if result is not None:\n        result = self.parse_result(result, WorkflowStore)\n    return result\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.save_workflow","title":"save_workflow","text":"<pre><code>save_workflow(workflow_data: Dict[str, Any], table: Optional[str] = None, *args, **kwargs)\n</code></pre> <p>Save or update a workflow's data.</p> <p>Attributes:</p> Name Type Description <code>workflow_data</code> <code>Dict[str, Any]</code> <p>The workflow's data, must include 'name' field.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'workflow' if None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'name' field is missing or if Pydantic validation fails.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def save_workflow(self, workflow_data: Dict[str, Any], table: Optional[str] = None, *args, **kwargs):\n    \"\"\"\n    Save or update a workflow's data.\n\n    Attributes:\n        workflow_data (Dict[str, Any]): The workflow's data, must include 'name' field.\n        table (Optional[str]): The table name; defaults to 'workflow' if None.\n\n    Raises:\n        ValueError: If 'name' field is missing or if Pydantic validation fails.\n    \"\"\"\n    table = table or TableType.store_workflow.value\n    workflow_id = workflow_data.get(\"name\")\n    if not workflow_id:\n        raise ValueError(\"Workflow data must include a 'name' field\")\n    # Check if workflow exists to decide between insert or update\n    existing = self.storageDB.get_by_id(workflow_id, store_type=\"workflow\", table=table)\n    if existing:\n\n        self.storageDB.update(workflow_id, new_metadata=workflow_data, store_type=\"workflow\", table=table)\n    else:\n        self.storageDB.insert(metadata=workflow_data, store_type=\"workflow\", table=table)\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.load_history","title":"load_history","text":"<pre><code>load_history(memory_id: str, table: Optional[str] = None, *args, **kwargs) -&gt; Dict[str, Any]\n</code></pre> <p>Load a single history entry.</p> <p>Attributes:</p> Name Type Description <code>memory_id</code> <code>str</code> <p>The ID of the memory associated with the history entry.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'history' if None.</p> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: The history data, or None if not found.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def load_history(self, memory_id: str, table: Optional[str] = None, *args, **kwargs) -&gt; Dict[str, Any]:\n    \"\"\"\n    Load a single history entry.\n\n    Attributes:\n        memory_id (str): The ID of the memory associated with the history entry.\n        table (Optional[str]): The table name; defaults to 'history' if None.\n\n    Returns:\n        Dict[str, Any]: The history data, or None if not found.\n    \"\"\"\n    table = table or TableType.store_history.value\n    result = self.storageDB.get_by_id(memory_id, store_type=\"history\", table=table)\n    # Parse the result to convert JSON strings to Python objects (if any)\n    if result is not None:\n        result = self.parse_result(result, HistoryStore)\n    return result\n</code></pre>"},{"location":"api/storages.html#evoagentx.storages.StorageHandler.save_history","title":"save_history","text":"<pre><code>save_history(history_data: Dict[str, Any], table: Optional[str] = None, *args, **kwargs)\n</code></pre> <p>Save or update a single history entry.</p> <p>Attributes:</p> Name Type Description <code>history_data</code> <code>Dict[str, Any]</code> <p>The history data, must include 'memory_id' field.</p> <code>table</code> <code>Optional[str]</code> <p>The table name; defaults to 'history' if None.</p> <p>Raises:</p> Type Description <code>ValueError</code> <p>If 'memory_id' field is missing or if Pydantic validation fails.</p> Source code in <code>evoagentx/storages/base.py</code> <pre><code>def save_history(self, history_data: Dict[str, Any], table: Optional[str] = None, *args, **kwargs):\n    \"\"\"\n    Save or update a single history entry.\n\n    Attributes:\n        history_data (Dict[str, Any]): The history data, must include 'memory_id' field.\n        table (Optional[str]): The table name; defaults to 'history' if None.\n\n    Raises:\n        ValueError: If 'memory_id' field is missing or if Pydantic validation fails.\n    \"\"\"\n    table = table or TableType.store_history.value\n    memory_id = history_data.get(\"memory_id\")\n    if not memory_id:\n        raise ValueError(\"History data must include a 'memory_id' field\")\n    # Check if history entry exists to decide between insert or update\n    existing = self.storageDB.get_by_id(memory_id, store_type=\"history\", table=table)\n    if existing:\n        # parse the history, then change the old_hisotry\n        result = HistoryStore.model_validate(self.parse_result(existing, HistoryStore))\n        history_data[\"old_memory\"] = result.old_memory\n        self.storageDB.update(memory_id, new_metadata=history_data, store_type=\"history\", table=table)\n    else:\n        self.storageDB.insert(metadata=history_data, store_type=\"history\", table=table)\n</code></pre>"},{"location":"api/tools.html","title":"\ud83d\udee0\ufe0f Tools","text":""},{"location":"api/tools.html#evoagentx.tools","title":"evoagentx.tools","text":""},{"location":"api/tools.html#evoagentx.tools.Tool","title":"Tool","text":"<pre><code>Tool(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Base interface for all tools. All tools must implement: - <code>get_tool_schemas</code>: Returns the OpenAI-compatible function schema - <code>get_tools</code>: Returns a list of callable functions for all tools - <code>get_tool_descriptions</code>: Returns a list of descriptions for all tools</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.Tool.get_tool_schemas","title":"get_tool_schemas","text":"<pre><code>get_tool_schemas() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Returns the OpenAI-compatible function schema for this tool. The schema follows the format used by MCP servers and OpenAI function calling.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>Dict[str, Any]: The function schema in OpenAI format</p> Source code in <code>evoagentx/tools/tool.py</code> <pre><code>def get_tool_schemas(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Returns the OpenAI-compatible function schema for this tool.\n    The schema follows the format used by MCP servers and OpenAI function calling.\n\n    Returns:\n        Dict[str, Any]: The function schema in OpenAI format\n    \"\"\"\n    raise NotImplementedError(\"All tools must implement get_tool_schemas\")\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.Tool.get_tools","title":"get_tools","text":"<pre><code>get_tools() -&gt; List[Callable]\n</code></pre> <p>Returns a list of callable functions for all tools</p> <p>Returns:</p> Type Description <code>List[Callable]</code> <p>List[Callable]: A list of callable functions</p> Source code in <code>evoagentx/tools/tool.py</code> <pre><code>def get_tools(self) -&gt; List[Callable]:\n    \"\"\"\n    Returns a list of callable functions for all tools\n\n    Returns:\n        List[Callable]: A list of callable functions\n    \"\"\"\n    raise NotImplementedError(\"All tools must implement get_tools\")\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.BaseInterpreter","title":"BaseInterpreter","text":"<pre><code>BaseInterpreter(name: str = 'BaseInterpreter', **kwargs)\n</code></pre> <p>               Bases: <code>Tool</code></p> <p>Base class for interpreter tools that execute code securely. Implements the standard tool interface with get_tool_schemas and execute methods.</p> Source code in <code>evoagentx/tools/interpreter_base.py</code> <pre><code>def __init__(\n    self, \n    name: str = 'BaseInterpreter',\n    **kwargs\n):\n    super().__init__(name=name, **kwargs)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.DockerInterpreter","title":"DockerInterpreter","text":"<pre><code>DockerInterpreter(name: str = 'DockerInterpreter', image_tag: str = None, dockerfile_path: str = None, require_confirm: bool = False, print_stdout: bool = True, print_stderr: bool = True, host_directory: str = '', container_directory: str = '/home/app/', container_command: str = 'tail -f /dev/null', tmp_directory: str = '/tmp', **data)\n</code></pre> <p>               Bases: <code>BaseInterpreter</code></p> <p>A Docker-based interpreter for executing Python, Bash, and R scripts in an isolated environment.</p> <p>Initialize a Docker-based interpreter for executing code in an isolated environment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the interpreter</p> <code>'DockerInterpreter'</code> <code>image_tag</code> <code>str</code> <p>The Docker image tag to use. Must be provided if dockerfile_path is not.</p> <code>None</code> <code>dockerfile_path</code> <code>str</code> <p>Path to the Dockerfile to build. Must be provided if image_tag is not.</p> <code>None</code> <code>require_confirm</code> <code>bool</code> <p>Whether to require confirmation before executing code</p> <code>False</code> <code>print_stdout</code> <code>bool</code> <p>Whether to print stdout from code execution</p> <code>True</code> <code>print_stderr</code> <code>bool</code> <p>Whether to print stderr from code execution</p> <code>True</code> <code>host_directory</code> <code>str</code> <p>The path to the host directory to mount in the container</p> <code>''</code> <code>container_directory</code> <code>str</code> <p>The target directory inside the container</p> <code>'/home/app/'</code> <code>container_command</code> <code>str</code> <p>The command to run in the container</p> <code>'tail -f /dev/null'</code> <code>tmp_directory</code> <code>str</code> <p>The temporary directory to use for file creation in the container</p> <code>'/tmp'</code> <code>**data</code> <p>Additional data to pass to the parent class</p> <code>{}</code> Source code in <code>evoagentx/tools/interpreter_docker.py</code> <pre><code>def __init__(\n    self, \n    name:str = \"DockerInterpreter\",\n    image_tag:str = None,\n    dockerfile_path:str = None,\n    require_confirm:bool = False,\n    print_stdout:bool = True,\n    print_stderr:bool = True,\n    host_directory:str = \"\",\n    container_directory:str = \"/home/app/\",\n    container_command:str = \"tail -f /dev/null\",\n    tmp_directory:str = \"/tmp\",\n    **data\n):\n    \"\"\"\n    Initialize a Docker-based interpreter for executing code in an isolated environment.\n\n    Args:\n        name (str): The name of the interpreter\n        image_tag (str, optional): The Docker image tag to use. Must be provided if dockerfile_path is not.\n        dockerfile_path (str, optional): Path to the Dockerfile to build. Must be provided if image_tag is not.\n        require_confirm (bool): Whether to require confirmation before executing code\n        print_stdout (bool): Whether to print stdout from code execution\n        print_stderr (bool): Whether to print stderr from code execution\n        host_directory (str): The path to the host directory to mount in the container\n        container_directory (str): The target directory inside the container\n        container_command (str): The command to run in the container\n        tmp_directory (str): The temporary directory to use for file creation in the container\n        **data: Additional data to pass to the parent class\n    \"\"\"\n    # Extract or generate schemas, descriptions, tools\n    schemas = data.pop('schemas', None) or self.get_tool_schemas()\n    descriptions = data.pop('descriptions', None) or self.get_tool_descriptions()\n    tools = data.pop('tools', None)\n    tools = self.get_tools()\n\n    # Pass these to the parent class initialization\n    super().__init__(\n        name=name,\n        schemas=schemas,\n        descriptions=descriptions,\n        tools=tools,\n        **data\n    )\n    self.require_confirm = require_confirm\n    self.print_stdout = print_stdout\n    self.print_stderr = print_stderr\n    self.host_directory = host_directory\n    self.container_directory = container_directory\n    self.container_command = container_command\n    self.tmp_directory = tmp_directory\n\n    # Initialize Docker client and container\n    self.client = docker.from_env()\n    self.container = None\n    self.image_tag = image_tag\n    self.dockerfile_path = dockerfile_path\n    self._initialize_if_needed()\n\n    # Upload directory if specified\n    if self.host_directory:\n        self._upload_directory_to_container(self.host_directory)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.DockerInterpreter.execute","title":"execute","text":"<pre><code>execute(code: str, language: str) -&gt; str\n</code></pre> <p>Executes code in a Docker container.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The code to execute</p> required <code>language</code> <code>str</code> <p>The programming language to use</p> required <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The execution output</p> <p>Raises:</p> Type Description <code>RuntimeError</code> <p>If container is not properly initialized or execution fails</p> <code>ValueError</code> <p>If code content is invalid or exceeds limits</p> Source code in <code>evoagentx/tools/interpreter_docker.py</code> <pre><code>def execute(self, code: str, language: str) -&gt; str:\n    \"\"\"\n    Executes code in a Docker container.\n\n    Args:\n        code (str): The code to execute\n        language (str): The programming language to use\n\n    Returns:\n        str: The execution output\n\n    Raises:\n        RuntimeError: If container is not properly initialized or execution fails\n        ValueError: If code content is invalid or exceeds limits\n    \"\"\"\n    if not code or not code.strip():\n        raise ValueError(\"Code content cannot be empty\")\n\n    if not self.container:\n        raise RuntimeError(\"Container is not initialized\")\n\n    # Check container status\n    try:\n        container_info = self.client.api.inspect_container(self.container.id)\n        if not container_info['State']['Running']:\n            raise RuntimeError(\"Container is not running\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to check container status: {e}\")\n\n    if self.host_directory:\n        code = f\"import sys; sys.path.insert(0, '{self.container_directory}');\" + code\n\n    language = self._check_language(language)\n\n    if self.require_confirm:\n        confirmation = input(f\"Confirm execution of {language} code? [Y/n]: \")\n        if confirmation.lower() not in [\"y\", \"yes\", \"\"]:\n            raise RuntimeError(\"Execution aborted by user.\")\n\n    try:\n        file_path = self._create_file_in_container(code)\n        return self._run_file_in_container(file_path, language)\n    except Exception as e:\n        raise RuntimeError(f\"Code execution failed: {e}\")\n    finally:\n        # Clean up temporary files\n        try:\n            if hasattr(self, 'container') and self.container:\n                self.container.exec_run(f\"rm -f {file_path}\")\n        except Exception:\n            pass  # Ignore cleanup errors\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.DockerInterpreter.execute_script","title":"execute_script","text":"<pre><code>execute_script(file_path: str, language: str = None) -&gt; str\n</code></pre> <p>Reads code from a file and executes it in a Docker container.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the script file to execute</p> required <code>language</code> <code>str</code> <p>The programming language of the code. If None, will be determined from the file extension.</p> <code>None</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The execution output</p> <p>Raises:</p> Type Description <code>FileNotFoundError</code> <p>If the script file does not exist</p> <code>RuntimeError</code> <p>If container is not properly initialized or execution fails</p> <code>ValueError</code> <p>If file content is invalid or exceeds limits</p> Source code in <code>evoagentx/tools/interpreter_docker.py</code> <pre><code>def execute_script(self, file_path: str, language: str = None) -&gt; str:\n    \"\"\"\n    Reads code from a file and executes it in a Docker container.\n\n    Args:\n        file_path (str): The path to the script file to execute\n        language (str, optional): The programming language of the code. If None, will be determined from the file extension.\n\n    Returns:\n        str: The execution output\n\n    Raises:\n        FileNotFoundError: If the script file does not exist\n        RuntimeError: If container is not properly initialized or execution fails\n        ValueError: If file content is invalid or exceeds limits\n    \"\"\"\n    # Check if file exists and is readable\n    if not os.path.isfile(file_path):\n        raise FileNotFoundError(f\"Script file not found: {file_path}\")\n\n    if not os.access(file_path, os.R_OK):\n        raise PermissionError(f\"Cannot read script file: {file_path}\")\n\n    # Read the file content\n    try:\n        with open(file_path, 'r', encoding='utf-8') as f:\n            code = f.read()\n    except Exception as e:\n        raise RuntimeError(f\"Failed to read script file: {e}\")\n\n    # Execute the code\n    return self.execute(code, language)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.DockerInterpreter.get_tool_schemas","title":"get_tool_schemas","text":"<pre><code>get_tool_schemas() -&gt; list[Dict[str, Any]]\n</code></pre> <p>Returns the OpenAI-compatible function schema for the Docker interpreter.</p> <p>Returns:</p> Type Description <code>list[Dict[str, Any]]</code> <p>list[Dict[str, Any]]: Function schema in OpenAI format</p> Source code in <code>evoagentx/tools/interpreter_docker.py</code> <pre><code>def get_tool_schemas(self) -&gt; list[Dict[str, Any]]:\n    \"\"\"\n    Returns the OpenAI-compatible function schema for the Docker interpreter.\n\n    Returns:\n        list[Dict[str, Any]]: Function schema in OpenAI format\n    \"\"\"\n    schemas = [\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"execute\",\n                \"description\": \"Execute code in a secure Docker container environment.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"code\": {\n                            \"type\": \"string\",\n                            \"description\": \"The code to execute\"\n                        },\n                        \"language\": {\n                            \"type\": \"string\",\n                            \"description\": \"The programming language of the code (e.g., python, py, python3)\",\n                            \"enum\": list(self.CODE_TYPE_MAPPING.keys())\n                        }\n                    },\n                    \"required\": [\"code\", \"language\"]\n                }\n            }\n        },\n        {\n            \"type\": \"function\",\n            \"function\": {\n                \"name\": \"execute_script\",\n                \"description\": \"Execute code from a script file in a secure Docker container environment.\",\n                \"parameters\": {\n                    \"type\": \"object\",\n                    \"properties\": {\n                        \"file_path\": {\n                            \"type\": \"string\",\n                            \"description\": \"The path to the script file to execute\"\n                        },\n                        \"language\": {\n                            \"type\": \"string\",\n                            \"description\": \"The programming language of the code. If not provided, will be determined from file extension.\",\n                            \"enum\": list(self.CODE_TYPE_MAPPING.keys())\n                        }\n                    },\n                    \"required\": [\"file_path\"]\n                }\n            }\n        }\n    ]\n    return schemas\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.DockerInterpreter.get_tool_descriptions","title":"get_tool_descriptions","text":"<pre><code>get_tool_descriptions() -&gt; List[str]\n</code></pre> <p>Returns a brief description of the Docker interpreter tool.</p> <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: Tool description</p> Source code in <code>evoagentx/tools/interpreter_docker.py</code> <pre><code>def get_tool_descriptions(self) -&gt; List[str]:\n    \"\"\"\n    Returns a brief description of the Docker interpreter tool.\n\n    Returns:\n        List[str]: Tool description\n    \"\"\"\n    return [\n        \"Execute code in a secure Docker container environment.\",\n        \"Execute code from script files in a secure Docker container environment.\"\n    ]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.DockerInterpreter.get_tools","title":"get_tools","text":"<pre><code>get_tools() -&gt; List[callable]\n</code></pre> <p>Returns a list of callable methods provided by this tool.</p> <p>Returns:</p> Type Description <code>List[callable]</code> <p>List[callable]: List of callable methods</p> Source code in <code>evoagentx/tools/interpreter_docker.py</code> <pre><code>def get_tools(self) -&gt; List[callable]:\n    \"\"\"\n    Returns a list of callable methods provided by this tool.\n\n    Returns:\n        List[callable]: List of callable methods\n    \"\"\"\n    return [self.execute, self.execute_script]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.PythonInterpreter","title":"PythonInterpreter","text":"<pre><code>PythonInterpreter(name: str = 'PythonInterpreter', project_path: Optional[str] = '.', directory_names: Optional[List[str]] = [], allowed_imports: Optional[Set[str]] = None, **kwargs)\n</code></pre> <p>               Bases: <code>BaseInterpreter</code></p> <p>Initialize a Python interpreter for executing code in a controlled environment.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the interpreter</p> <code>'PythonInterpreter'</code> <code>project_path</code> <code>Optional[str]</code> <p>Path to the project directory for module resolution</p> <code>'.'</code> <code>directory_names</code> <code>Optional[List[str]]</code> <p>List of directory names to check for imports</p> <code>[]</code> <code>allowed_imports</code> <code>Optional[Set[str]]</code> <p>Set of allowed module imports to enforce security</p> <code>None</code> <code>**kwargs</code> <p>Additional data to pass to the parent class</p> <code>{}</code> Source code in <code>evoagentx/tools/interpreter_python.py</code> <pre><code>def __init__(\n    self, \n    name: str = 'PythonInterpreter',\n    project_path:Optional[str] = \".\",\n    directory_names:Optional[List[str]] = [],\n    allowed_imports:Optional[Set[str]] = None,\n    **kwargs\n):\n    \"\"\"\n    Initialize a Python interpreter for executing code in a controlled environment.\n\n    Args:\n        name (str): The name of the interpreter\n        project_path (Optional[str]): Path to the project directory for module resolution\n        directory_names (Optional[List[str]]): List of directory names to check for imports\n        allowed_imports (Optional[Set[str]]): Set of allowed module imports to enforce security\n        **kwargs: Additional data to pass to the parent class\n    \"\"\"\n    super().__init__(\n        name=name, \n        project_path=project_path,\n        directory_names=directory_names,\n        allowed_imports=allowed_imports,\n        **kwargs\n    )\n    self.allowed_imports = allowed_imports or set()\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.PythonInterpreter.execute","title":"execute","text":"<pre><code>execute(code: str, language: str = 'python') -&gt; str\n</code></pre> <p>Analyzes and executes the provided Python code in a controlled environment.</p> <p>NOTE: This method only returns content printed to stdout during execution. It does not return any values from the code itself. To see results, use print statements in your code.</p> <p>WARNING: This method uses Python's exec() function internally, which executes code with full privileges. While safety checks are performed, there is still a security risk. Do not use with untrusted code.</p> <p>Parameters:</p> Name Type Description Default <code>code</code> <code>str</code> <p>The Python code to execute.</p> required <code>language</code> <code>str</code> <p>The programming language of the code. Defaults to \"python\".</p> <code>'python'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The output of the executed code (printed content only), or a list of violations if found.</p> Source code in <code>evoagentx/tools/interpreter_python.py</code> <pre><code>def execute(self, code: str, language: str = \"python\") -&gt; str:\n    \"\"\"\n    Analyzes and executes the provided Python code in a controlled environment.\n\n    NOTE: This method only returns content printed to stdout during execution.\n    It does not return any values from the code itself. To see results, use\n    print statements in your code.\n\n    WARNING: This method uses Python's exec() function internally, which executes\n    code with full privileges. While safety checks are performed, there is still\n    a security risk. Do not use with untrusted code.\n\n    Args:\n        code (str): The Python code to execute.\n        language (str, optional): The programming language of the code. Defaults to \"python\".\n\n    Returns:\n        str: The output of the executed code (printed content only), or a list of violations if found.\n    \"\"\"\n    # Verify language is python\n    if language.lower() != \"python\":\n        return f\"Error: This interpreter only supports Python language. Received: {language}\"\n\n    self.visited_modules = {}\n    self.namespace = {}\n\n    # Change to the project directory and update sys.path for module resolution\n    if not self.project_path:\n        raise ValueError(\"Project path (project_path) is not set\")\n\n    if not os.path.exists(self.project_path):\n        raise ValueError(f\"Project path '{self.project_path}' does not exist\")\n\n    if not os.path.isdir(self.project_path):\n        raise ValueError(f\"Project path '{self.project_path}' is not a directory\")\n\n    os.chdir(self.project_path)\n    sys.path.insert(0, self.project_path)\n\n    if self.allowed_imports:\n        violations = self._analyze_code(code)\n        if violations:\n            return\"\\n\".join(violations)\n\n\n    # Capture standard output during execution\n    stdout_capture = io.StringIO()\n    with contextlib.redirect_stdout(stdout_capture):\n        try:\n            # Execute the code with basic builtins\n            exec(code, {\"__builtins__\": __builtins__})\n        except Exception:\n            exc_type, exc_value, exc_tb = sys.exc_info()\n            error_msg = \"\".join(traceback.format_exception(exc_type, exc_value, exc_tb))\n            return error_msg\n\n    # Retrieve and return the captured output\n    return stdout_capture.getvalue().strip()\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.PythonInterpreter.execute_script","title":"execute_script","text":"<pre><code>execute_script(file_path: str, language: str = 'python') -&gt; str\n</code></pre> <p>Reads Python code from a file and executes it using the <code>execute</code> method.</p> <p>NOTE: This method only returns content printed to stdout during execution. It does not return any values from the code itself. To see results, use print statements in your code.</p> <p>WARNING: This method uses Python's exec() function internally, which executes code with full privileges. While safety checks are performed, there is still a security risk. Do not use with untrusted code.</p> <p>Parameters:</p> Name Type Description Default <code>file_path</code> <code>str</code> <p>The path to the Python file to be executed.</p> required <code>language</code> <code>str</code> <p>The programming language of the code. Defaults to \"python\".</p> <code>'python'</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The output of the executed code (printed content only), or an error message if the execution fails.</p> Source code in <code>evoagentx/tools/interpreter_python.py</code> <pre><code>def execute_script(self, file_path: str, language: str = \"python\") -&gt; str:\n    \"\"\"\n    Reads Python code from a file and executes it using the `execute` method.\n\n    NOTE: This method only returns content printed to stdout during execution.\n    It does not return any values from the code itself. To see results, use\n    print statements in your code.\n\n    WARNING: This method uses Python's exec() function internally, which executes\n    code with full privileges. While safety checks are performed, there is still\n    a security risk. Do not use with untrusted code.\n\n    Args:\n        file_path (str): The path to the Python file to be executed.\n        language (str, optional): The programming language of the code. Defaults to \"python\".\n\n    Returns:\n        str: The output of the executed code (printed content only), or an error message if the execution fails.\n    \"\"\"\n\n    if not os.path.isfile(file_path):\n        return f\"Error: File '{file_path}' does not exist.\"\n\n    try:\n        with open(file_path, 'r', encoding=DEFAULT_ENCODING) as file:\n            code = file.read()\n    except Exception as e:\n        return f\"Error reading file: {e}\"\n\n    return self.execute(code, language)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.PythonInterpreter.get_tool_schemas","title":"get_tool_schemas","text":"<pre><code>get_tool_schemas() -&gt; list[Dict[str, Any]]\n</code></pre> <p>Returns the OpenAI-compatible function schema for the Python interpreter.</p> <p>Returns:</p> Type Description <code>list[Dict[str, Any]]</code> <p>list[Dict[str, Any]]: Function schema in OpenAI format</p> Source code in <code>evoagentx/tools/interpreter_python.py</code> <pre><code>def get_tool_schemas(self) -&gt; list[Dict[str, Any]]:\n    \"\"\"\n    Returns the OpenAI-compatible function schema for the Python interpreter.\n\n    Returns:\n        list[Dict[str, Any]]: Function schema in OpenAI format\n    \"\"\"\n    return [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"execute\",\n            \"description\": \"Execute Python code in a secure environment with safety checks.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"code\": {\n                        \"type\": \"string\",\n                        \"description\": \"The Python code to execute\"\n                    },\n                    \"language\": {\n                        \"type\": \"string\",\n                        \"description\": \"The programming language of the code (only 'python' is supported)\",\n                        \"enum\": [\"python\"]\n                    }\n                },\n                \"required\": [\"code\"]\n            }\n        }\n    },\n    {\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"execute_script\",\n            \"description\": \"Execute Python code from a file in a secure environment with safety checks.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"file_path\": {\n                        \"type\": \"string\",\n                        \"description\": \"The path to the Python file to be executed\"\n                    },\n                    \"language\": {\n                        \"type\": \"string\",\n                        \"description\": \"The programming language of the code (only 'python' is supported)\",\n                        \"enum\": [\"python\"]\n                    }\n                },\n                \"required\": [\"file_path\"]\n            }\n        }\n    }]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.PythonInterpreter.get_tool_descriptions","title":"get_tool_descriptions","text":"<pre><code>get_tool_descriptions() -&gt; list[str]\n</code></pre> <p>Returns a brief description of the Python interpreter tool.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Tool descriptions</p> Source code in <code>evoagentx/tools/interpreter_python.py</code> <pre><code>def get_tool_descriptions(self) -&gt; list[str]:\n    \"\"\"\n    Returns a brief description of the Python interpreter tool.\n\n    Returns:\n        list[str]: Tool descriptions\n    \"\"\"\n    return [\n        \"Execute Python code in a secure environment with safety checks.\",\n        \"Execute Python code from a file in a secure environment with safety checks.\"\n    ]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchBase","title":"SearchBase","text":"<pre><code>SearchBase(name: str = 'SearchBase', num_search_pages: Optional[int] = 5, max_content_words: Optional[int] = None, **kwargs)\n</code></pre> <p>               Bases: <code>Tool</code></p> <p>Base class for search tools that retrieve information from various sources. Implements the standard tool interface with get_tool_schemas and execute methods.</p> <p>Initialize the base search tool.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the tool</p> <code>'SearchBase'</code> <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>5</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content, default None means no limit. </p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for parent class initialization</p> <code>{}</code> Source code in <code>evoagentx/tools/search_base.py</code> <pre><code>def __init__(\n    self, \n    name: str = \"SearchBase\",\n    num_search_pages: Optional[int] = 5, \n    max_content_words: Optional[int] = None, \n    **kwargs\n):\n    \"\"\"\n    Initialize the base search tool.\n\n    Args:\n        name (str): Name of the tool\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int): Maximum number of words to include in content, default None means no limit. \n        **kwargs: Additional keyword arguments for parent class initialization\n    \"\"\" \n    # Pass to parent class initialization\n    super().__init__(name=name, num_search_pages=num_search_pages, max_content_words=max_content_words, **kwargs)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogleFree","title":"SearchGoogleFree","text":"<pre><code>SearchGoogleFree(name: str = 'GoogleFreeSearch', num_search_pages: Optional[int] = 5, max_content_words: Optional[int] = None, **kwargs)\n</code></pre> <p>               Bases: <code>SearchBase</code></p> <p>Free Google Search tool that doesn't require API keys.</p> <p>Initialize the Free Google Search tool.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>Name of the tool</p> <code>'GoogleFreeSearch'</code> <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>5</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content</p> <code>None</code> <code>**kwargs</code> <p>Additional keyword arguments for parent class initialization</p> <code>{}</code> Source code in <code>evoagentx/tools/search_google_f.py</code> <pre><code>def __init__(\n    self, \n    name: str = \"GoogleFreeSearch\",\n    num_search_pages: Optional[int] = 5, \n    max_content_words: Optional[int] = None,\n   **kwargs \n):\n    \"\"\"\n    Initialize the Free Google Search tool.\n\n    Args:\n        name (str): Name of the tool\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int): Maximum number of words to include in content\n        **kwargs: Additional keyword arguments for parent class initialization\n    \"\"\"\n    super().__init__(name=name, num_search_pages=num_search_pages, max_content_words=max_content_words, **kwargs)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogleFree.search","title":"search","text":"<pre><code>search(query: str, num_search_pages: int = None, max_content_words: int = None) -&gt; Dict[str, Any]\n</code></pre> <p>Searches Google for the given query and retrieves content from multiple pages.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>None</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content, None means no limit</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Contains a list of search results and optional error message.</p> Source code in <code>evoagentx/tools/search_google_f.py</code> <pre><code>def search(self, query: str, num_search_pages: int = None, max_content_words: int = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Searches Google for the given query and retrieves content from multiple pages.\n\n    Args:\n        query (str): The search query.\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int): Maximum number of words to include in content, None means no limit\n\n    Returns:\n        Dict[str, Any]: Contains a list of search results and optional error message.\n    \"\"\"\n    # Use class defaults\n    num_search_pages = num_search_pages or self.num_search_pages\n    max_content_words = max_content_words or self.max_content_words \n\n    results = []\n    try:\n        # Step 1: Get top search result links\n        logger.info(f\"Searching Google (Free) for: {query}, num_results={num_search_pages}, max_content_words={max_content_words}\")\n        search_results = list(google_f_search(query, num_results=num_search_pages))\n        if not search_results:\n            return {\"results\": [], \"error\": \"No search results found.\"}\n\n        logger.info(f\"Found {len(search_results)} search results\")\n\n        # Step 2: Fetch content from each page\n        for url in search_results:\n            try:\n                title, content = self._scrape_page(url)\n                if content:  # Ensure valid content exists\n                    # Use the base class's content truncation method\n                    display_content = self._truncate_content(content, max_content_words)\n\n                    results.append({\n                        \"title\": title,\n                        \"content\": display_content,\n                        \"url\": url,\n                    })\n            except Exception as e:\n                logger.warning(f\"Error processing URL {url}: {str(e)}\")\n                continue  # Skip pages that cannot be processed\n\n        return {\"results\": results, \"error\": None}\n\n    except Exception as e:\n        logger.error(f\"Error in free Google search: {str(e)}\")\n        return {\"results\": [], \"error\": str(e)}\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogleFree.get_tool_schemas","title":"get_tool_schemas","text":"<pre><code>get_tool_schemas() -&gt; List[Dict[str, Any]]\n</code></pre> <p>Returns the OpenAI-compatible function schema for the free Google search tool.</p> <p>Returns:</p> Type Description <code>List[Dict[str, Any]]</code> <p>list[Dict[str, Any]]: Function schema in OpenAI format</p> Source code in <code>evoagentx/tools/search_google_f.py</code> <pre><code>def get_tool_schemas(self) -&gt; List[Dict[str, Any]]:\n    \"\"\"\n    Returns the OpenAI-compatible function schema for the free Google search tool.\n\n    Returns:\n        list[Dict[str, Any]]: Function schema in OpenAI format\n    \"\"\"\n    return [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search\",\n            \"description\": \"Search Google without requiring an API key and retrieve content from search results.\",\n            \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"query\": {\n                    \"type\": \"string\",\n                    \"description\": \"The search query to execute on Google\"\n                },\n                \"num_search_pages\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Number of search results to retrieve. Default: 5\"\n                },\n                \"max_content_words\": {\n                    \"type\": \"integer\",\n                    \"description\": \"Maximum number of words to include in content per result. None means no limit. Default: None\"\n                }\n            },\n            \"required\": [\"query\"]\n            }\n        }\n    }]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchWiki","title":"SearchWiki","text":"<pre><code>SearchWiki(name: str = 'SearchWiki', num_search_pages: Optional[int] = 5, max_content_words: Optional[int] = None, max_summary_sentences: Optional[int] = None, **kwargs)\n</code></pre> <p>               Bases: <code>SearchBase</code></p> <p>Initialize the Wikipedia Search tool.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the search tool</p> <code>'SearchWiki'</code> <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>5</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content, None means no limit</p> <code>None</code> <code>max_summary_sentences</code> <code>int</code> <p>Maximum number of sentences in the summary, None means no limit</p> <code>None</code> <code>**kwargs</code> <p>Additional data to pass to the parent class</p> <code>{}</code> Source code in <code>evoagentx/tools/search_wiki.py</code> <pre><code>def __init__(\n    self, \n    name: str = 'SearchWiki',\n    num_search_pages: Optional[int] = 5, \n    max_content_words: Optional[int] = None,\n    max_summary_sentences: Optional[int] = None,\n    **kwargs\n):\n    \"\"\"\n    Initialize the Wikipedia Search tool.\n\n    Args:\n        name (str): The name of the search tool\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int, optional): Maximum number of words to include in content, None means no limit\n        max_summary_sentences (int, optional): Maximum number of sentences in the summary, None means no limit\n        **kwargs: Additional data to pass to the parent class\n    \"\"\"\n\n    super().__init__(\n        name=name,\n        num_search_pages=num_search_pages,\n        max_content_words=max_content_words,\n        max_summary_sentences=max_summary_sentences,\n        **kwargs\n    )\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchWiki.search","title":"search","text":"<pre><code>search(query: str, num_search_pages: int = None, max_content_words: int = None, max_summary_sentences: int = None) -&gt; Dict[str, Any]\n</code></pre> <p>Searches Wikipedia for the given query and returns the summary and truncated full content.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query.</p> required <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>None</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content, None means no limit</p> <code>None</code> <code>max_summary_sentences</code> <code>int</code> <p>Maximum number of sentences in the summary, None means no limit</p> <code>None</code> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict[str, Any]</code> <p>A dictionary with the title, summary, truncated content, and Wikipedia page link.</p> Source code in <code>evoagentx/tools/search_wiki.py</code> <pre><code>def search(self, query: str, num_search_pages: int = None, max_content_words: int = None, max_summary_sentences: int = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Searches Wikipedia for the given query and returns the summary and truncated full content.\n\n    Args:\n        query (str): The search query.\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int): Maximum number of words to include in content, None means no limit\n        max_summary_sentences (int): Maximum number of sentences in the summary, None means no limit\n\n    Returns:\n        dict: A dictionary with the title, summary, truncated content, and Wikipedia page link.\n    \"\"\"\n    num_search_pages = num_search_pages or self.num_search_pages\n    max_content_words = max_content_words or self.max_content_words\n    max_summary_sentences = max_summary_sentences or self.max_summary_sentences\n\n    try:\n        logger.info(f\"Searching wikipedia: {query}, num_results={num_search_pages}, max_content_words={max_content_words}, max_summary_sentences={max_summary_sentences}\")\n        # Search for top matching titles\n        search_results = wikipedia.search(query, results=num_search_pages)\n        logger.info(f\"Search results: {search_results}\")\n        if not search_results:\n            return {\"results\": [], \"error\": \"No search results found.\"}\n\n        # Try fetching the best available page\n        results = []\n        for title in search_results:\n            try:\n                page = wikipedia.page(title, auto_suggest=False)\n\n                # Handle the max_summary_sentences parameter\n                if max_summary_sentences is not None and max_summary_sentences &gt; 0:\n                    summary = wikipedia.summary(title, sentences=max_summary_sentences)\n                else:\n                    # Get the full summary without limiting sentences\n                    summary = wikipedia.summary(title)\n\n                # Use the base class's content truncation method\n                display_content = self._truncate_content(page.content, max_content_words)\n\n                results.append({\n                    \"title\": page.title,\n                    \"summary\": summary,\n                    \"content\": display_content,\n                    \"url\": page.url,\n                })\n            except wikipedia.exceptions.DisambiguationError:\n                # Skip ambiguous results and try the next\n                continue\n            except wikipedia.exceptions.PageError:\n                # Skip non-existing pages and try the next\n                continue\n\n        # logger.info(f\"get results from wikipedia: {results}\")\n        return {\"results\": results, \"error\": None}\n\n    except Exception as e:\n        logger.error(f\"Error searching Wikipedia: {str(e)}\")\n        return {\"results\": [], \"error\": str(e)}\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchWiki.get_tool_schemas","title":"get_tool_schemas","text":"<pre><code>get_tool_schemas() -&gt; list[Dict[str, Any]]\n</code></pre> <p>Returns the OpenAI-compatible function schema for the Wikipedia search tool.</p> <p>Returns:</p> Type Description <code>list[Dict[str, Any]]</code> <p>list[Dict[str, Any]]: Function schema in OpenAI format</p> Source code in <code>evoagentx/tools/search_wiki.py</code> <pre><code>def get_tool_schemas(self) -&gt; list[Dict[str, Any]]:\n    \"\"\"\n    Returns the OpenAI-compatible function schema for the Wikipedia search tool.\n\n    Returns:\n        list[Dict[str, Any]]: Function schema in OpenAI format\n    \"\"\"\n    return [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search\",\n            \"description\": \"Search Wikipedia for relevant articles and content.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The search query to look up on Wikipedia\"\n                    },\n                    \"num_search_pages\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Number of search results to retrieve. Default: 5\"\n                    },\n                    \"max_content_words\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Maximum number of words to include in content per result. None means no limit. Default: None\"\n                    },\n                    \"max_summary_sentences\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Maximum number of sentences in the summary. None means no limit. Default: None\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchWiki.get_tool_descriptions","title":"get_tool_descriptions","text":"<pre><code>get_tool_descriptions() -&gt; list[str]\n</code></pre> <p>Returns a brief description of the Wikipedia search tool.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Tool descriptions</p> Source code in <code>evoagentx/tools/search_wiki.py</code> <pre><code>def get_tool_descriptions(self) -&gt; list[str]:\n    \"\"\"\n    Returns a brief description of the Wikipedia search tool.\n\n    Returns:\n        list[str]: Tool descriptions\n    \"\"\"\n    return [\n        \"Search Wikipedia for relevant articles and content.\"\n    ]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogle","title":"SearchGoogle","text":"<pre><code>SearchGoogle(name: str = 'SearchGoogle', num_search_pages: Optional[int] = 5, max_content_words: Optional[int] = None, **kwargs)\n</code></pre> <p>               Bases: <code>SearchBase</code></p> <p>Initialize the Google Search tool.</p> <p>Parameters:</p> Name Type Description Default <code>name</code> <code>str</code> <p>The name of the search tool</p> <code>'SearchGoogle'</code> <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>5</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content, None means no limit</p> <code>None</code> <code>**kwargs</code> <p>Additional data to pass to the parent class</p> <code>{}</code> Source code in <code>evoagentx/tools/search_google.py</code> <pre><code>def __init__(\n    self, \n    name: str = 'SearchGoogle',\n    num_search_pages: Optional[int] = 5, \n    max_content_words: Optional[int] = None,\n    **kwargs\n):\n    \"\"\"\n    Initialize the Google Search tool.\n\n    Args:\n        name (str): The name of the search tool\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int, optional): Maximum number of words to include in content, None means no limit\n        **kwargs: Additional data to pass to the parent class\n    \"\"\"\n    # Pass these to the parent class initialization\n    super().__init__(name=name, num_search_pages=num_search_pages, max_content_words=max_content_words, **kwargs)\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogle.search","title":"search","text":"<pre><code>search(query: str, num_search_pages: int = None, max_content_words: int = None) -&gt; Dict[str, Any]\n</code></pre> <p>Search Google using the Custom Search API and retrieve detailed search results with content snippets.</p> <p>Parameters:</p> Name Type Description Default <code>query</code> <code>str</code> <p>The search query to execute on Google</p> required <code>num_search_pages</code> <code>int</code> <p>Number of search results to retrieve</p> <code>None</code> <code>max_content_words</code> <code>int</code> <p>Maximum number of words to include in content, None means no limit</p> <code>None</code> <p>Returns:</p> Type Description <code>Dict[str, Any]</code> <p>Dict[str, Any]: Contains search results and optional error message</p> Source code in <code>evoagentx/tools/search_google.py</code> <pre><code>def search(self, query: str, num_search_pages: int = None, max_content_words: int = None) -&gt; Dict[str, Any]:\n    \"\"\"\n    Search Google using the Custom Search API and retrieve detailed search results with content snippets.\n\n    Args:\n        query (str): The search query to execute on Google\n        num_search_pages (int): Number of search results to retrieve\n        max_content_words (int): Maximum number of words to include in content, None means no limit\n\n    Returns:\n        Dict[str, Any]: Contains search results and optional error message\n    \"\"\"\n    num_search_pages = num_search_pages or self.num_search_pages\n    max_content_words = max_content_words or self.max_content_words\n    results = []\n\n    # Get API credentials from environment variables\n    api_key = os.getenv('GOOGLE_API_KEY', '')\n    search_engine_id = os.getenv('GOOGLE_SEARCH_ENGINE_ID', '')\n\n    # print(f\"api_key: {api_key}\")\n    # print(f\"search_engine_id: {search_engine_id}\")\n\n    if not api_key or not search_engine_id:\n        error_msg = (\n            \"API key and search engine ID are required. \"\n            \"Please set GOOGLE_API_KEY and GOOGLE_SEARCH_ENGINE_ID environment variables. \"\n            \"You can get these from the Google Cloud Console: https://console.cloud.google.com/apis/\"\n        )\n        logger.error(error_msg)\n        return {\"results\": [], \"error\": error_msg}\n\n    try:\n        # Step 1: Query Google Custom Search API\n        logger.info(f\"Searching Google for: {query}, num_results={num_search_pages}, max_content_words={max_content_words}\")\n        search_url = \"https://www.googleapis.com/customsearch/v1\"\n        params = {\n            \"key\": api_key,\n            \"cx\": search_engine_id,\n            \"q\": query,\n            \"num\": num_search_pages,\n        }\n        response = requests.get(search_url, params=params)\n        data = response.json()\n\n        if \"items\" not in data:\n            return {\"results\": [], \"error\": \"No search results found.\"}\n\n        search_results = data[\"items\"]\n        logger.info(f\"Found {len(search_results)} search results\")\n\n        # Step 2: Fetch content from each valid search result\n        for item in search_results:\n            url = item[\"link\"]\n            title = item[\"title\"]\n            try:\n                title, content = self._scrape_page(url)\n                if content:  # Ensure valid content exists\n                    # Use the base class's content truncation method\n                    display_content = self._truncate_content(content, max_content_words)\n\n                    results.append({\n                        \"title\": title,\n                        \"content\": display_content,\n                        \"url\": url,\n                    })\n            except Exception as e:\n                logger.warning(f\"Error processing URL {url}: {str(e)}\")\n                continue  # Skip pages that cannot be processed\n\n        return {\"results\": results, \"error\": None}\n\n    except Exception as e:\n        logger.error(f\"Error searching Google: {str(e)}\")\n        return {\"results\": [], \"error\": str(e)}\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogle.get_tool_schemas","title":"get_tool_schemas","text":"<pre><code>get_tool_schemas() -&gt; list[Dict[str, Any]]\n</code></pre> <p>Returns the OpenAI-compatible function schema for the Google search tool.</p> <p>Returns:</p> Type Description <code>list[Dict[str, Any]]</code> <p>list[Dict[str, Any]]: Function schema in OpenAI format</p> Source code in <code>evoagentx/tools/search_google.py</code> <pre><code>def get_tool_schemas(self) -&gt; list[Dict[str, Any]]:\n    \"\"\"\n    Returns the OpenAI-compatible function schema for the Google search tool.\n\n    Returns:\n        list[Dict[str, Any]]: Function schema in OpenAI format\n    \"\"\"\n    return [{\n        \"type\": \"function\",\n        \"function\": {\n            \"name\": \"search\",\n            \"description\": \"Search Google and retrieve content from search results.\",\n            \"parameters\": {\n                \"type\": \"object\",\n                \"properties\": {\n                    \"query\": {\n                        \"type\": \"string\",\n                        \"description\": \"The search query to execute on Google\"\n                    },\n                    \"num_search_pages\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Number of search results to retrieve. Default: 5\"\n                    },\n                    \"max_content_words\": {\n                        \"type\": \"integer\",\n                        \"description\": \"Maximum number of words to include in content per result. None means no limit. Default: None\"\n                    }\n                },\n                \"required\": [\"query\"]\n            }\n        }\n    }]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.SearchGoogle.get_tool_descriptions","title":"get_tool_descriptions","text":"<pre><code>get_tool_descriptions() -&gt; list[str]\n</code></pre> <p>Returns a brief description of the Google search tool.</p> <p>Returns:</p> Type Description <code>list[str]</code> <p>list[str]: Tool descriptions</p> Source code in <code>evoagentx/tools/search_google.py</code> <pre><code>def get_tool_descriptions(self) -&gt; list[str]:\n    \"\"\"\n    Returns a brief description of the Google search tool.\n\n    Returns:\n        list[str]: Tool descriptions\n    \"\"\"\n    return [\"Search Google and retrieve content from search results.\"]\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.MCPClient","title":"MCPClient","text":"<pre><code>MCPClient(server_configs: StdioServerParameters | dict[str, Any] | list[StdioServerParameters | dict[str, Any]], connect_timeout: float = 120.0)\n</code></pre> Source code in <code>evoagentx/tools/mcp.py</code> <pre><code>def __init__(\n    self, \n    server_configs: StdioServerParameters | dict[str, Any] | list[StdioServerParameters | dict[str, Any]],\n    connect_timeout: float = 120.0,\n):\n\n    if isinstance(server_configs, list):\n        self.server_configs = server_configs\n    else:\n        self.server_configs = [server_configs]\n\n    self.event_loop = asyncio.new_event_loop()\n\n    self.sessions:list[mcp.ClientSession] = []\n    self.mcp_tools:list[list[mcp.types.Tool]] = []\n\n    self.task = None\n    self.thread_running = threading.Event()\n    ## Testing\n    self.working_thread = threading.Thread(target=self._run_event, daemon=True)\n    self.connect_timeout = connect_timeout\n\n    self.tools = None\n    self.tool_schemas = None\n    self.tool_descriptions = None\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.MCPToolkit","title":"MCPToolkit","text":"<pre><code>MCPToolkit(servers: Optional[list[MCPClient]] = None, config_path: Optional[str] = None, config: Optional[dict[str, Any]] = None)\n</code></pre> Source code in <code>evoagentx/tools/mcp.py</code> <pre><code>def __init__(self, servers: Optional[list[MCPClient]] = None, config_path: Optional[str] = None, config: Optional[dict[str, Any]] = None):\n\n    parameters = []\n    if config_path:\n        parameters += self._from_config_file(config_path)\n    if config:\n        parameters += self._from_config(config)\n\n    self.servers = [MCPClient(parameters)]\n    if servers:\n        self.servers += servers\n    for server in self.servers:\n        try:\n            server._connect()\n            logger.info(\"Successfully connected to MCP servers\")\n        except TimeoutError as e:\n            logger.warning(f\"Timeout connecting to MCP servers: {str(e)}. Some tools may not be available.\")\n        except Exception as e:\n            logger.error(f\"Error connecting to MCP servers: {str(e)}\")\n</code></pre>"},{"location":"api/tools.html#evoagentx.tools.MCPToolkit.get_tools","title":"get_tools","text":"<pre><code>get_tools()\n</code></pre> <p>Return a flattened list of all tools across all servers</p> Source code in <code>evoagentx/tools/mcp.py</code> <pre><code>def get_tools(self):\n    \"\"\"Return a flattened list of all tools across all servers\"\"\"\n    all_tools = []\n    for server in self.servers:\n        try:\n            tools = server.get_tools()\n            all_tools.extend(tools)\n            logger.info(f\"Added {len(tools)} tools from MCP server\")\n        except Exception as e:\n            logger.error(f\"Error getting tools from MCP server: {str(e)}\")\n    return all_tools\n</code></pre>"},{"location":"api/workflow.html","title":"\ud83d\udd01 Workflow","text":""},{"location":"api/workflow.html#evoagentx.workflow","title":"evoagentx.workflow","text":""},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGenerator","title":"WorkFlowGenerator","text":"<pre><code>WorkFlowGenerator(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Automated workflow generation system based on high-level goals.</p> <p>The WorkFlowGenerator is responsible for creating complete workflow graphs from high-level goals or task descriptions. It breaks down the goal into subtasks, creates the necessary dependency connections between tasks, and assigns or generates appropriate agents for each task.</p> <p>Attributes:</p> Name Type Description <code>llm</code> <code>Optional[BaseLLM]</code> <p>Language model used for generation and planning</p> <code>task_planner</code> <code>Optional[TaskPlanner]</code> <p>Component responsible for breaking down goals into subtasks</p> <code>agent_generator</code> <code>Optional[AgentGenerator]</code> <p>Component responsible for agent assignment or creation</p> <code>workflow_reviewer</code> <code>Optional[WorkFlowReviewer]</code> <p>Component for reviewing and improving workflows</p> <code>num_turns</code> <code>Optional[PositiveInt]</code> <p>Number of refinement iterations for the workflow</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph","title":"WorkFlowGraph","text":"<pre><code>WorkFlowGraph(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> <p>Represents a complete workflow as a directed graph.</p> <p>WorkFlowGraph models a workflow as a directed graph where nodes represent tasks and edges represent dependencies and data flow between tasks. It provides methods for constructing, validating, traversing, and executing workflows.</p> <p>The graph structure supports advanced features like detecting and handling loops, determining execution order, and tracking execution state.</p> <p>Attributes:</p> Name Type Description <code>goal</code> <code>str</code> <p>The high-level objective of this workflow</p> <code>nodes</code> <code>Optional[List[WorkFlowNode]]</code> <p>List of WorkFlowNode instances representing tasks</p> <code>edges</code> <code>Optional[List[WorkFlowEdge]]</code> <p>List of WorkFlowEdge instances representing dependencies</p> <code>graph</code> <code>Optional[Union[MultiDiGraph, WorkFlowGraph]]</code> <p>Internal NetworkX MultiDiGraph or another WorkFlowGraph</p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.edge_exists","title":"edge_exists","text":"<pre><code>edge_exists(edge: Union[Tuple[str, str], WorkFlowEdge], **attr_filters) -&gt; bool\n</code></pre> <p>Check whether an edge exists in the workflow graph. The input <code>edge</code> can either be a tuple or a WorkFlowEdge instance.</p> <ol> <li>If a tuple is passed, it should be (source, target). The function will only determin whether there is an edge between the source node and the target node.  If attr_filters is passed, they will also be used to match the edge attributes. </li> <li>If a WorkFlowEdge is passed, it will use the eq method in WorkFlowEdge to determine </li> </ol> <pre><code>edge (Union[Tuple[str, str], WorkFlowEdge]):\n    - If a tuple is provided, it should be in the format `(source, target)`. \n    The method will check whether there is an edge between the source and target nodes.\n    If `attr_filters` are provided, they will be used to match edge attributes.\n    - If a WorkFlowEdge instance is provided, the method will use the `__eq__` method in WorkFlowEdge \n    to determine whether the edge exists.\n\nattr_filters (dict, optional):\n    Additional attributes to filter edges when `edge` is a tuple.\n</code></pre> <pre><code>bool: True if the edge exists and matches the filters (if provided); False otherwise.\n</code></pre> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def edge_exists(self, edge: Union[Tuple[str, str], WorkFlowEdge], **attr_filters) -&gt; bool:\n\n    \"\"\"\n    Check whether an edge exists in the workflow graph. The input `edge` can either be a tuple or a WorkFlowEdge instance.\n\n    1. If a tuple is passed, it should be (source, target). The function will only determin whether there is an edge between the source node and the target node. \n    If attr_filters is passed, they will also be used to match the edge attributes. \n    2. If a WorkFlowEdge is passed, it will use the __eq__ method in WorkFlowEdge to determine \n\n    Parameters:\n    ----------\n        edge (Union[Tuple[str, str], WorkFlowEdge]):\n            - If a tuple is provided, it should be in the format `(source, target)`. \n            The method will check whether there is an edge between the source and target nodes.\n            If `attr_filters` are provided, they will be used to match edge attributes.\n            - If a WorkFlowEdge instance is provided, the method will use the `__eq__` method in WorkFlowEdge \n            to determine whether the edge exists.\n\n        attr_filters (dict, optional):\n            Additional attributes to filter edges when `edge` is a tuple.\n\n    Returns:\n    -------\n        bool: True if the edge exists and matches the filters (if provided); False otherwise.\n    \"\"\"\n    if isinstance(edge, tuple):\n        assert len(edge) == 2, \"edge must be a tuple (source, target) or WorkFlowEdge instance\"\n        source, target = edge \n        return self._edge_exists(source, target, **attr_filters)\n    elif isinstance(edge, WorkFlowEdge):\n        return edge in self.edges \n    else:\n        raise TypeError(\"edge must be a tuple (source, target) or WorkFlowEdge instance\")\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.list_nodes","title":"list_nodes","text":"<pre><code>list_nodes() -&gt; List[str]\n</code></pre> <p>return the names of all nodes</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def list_nodes(self) -&gt; List[str]:\n    \"\"\"\n    return the names of all nodes \n    \"\"\"\n    return [node.name for node in self.nodes]\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.get_node","title":"get_node","text":"<pre><code>get_node(node_name: str) -&gt; WorkFlowNode\n</code></pre> <p>return a WorkFlowNode instance based on its name.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_node(self, node_name: str) -&gt; WorkFlowNode:\n    \"\"\"\n    return a WorkFlowNode instance based on its name.\n    \"\"\"\n    if not self.node_exists(node=node_name):\n        raise KeyError(f\"{node_name} is an invalid node name. Currently available node names: {self.list_nodes()}\")\n    return self.graph.nodes[node_name][\"ref\"]\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.reset_graph","title":"reset_graph","text":"<pre><code>reset_graph()\n</code></pre> <p>set the status of all nodes to pending</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def reset_graph(self):\n    \"\"\"\n    set the status of all nodes to pending\n    \"\"\"\n    for node in self.nodes:\n        node.set_status(WorkFlowNodeState.PENDING)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.set_node_status","title":"set_node_status","text":"<pre><code>set_node_status(node: Union[str, WorkFlowNode], new_state: WorkFlowNodeState) -&gt; bool\n</code></pre> <p>Update the state of a specific node. </p> <p>Parameters:</p> Name Type Description Default <code>node</code> <code>Union[str, WorkFlowNode]</code> <p>The name of a node or the node instance.</p> required <code>new_state</code> <code>WorkFlowNodeState</code> <p>The new state to set.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if the state was updated successfully, False otherwise.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def set_node_status(self, node: Union[str, WorkFlowNode], new_state: WorkFlowNodeState) -&gt; bool:\n    \"\"\"\n    Update the state of a specific node. \n\n    Args:\n        node (Union[str, WorkFlowNode]): The name of a node or the node instance.\n        new_state (WorkFlowNodeState): The new state to set.\n\n    Returns:\n        bool: True if the state was updated successfully, False otherwise.\n    \"\"\"\n    flag = False\n    try:\n        if isinstance(node, str):\n            node = self.get_node(node_name=node)\n        node.set_status(new_state)\n        flag = True\n    except Exception as e:\n        raise ValueError(f\"An error occurs when setting node status: {e}\")\n    return flag\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.filter_completed_nodes","title":"filter_completed_nodes","text":"<pre><code>filter_completed_nodes(nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]\n</code></pre> <p>remove completed nodes from <code>nodes</code></p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def filter_completed_nodes(self, nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]:\n    \"\"\"\n    remove completed nodes from `nodes`\n    \"\"\"\n    node_names = [node if isinstance(node, str) else node.name for node in nodes]\n    uncompleted_nodes = [] \n    for node_name in node_names:\n        if self.get_node(node_name).is_complete:\n            continue\n        uncompleted_nodes.append(node_name)\n    return uncompleted_nodes\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.get_candidate_children_nodes","title":"get_candidate_children_nodes","text":"<pre><code>get_candidate_children_nodes(completed_nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]\n</code></pre> <p>Return the next set of possible tasks to execute. If there are no loops in the graph, consider only the uncompleted children.  If there exists loops, also consider the previous completed tasks.</p> <p>Parameters:</p> Name Type Description Default <code>completed_nodes</code> <code>List[Union[str, WorkFlowNode]]</code> <p>A list of completed nodes.</p> required <p>Returns:</p> Type Description <code>List[str]</code> <p>List[str]: List of node names that are candidates for execution.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_candidate_children_nodes(self, completed_nodes: List[Union[str, WorkFlowNode]]) -&gt; List[str]:\n    \"\"\"\n    Return the next set of possible tasks to execute. If there are no loops in the graph, consider only the uncompleted children. \n    If there exists loops, also consider the previous completed tasks.\n\n    Args:\n        completed_nodes (List[Union[str, WorkFlowNode]]): A list of completed nodes.\n\n    Returns:\n        List[str]: List of node names that are candidates for execution.\n    \"\"\"\n    node_names = [node if isinstance(node, str) else node.name for node in completed_nodes]\n    has_loop = (len(self._loops) &gt; 0)\n    if has_loop:\n        # if there exists loops, we need to check the completed nodes and their children nodes\n        uncompleted_children_nodes = []\n        for node_name in node_names:\n            children_nodes = self.get_all_children_nodes(nodes=[node_name])\n            if self.is_loop_end(node=node_name):\n                current_uncompleted_children_nodes = [] \n                for child in children_nodes:\n                    if self.is_loop_start(node=child):\n                        # node_name\u662f\u4e00\u4e2a\u73af\u7684\u7ed3\u675f\u7684\u65f6\u5019\uff0c\u5982\u679c\u5b83\u7684\u5b50\u8282\u70b9\u662f\u73af\u7684\u5f00\u59cb\uff0c\u90a3\u4e48\u65e0\u8bba\u5b83\u662f\u5426completed\uff0c\u90fd\u6dfb\u52a0\u5230\u4e0b\u4e00\u6b65\u53ef\u6267\u884c\u7684\u64cd\u4f5c\n                        current_uncompleted_children_nodes.append(child)\n                    else:\n                        # node_name\u662f\u73af\u7684\u7ed3\u675f\uff0c\u4f46\u662f\u5b50\u8282\u70b9\u4e0d\u662f\u73af\u7684\u5f00\u59cb\u65f6\uff0c\u9700\u8981\u68c0\u67e5child\u662f\u5426\u5df2\u7ecfcompleted\uff0c\u53ea\u6dfb\u52a0\u672a\u5b8c\u6210\u7684\u4efb\u52a1\n                        current_uncompleted_children_nodes.extend(self.filter_completed_nodes(nodes=[child]))\n            else:\n                current_uncompleted_children_nodes = self.filter_completed_nodes(nodes=children_nodes)\n            for child in current_uncompleted_children_nodes:\n                if child not in uncompleted_children_nodes:\n                    uncompleted_children_nodes.append(child)\n    else:\n        # \u4e0d\u5b58\u5728\u73af\u7684\u65f6\u5019\u76f4\u63a5\u5f97\u5230\u6240\u6709\u7684\u5b50\u8282\u70b9\uff0c\u5e76\u53bb\u6389\u5176\u4e2d\u5df2\u5b8c\u6210\u7684\u90e8\u5206\n        children_nodes = self.get_all_children_nodes(nodes=node_names)\n        uncompleted_children_nodes = self.filter_completed_nodes(nodes=children_nodes)\n\n    return uncompleted_children_nodes\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.are_dependencies_complete","title":"are_dependencies_complete","text":"<pre><code>are_dependencies_complete(node_name: str) -&gt; bool\n</code></pre> <p>Check if all predecessors for a node are complete.</p> <p>Parameters:</p> Name Type Description Default <code>node_name</code> <code>str</code> <p>The name of the task/node to check.</p> required <p>Returns:</p> Name Type Description <code>bool</code> <code>bool</code> <p>True if all predecessors are complete, False otherwise.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def are_dependencies_complete(self, node_name: str) -&gt; bool:\n    \"\"\"\n    Check if all predecessors for a node are complete.\n\n    Args:\n        node_name (str): The name of the task/node to check.\n\n    Returns:\n        bool: True if all predecessors are complete, False otherwise.\n    \"\"\"\n    has_loop = (len(self._loops) &gt; 0)\n    predecessors = self.get_node_predecessors(node=node_name)\n    if has_loop and self.is_loop_start(node=node_name):\n        flag = True \n        for pre in predecessors:\n            if self.is_loop_end(pre):\n                pass \n            else:\n                flag &amp;= self.get_node(pre).is_complete\n    else:\n        flag = all(self.get_node(pre).is_complete for pre in predecessors)\n    return flag\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.display","title":"display","text":"<pre><code>display()\n</code></pre> <p>Display the workflow graph with node and edge attributes. Nodes are colored based on their status.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def display(self):\n    \"\"\"\n    Display the workflow graph with node and edge attributes.\n    Nodes are colored based on their status.\n    \"\"\"\n    import matplotlib.pyplot as plt\n\n    # Define colors for node statuses\n    status_colors = {\n        WorkFlowNodeState.PENDING: 'lightgray',\n        WorkFlowNodeState.RUNNING: 'orange',\n        WorkFlowNodeState.COMPLETED: 'green',\n        WorkFlowNodeState.FAILED: 'red'\n    }\n\n    if not self.graph.nodes:\n        print(\"Graph is empty. No nodes to display.\")\n        return\n\n    # Get node colors based on their statuses\n    node_colors = [status_colors.get(self.get_node_status(node), 'lightgray') for node in self.graph.nodes]\n\n    # Prepare node labels with additional information\n    node_labels = {node: self.get_node_description(data[\"ref\"]) for node, data in self.graph.nodes(data=True)}\n\n    # Draw the graph\n    # pos = nx.shell_layout(self.graph)\n    if len(self.graph.nodes) == 1:\n        single_node = list(self.graph.nodes)[0]\n        pos = {single_node: (0, 0)}  # Place the single node at the center\n    else:\n        pos = nx.shell_layout(self.graph)\n\n    plt.figure(figsize=(12, 8))\n    nx.draw(\n        self.graph, pos, with_labels=False, node_color=node_colors, edge_color='black',\n        node_size=1500, font_size=8, font_color='black', font_weight='bold'\n    )\n\n    if len(self.graph.nodes) == 1:\n        for node, (x, y) in pos.items():\n            plt.text(x+0.005, y, node_labels[node], ha='left', va='center', fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n    else:\n        # Draw node labels next to the nodes (left-aligned)\n        # text_offsets = {node: (pos[node][0]-0.2, pos[node][1]-0.22) for node in self.graph.nodes}\n        y_positions = [y for _, y in pos.values()]\n        y_min, y_max = min(y_positions), max(y_positions)\n        lower_third_boundary = y_min + (y_max - y_min) / 3\n\n        # Adjust text offsets based on node position in the graph\n        text_offsets = {}\n        for node, (x, y) in pos.items():\n            if y &lt; lower_third_boundary:  # If in the lower third, display label above the node\n                text_offsets[node] = (x-0.2, y + 0.23)\n            else:  # Otherwise, display label below the node\n                text_offsets[node] = (x-0.2, y - 0.23)\n\n        for node, (x, y) in text_offsets.items():\n            plt.text(x, y, node_labels[node], ha='left', va='center', fontsize=9, bbox=dict(facecolor='white', alpha=0.7))\n\n    # Draw edge labels for priorities\n    edge_labels = nx.get_edge_attributes(self.graph, 'priority')\n    nx.draw_networkx_edge_labels(self.graph, pos, edge_labels=edge_labels)\n\n    # Add a legend to show node status colors\n    legend_elements = [\n        plt.Line2D([0], [0], marker='o', color='w', label=status.name, markersize=10, markerfacecolor=color)\n        for status, color in status_colors.items()\n    ]\n    plt.legend(handles=legend_elements, title=\"Workflow Node Status\", loc='upper left', fontsize='medium')\n\n    plt.title(\"Workflow Graph\")\n    plt.show()\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlowGraph.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; dict\n</code></pre> <p>Get a dictionary containing all necessary configuration to recreate this workflow graph.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A configuration dictionary that can be used to initialize a new WorkFlowGraph instance</p> <code>dict</code> <p>with the same properties as this one.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"\n    Get a dictionary containing all necessary configuration to recreate this workflow graph.\n\n    Returns:\n        dict: A configuration dictionary that can be used to initialize a new WorkFlowGraph instance\n        with the same properties as this one.\n    \"\"\"\n    config = self.to_dict() \n    config.pop(\"graph\", None)\n    return config\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.SequentialWorkFlowGraph","title":"SequentialWorkFlowGraph","text":"<pre><code>SequentialWorkFlowGraph(goal: str, tasks: List[dict], **kwargs)\n</code></pre> <p>               Bases: <code>WorkFlowGraph</code></p> <p>A linear workflow graph with a single path from start to end.</p> <p>Parameters:</p> Name Type Description Default <code>goal</code> <code>str</code> <p>The goal of the workflow.</p> required <code>tasks</code> <code>List[dict]</code> <p>A list of tasks with their descriptions and inputs. Each task should have the following format: {     \"name\": str,     \"description\": str,     \"inputs\": [{\"name\": str, \"type\": str, \"required\": bool, \"description\": str}, ...],     \"outputs\": [{\"name\": str, \"type\": str, \"required\": bool, \"description\": str}, ...],     \"prompt\": str,      \"prompt_template\": PromptTemplate,      \"system_prompt\" (optional): str, default is DEFAULT_SYSTEM_PROMPT,     \"output_parser\" (optional): Type[ActionOutput],     \"parse_mode\" (optional): str, default is \"str\"      \"parse_func\" (optional): Callable,     \"parse_title\" (optional): str  }</p> required Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def __init__(self, goal: str, tasks: List[dict], **kwargs):\n    nodes = self._infer_nodes_from_tasks(tasks=tasks)\n    edges = self._infer_edges_from_nodes(nodes=nodes)\n    super().__init__(goal=goal, nodes=nodes, edges=edges, **kwargs)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.SequentialWorkFlowGraph.get_graph_info","title":"get_graph_info","text":"<pre><code>get_graph_info(**kwargs) -&gt; dict\n</code></pre> <p>Get the information of the workflow graph.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_graph_info(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Get the information of the workflow graph.\n    \"\"\"\n    config = {\n        \"class_name\": self.__class__.__name__,\n        \"goal\": self.goal, \n        \"tasks\": [\n            {\n                \"name\": node.name,\n                \"description\": node.description,\n                \"inputs\": [param.to_dict(ignore=[\"class_name\"]) for param in node.inputs],\n                \"outputs\": [param.to_dict(ignore=[\"class_name\"]) for param in node.outputs],\n                \"prompt\": node.agents[0].get(\"prompt\", None),\n                \"prompt_template\": node.agents[0].get(\"prompt_template\", None).to_dict() if node.agents[0].get(\"prompt_template\", None) else None,\n                \"system_prompt\": node.agents[0].get(\"system_prompt\", None),\n                \"parse_mode\": node.agents[0].get(\"parse_mode\", \"str\"), \n                \"parse_func\": node.agents[0].get(\"parse_func\", None).__name__ if node.agents[0].get(\"parse_func\", None) else None,\n                \"parse_title\": node.agents[0].get(\"parse_title\", None)\n            }\n            for node in self.nodes\n        ]\n    }\n    return config\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.SequentialWorkFlowGraph.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs)\n</code></pre> <p>Save the workflow graph to a module file.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs):\n    \"\"\"\n    Save the workflow graph to a module file.\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    config = self.get_graph_info()\n    for ignore_key in ignore:\n        config.pop(ignore_key, None)\n    make_parent_folder(path)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=4)\n    return path\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.SequentialWorkFlowGraph.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; Dict\n</code></pre> <p>Get a dictionary containing all necessary configuration to recreate this workflow graph.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>Dict</code> <p>A configuration dictionary that can be used to initialize a new SequentialWorkFlowGraph instance</p> <code>Dict</code> <p>with the same properties as this one.</p> Source code in <code>evoagentx/workflow/workflow_graph.py</code> <pre><code>def get_config(self) -&gt; Dict:\n    \"\"\"\n    Get a dictionary containing all necessary configuration to recreate this workflow graph.\n\n    Returns:\n        dict: A configuration dictionary that can be used to initialize a new SequentialWorkFlowGraph instance\n        with the same properties as this one.\n    \"\"\"\n    return self.get_graph_info()\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlow","title":"WorkFlow","text":"<pre><code>WorkFlow(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlow.execute","title":"execute","text":"<pre><code>execute(inputs: dict = {}, **kwargs) -&gt; str\n</code></pre> <p>Synchronous wrapper for async_execute. Creates a new event loop and runs the async method.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>Dictionary of inputs for workflow execution</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The output of the workflow execution</p> Source code in <code>evoagentx/workflow/workflow.py</code> <pre><code>def execute(self, inputs: dict = {}, **kwargs) -&gt; str:\n    \"\"\"\n    Synchronous wrapper for async_execute. Creates a new event loop and runs the async method.\n\n    Args:\n        inputs: Dictionary of inputs for workflow execution\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The output of the workflow execution\n    \"\"\"\n    loop = asyncio.new_event_loop()\n    asyncio.set_event_loop(loop)\n    try:\n        return loop.run_until_complete(self.async_execute(inputs, **kwargs))\n    finally:\n        loop.close()\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlow.async_execute","title":"async_execute  <code>async</code>","text":"<pre><code>async_execute(inputs: dict = {}, **kwargs) -&gt; str\n</code></pre> <p>Asynchronously execute the workflow.</p> <p>Parameters:</p> Name Type Description Default <code>inputs</code> <code>dict</code> <p>Dictionary of inputs for workflow execution</p> <code>{}</code> <code>**kwargs</code> <code>Any</code> <p>Additional keyword arguments</p> <code>{}</code> <p>Returns:</p> Name Type Description <code>str</code> <code>str</code> <p>The output of the workflow execution</p> Source code in <code>evoagentx/workflow/workflow.py</code> <pre><code>async def async_execute(self, inputs: dict = {}, **kwargs) -&gt; str:\n    \"\"\"\n    Asynchronously execute the workflow.\n\n    Args:\n        inputs: Dictionary of inputs for workflow execution\n        **kwargs (Any): Additional keyword arguments\n\n    Returns:\n        str: The output of the workflow execution\n    \"\"\"\n    goal = self.graph.goal\n    # inputs.update({\"goal\": goal})\n    inputs = self._prepare_inputs(inputs)\n\n    # prepare for hitl functionalities\n    if hasattr(self, \"hitl_manager\") and (self.hitl_manager is not None):\n        self._prepare_hitl()\n\n    # check the inputs and outputs of the task \n    self._validate_workflow_structure(inputs=inputs, **kwargs)\n    inp_message = Message(content=inputs, msg_type=MessageType.INPUT, wf_goal=goal)\n    self.environment.update(message=inp_message, state=TrajectoryState.COMPLETED)\n\n    failed = False\n    error_message = None\n    while not self.graph.is_complete and not failed:\n        try:\n            task: WorkFlowNode = await self.get_next_task()\n            if task is None:\n                break\n            logger.info(f\"Executing subtask: {task.name}\")\n            await self.execute_task(task=task)\n        except Exception as e:\n            failed = True\n            error_message = Message(\n                content=f\"An Error occurs when executing the workflow: {e}\",\n                msg_type=MessageType.ERROR, \n                wf_goal=goal\n            )\n            self.environment.update(message=error_message, state=TrajectoryState.FAILED, error=str(e))\n\n    if failed:\n        logger.error(error_message.content)\n        return \"Workflow Execution Failed\"\n\n    logger.info(\"Extracting WorkFlow Output ...\")\n    output: str = await self.workflow_manager.extract_output(graph=self.graph, env=self.environment)\n    return output\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.WorkFlow.execute_task","title":"execute_task  <code>async</code>","text":"<pre><code>execute_task(task: WorkFlowNode)\n</code></pre> <p>Asynchronously execute a workflow task.</p> <p>Parameters:</p> Name Type Description Default <code>task</code> <code>WorkFlowNode</code> <p>The workflow node to execute</p> required Source code in <code>evoagentx/workflow/workflow.py</code> <pre><code>async def execute_task(self, task: WorkFlowNode):\n    \"\"\"\n    Asynchronously execute a workflow task.\n\n    Args:\n        task: The workflow node to execute\n    \"\"\"\n    last_executed_task = self.environment.get_last_executed_task()\n    self.graph.step(source_node=last_executed_task, target_node=task)\n    next_action: NextAction = await self.workflow_manager.schedule_next_action(\n        goal=self.graph.goal,\n        task=task, \n        agent_manager=self.agent_manager, \n        env=self.environment\n    )\n    if next_action.action_graph is not None:\n        await self._async_execute_task_by_action_graph(task=task, next_action=next_action)\n    else:\n        await self._async_execute_task_by_agents(task=task, next_action=next_action)\n    self.graph.completed(node=task)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.ActionGraph","title":"ActionGraph","text":"<pre><code>ActionGraph(**kwargs)\n</code></pre> <p>               Bases: <code>BaseModule</code></p> Source code in <code>evoagentx/core/module.py</code> <pre><code>def __init__(self, **kwargs):\n    \"\"\"\n    Initializes a BaseModule instance.\n\n    Args:\n        **kwargs (Any): Keyword arguments used to initialize the instance\n\n    Raises:\n        ValidationError: When parameter validation fails\n        Exception: When other errors occur during initialization\n    \"\"\"\n\n    try:\n        for field_name, _ in type(self).model_fields.items():\n            field_value = kwargs.get(field_name, None)\n            if field_value:\n                kwargs[field_name] = self._process_data(field_value)\n            # if field_value and isinstance(field_value, dict) and \"class_name\" in field_value:\n            #     class_name = field_value.get(\"class_name\")\n            #     sub_cls = MODULE_REGISTRY.get_module(cls_name=class_name)\n            #     kwargs[field_name] = sub_cls._create_instance(field_value)\n        super().__init__(**kwargs) \n        self.init_module()\n    except (ValidationError, Exception) as e:\n        exception_handler = callback_manager.get_callback(\"exception_buffer\")\n        if exception_handler is None:\n            error_message = get_base_module_init_error_message(\n                cls=self.__class__, \n                data=kwargs, \n                errors=e\n            )\n            logger.error(error_message)\n            raise\n        else:\n            exception_handler.add(e)\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.ActionGraph.get_graph_info","title":"get_graph_info","text":"<pre><code>get_graph_info(**kwargs) -&gt; dict\n</code></pre> <p>Get the information of the action graph, including all operators from the instance.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>def get_graph_info(self, **kwargs) -&gt; dict:\n    \"\"\"\n    Get the information of the action graph, including all operators from the instance.\n    \"\"\"\n    operators = {}\n    # the extra fields are the fields that are not defined in the Pydantic model \n    for extra_name, extra_value in self.__pydantic_extra__.items():\n        if isinstance(extra_value, Operator):\n            operators[extra_name] = extra_value\n\n    config = {\n        \"class_name\": self.__class__.__name__,\n        \"name\": self.name,\n        \"description\": self.description, \n        \"operators\": {\n            operator_name: {\n                \"class_name\": operator.__class__.__name__,\n                \"name\": operator.name,\n                \"description\": operator.description,\n                \"interface\": operator.interface,\n                \"prompt\": operator.prompt\n            }\n            for operator_name, operator in operators.items()\n        }\n    }\n    return config\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.ActionGraph.load_module","title":"load_module  <code>classmethod</code>","text":"<pre><code>load_module(path: str, llm_config: LLMConfig = None, **kwargs) -&gt; Dict\n</code></pre> <p>Load the ActionGraph from a file.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>@classmethod\ndef load_module(cls, path: str, llm_config: LLMConfig = None, **kwargs) -&gt; Dict:\n    \"\"\"\n    Load the ActionGraph from a file.\n    \"\"\"\n    assert llm_config is not None, \"must provide `llm_config` when using `load_module` or `from_file` to load the ActionGraph from local storage\" \n    action_graph_data = super().load_module(path, **kwargs) \n    action_graph_data[\"llm_config\"] = llm_config.to_dict()\n    return action_graph_data\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.ActionGraph.from_dict","title":"from_dict  <code>classmethod</code>","text":"<pre><code>from_dict(data: Dict[str, Any], **kwargs) -&gt; ActionGraph\n</code></pre> <p>Create an ActionGraph from a dictionary.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>@classmethod\ndef from_dict(cls, data: Dict[str, Any], **kwargs) -&gt; \"ActionGraph\":\n    \"\"\"\n    Create an ActionGraph from a dictionary.\n    \"\"\"\n    class_name = data.get(\"class_name\", None)\n    if class_name:\n        cls = MODULE_REGISTRY.get_module(class_name)\n    operators_info = data.pop(\"operators\", None)\n    module = cls._create_instance(data)\n    if operators_info:\n        for extra_name, extra_value in module.__pydantic_extra__.items():\n            if isinstance(extra_value, Operator) and extra_name in operators_info:\n                extra_value.set_operator(operators_info[extra_name])\n    return module\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.ActionGraph.save_module","title":"save_module","text":"<pre><code>save_module(path: str, ignore: List[str] = [], **kwargs)\n</code></pre> <p>Save the workflow graph to a module file.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>def save_module(self, path: str, ignore: List[str] = [], **kwargs):\n    \"\"\"\n    Save the workflow graph to a module file.\n    \"\"\"\n    logger.info(\"Saving {} to {}\", self.__class__.__name__, path)\n    config = self.get_graph_info()\n    for ignore_key in ignore:\n        config.pop(ignore_key, None)\n    with open(path, \"w\", encoding=\"utf-8\") as f:\n        json.dump(config, f, indent=4)\n\n    return path\n</code></pre>"},{"location":"api/workflow.html#evoagentx.workflow.ActionGraph.get_config","title":"get_config","text":"<pre><code>get_config() -&gt; dict\n</code></pre> <p>Get a dictionary containing all necessary configuration to recreate this action graph.</p> <p>Returns:</p> Name Type Description <code>dict</code> <code>dict</code> <p>A configuration dictionary that can be used to initialize a new ActionGraph instance</p> <code>dict</code> <p>with the same properties as this one.</p> Source code in <code>evoagentx/workflow/action_graph.py</code> <pre><code>def get_config(self) -&gt; dict:\n    \"\"\"\n    Get a dictionary containing all necessary configuration to recreate this action graph.\n\n    Returns:\n        dict: A configuration dictionary that can be used to initialize a new ActionGraph instance\n        with the same properties as this one.\n    \"\"\"\n    config = self.get_graph_info()\n    config[\"llm_config\"] = self.llm_config.to_dict()\n    return config\n</code></pre>"},{"location":"modules/action_graph.html","title":"Action Graph","text":""},{"location":"modules/action_graph.html#introduction","title":"Introduction","text":"<p>The <code>ActionGraph</code> class is a fundamental component in the EvoAgentX framework for creating and executing sequences of operations (actions) within a single task. It provides a structured way to define, manage, and execute a series of operations that need to be performed in a specific order to complete a task.</p> <p>An action graph represents a collection of operators (actions) that are executed in a predefined sequence to process inputs and produce outputs. Unlike the <code>WorkFlowGraph</code> which manages multiple tasks and their dependencies at a higher level, the <code>ActionGraph</code> focuses on the detailed execution steps within a single task.</p>"},{"location":"modules/action_graph.html#architecture","title":"Architecture","text":""},{"location":"modules/action_graph.html#actiongraph-architecture","title":"ActionGraph Architecture","text":"<p>An <code>ActionGraph</code> consists of several key components:</p> <ol> <li> <p>Operators: </p> <p>Each operator represents a specific operation or action that can be performed as part of a task, with the following properties:</p> <ul> <li><code>name</code>: A unique identifier for the operator</li> <li><code>description</code>: Detailed description of what the operator does</li> <li><code>llm</code>: The LLM used to execute the operator</li> <li><code>outputs_format</code>: The structured format of the output of the operator</li> <li><code>interface</code>: The interface for calling the operator.</li> <li><code>prompt</code>: Template used to guide the LLM when executing this operator</li> </ul> </li> <li> <p>LLM: </p> <p>The ActionGraph uses a Language Learning Model (LLM) to execute the operators. It receives a <code>llm_config</code> as input and create an LLM instance, which will be passed to the operators for execution. The LLM provides the reasoning and generation capabilities needed to perform each action.</p> </li> <li> <p>Execution Flow:</p> <p>The ActionGraph defines a specific execution sequence:</p> <ul> <li>Actions are executed in a predetermined order (specified in the <code>execute</code> or <code>async_execute</code> method using code)</li> <li>Each action can use the results from previous actions</li> <li>The final output is produced after all actions have been executed</li> </ul> </li> </ol>"},{"location":"modules/action_graph.html#comparison-with-workflowgraph","title":"Comparison with WorkFlowGraph","text":"<p>While both <code>ActionGraph</code> and <code>WorkFlowGraph</code> manage execution flows, they operate at different levels of abstraction:</p> Feature ActionGraph WorkFlowGraph Scope Single task execution Multi-task workflow orchestration Components Operators (actions) Nodes (tasks) and edges (dependencies) Focus Detailed steps within a task Relationships between different tasks Flexibility Fixed execution sequence Dynamic execution based on dependencies Primary use Define reusable task execution patterns Orchestrate complex multi-step workflows Granularity Fine-grained operations Coarse-grained tasks"},{"location":"modules/action_graph.html#usage","title":"Usage","text":""},{"location":"modules/action_graph.html#basic-actiongraph-creation","title":"Basic ActionGraph Creation","text":"<pre><code>from evoagentx.workflow import ActionGraph\nfrom evoagentx.workflow.operators import Custom\nfrom evoagentx.models import OpenAILLMConfig \n\n# Create LLM configuration\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\n\n# Create a custom ActionGraph\nclass MyActionGraph(ActionGraph):\n    def __init__(self, llm_config, **kwargs):\n\n        name = kwargs.pop(\"name\") if \"name\" in kwargs else \"Custom Action Graph\"\n        description = kwargs.pop(\"description\") if \"description\" in kwargs else \"A custom action graph for text processing\"\n        # create an LLM instance `self._llm` based on the `llm_config` and pass it to the operators\n        super().__init__(name=name, description=description, llm_config=llm_config, **kwargs)\n        # Define operators\n        self.extract_entities = Custom(self._llm) # , prompt=\"Extract key entities from the following text: {input}\")\n        self.analyze_sentiment = Custom(self._llm) # , prompt=\"Analyze the sentiment of the following text: {input}\")\n        self.summarize = Custom(self._llm) # , prompt=\"Summarize the following text in one paragraph: {input}\")\n\n    def execute(self, text: str) -&gt; dict:\n        # Execute operators in sequence (specify the execution order of operators)\n        entities = self.extract_entities(input=text, instruction=\"Extract key entities from the provided text\")[\"response\"]\n        sentiment = self.analyze_sentiment(input=text, instruction=\"Analyze the sentiment of the provided text\")[\"response\"]\n        summary = self.summarize(input=text, instruction=\"Summarize the provided text in one paragraph\")[\"response\"]\n\n        # Return combined results\n        return {\n            \"entities\": entities,\n            \"sentiment\": sentiment,\n            \"summary\": summary\n        }\n\n# Create the action graph\naction_graph = MyActionGraph(llm_config=llm_config)\n\n# Execute the action graph\nresult = action_graph.execute(text=\"This is a test text\")\nprint(result)\n</code></pre>"},{"location":"modules/action_graph.html#using-actiongraph-in-workflowgraph","title":"Using ActionGraph in WorkFlowGraph","text":"<p>You can either use <code>ActionGraph</code> directly or use it in <code>WorkFlowGraph</code> as a node. </p> <pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph\nfrom evoagentx.workflow.action_graph import QAActionGraph\nfrom evoagentx.core.base_config import Parameter\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import WorkFlow\n\n# Create LLM configuration\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# Create an action graph\nqa_graph = QAActionGraph(llm_config=llm_config)\n\n# Create a workflow node that uses the action graph\nqa_node = WorkFlowNode(\n    name=\"QATask\",\n    description=\"Answer questions using a QA system\",\n    # input names should match the parameters in the `execute` method of the action graph\n    inputs=[Parameter(name=\"problem\", type=\"string\", description=\"The problem to answer\")],\n    outputs=[Parameter(name=\"answer\", type=\"string\", description=\"The answer to the problem\")],\n    action_graph=qa_graph  # Using action_graph instead of agents\n)\n\n# Create the workflow graph\nworkflow_graph = WorkFlowGraph(goal=\"Answer a question\", nodes=[qa_node])\n\n# define the workflow \nworkflow = WorkFlow(graph=workflow_graph, llm=llm)\n\n# Execute the workflow\nresult = workflow.execute(inputs={\"problem\": \"What is the capital of France?\"})\nprint(result)\n</code></pre> <p>Warning</p> <p>When using <code>ActionGraph</code> in <code>WorkFlowNode</code>, the <code>inputs</code> parameter of the <code>WorkFlowNode</code> should match the required parameters in the <code>execute</code> method of the <code>ActionGraph</code>. The <code>execute</code> method is expected to return a dictionary or <code>LLMOutputParser</code> instance with keys matching the names of the <code>outputs</code> in the <code>WorkFlowNode</code>. </p>"},{"location":"modules/action_graph.html#saving-and-loading-an-actiongraph","title":"Saving and Loading an ActionGraph","text":"<pre><code># Save action graph\naction_graph.save_module(\"examples/output/my_action_graph.json\")\n\n# Load action graph\nfrom evoagentx.workflow.action_graph import ActionGraph\nloaded_graph = ActionGraph.from_file(\"examples/output/my_action_graph.json\", llm_config=llm_config)\n</code></pre> <p>The <code>ActionGraph</code> class provides a powerful way to define complex sequences of operations within a single task, complementing the higher-level orchestration capabilities of the <code>WorkFlowGraph</code> in the EvoAgentX framework.</p>"},{"location":"modules/agent.html","title":"Agent","text":""},{"location":"modules/agent.html#introduction","title":"Introduction","text":"<p>The <code>Agent</code> class is the fundamental building block for creating intelligent AI agents within the EvoAgentX framework. It provides a structured way to combine language models with actions, and memory management. </p>"},{"location":"modules/agent.html#architecture","title":"Architecture","text":"<p>An Agent consists of several key components:</p> <ol> <li> <p>Large Language Model (LLM): </p> <p>The LLM is specified through the <code>llm</code> or <code>llm_config</code> parameter and serve as the building block for the agent. It is responsible for interpreting context, generating responses, and making high-level decisions. The LLM will be passed to an action for executing a specific task. </p> </li> <li> <p>Actions: </p> <p>Actions are the fundamental operational units of an agent. Each Action encapsulates a specific task and is the actual point where the LLM is invoked to reason, generate, or make decisions. While the Agent provides overall orchestration, it is through Actions that the LLM performs its core functions. Each Action is designed to do exactly one thing\u2014such as retrieving knowledge, summarizing input, or calling an API\u2014and can include the following components:</p> <ul> <li>prompt: The prompt template used to guide the LLM's behavior for this specific task.</li> <li>inputs_format: The expected structure and keys of the inputs passed into the action.</li> <li>outputs_format: The format used to interpret and parse the LLM's output.</li> <li>tools: Optional tools that can be integrated and utilized within the action.</li> </ul> </li> <li> <p>Memory Components:</p> <p>Memory allows the agent to retain and recall relevant information across interactions, enhancing contextual awareness. There are two types of memory within the EvoAgentX framework: </p> <ul> <li>Short-term memory: Maintains the intermediate conversation or context for the current task. </li> <li>Long-term memory (optional): Stores persistent knowledge that can span across sessions or tasks. This enables the agent to learn from past experiences, maintain user preferences, or build knowledge bases over time.</li> </ul> </li> </ol>"},{"location":"modules/agent.html#usage","title":"Usage","text":""},{"location":"modules/agent.html#basic-agent-creation","title":"Basic Agent Creation","text":"<p>In order to create an agent, you need to define the actions that the agent will perform. Each action is defined as a class that inherits from the <code>Action</code> class. The action class should define the following components: <code>name</code>, <code>description</code>, <code>prompt</code>, <code>inputs_format</code>, and <code>outputs_format</code>, and implement the <code>execute</code> method (and <code>async_exectue</code> if you want to use the agent asynchronously). </p> <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# Define a simple action that uses the LLM to answer a question\n\nclass AnswerQuestionInput(ActionInput):\n    question: str\n\nclass AnswerQuestionOutput(ActionOutput):\n    answer: str\n\nclass AnswerQuestionAction(Action):\n\n    def __init__(\n        self, \n        name = \"answer_question\",\n        description = \"Answers a factual question using the LLM\",   \n        prompt = \"Answer the following question as accurately as possible:\\n\\n{question}\",\n        inputs_format = AnswerQuestionInput,\n        outputs_format = AnswerQuestionOutput,\n        **kwargs\n    ):\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )\n\n        if return_prompt:\n            return response, prompt\n        return response \n\n    async def async_execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = await llm.async_generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )   \n        if return_prompt:\n            return response, prompt\n        return response \n\n# Configure LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your-api-key\")\n\n# Create an agent\nagent = Agent(\n    name=\"AssistantAgent\",\n    description=\"Answers a factual question using the LLM\",\n    llm_config=llm_config,\n    system_prompt=\"You are a helpful assistant.\",\n    actions=[AnswerQuestionAction()]\n)\n</code></pre>"},{"location":"modules/agent.html#executing-actions","title":"Executing Actions","text":"<p>You can directly call the <code>Agent</code> instance like a function. This will internally invoke the <code>execute()</code> method of the matching action using the specified <code>action_name</code> and <code>action_input_data</code>.</p> <pre><code># Execute an action with input data\nmessage = agent(\n    action_name=\"answer_question\",\n    action_input_data={\"question\": \"What is the capital of France?\"}\n)\n\n# Access the output\nresult = message.content.answer \n</code></pre>"},{"location":"modules/agent.html#asynchronous-execution","title":"Asynchronous Execution","text":"<p>You can also call the <code>Agent</code> instance in an asynchronous context. If the action defines an <code>async_execute</code> method, it will be used automatically when you <code>await</code> the agent.</p> <pre><code># Execute an action asynchronously\nimport asyncio \n\nasync def main():\n    message = await agent(\n        action_name=\"answer_question\",\n        action_input_data={\"question\": \"What is the capital of France?\"}\n    )\n    return message.content.answer \n\nresult = asyncio.run(main())\nprint(result)\n</code></pre>"},{"location":"modules/agent.html#memory-management","title":"Memory Management","text":"<p>The Agent maintains a short-term memory for tracking conversation context:</p> <pre><code># Access the agent's memory\nmessages = agent.short_term_memory.get(n=5)  # Get last 5 messages\n\n# Clear memory\nagent.clear_short_term_memory()\n</code></pre>"},{"location":"modules/agent.html#agent-profile","title":"Agent Profile","text":"<p>You can get a human-readable description of an agent and its capabilities:</p> <pre><code># Get description of all actions\nprofile = agent.get_agent_profile()\nprint(profile)\n\n# Get description of specific actions\nprofile = agent.get_agent_profile(action_names=[\"answer_question\"])\nprint(profile)\n</code></pre>"},{"location":"modules/agent.html#prompt-management","title":"Prompt Management","text":"<p>Access and modify the prompts used by an agent:</p> <pre><code># Get all prompts\nprompts = agent.get_prompts()\n# prompts is a dictionary with the structure:\n# {'answer_question': {'system_prompt': 'You are a helpful assistant.', 'prompt': 'Answer the following question as accurately as possible:\\n\\n{question}'}}\n\n# Set a specific prompt\nagent.set_prompt(\n    action_name=\"answer_question\",\n    prompt=\"Please provide a clear and concise answer to the following query:\\n\\n{question}\",\n    system_prompt=\"You are a helpful assistant.\" # optional, if not provided, the system prompt will remain unchanged \n)\n\n# Update all prompts\nprompts_dict = {\n    \"answer_question\": {\n        \"system_prompt\": \"You are an expert in providing concise, accurate information.\",\n        \"prompt\": \"Please answer this question with precision and clarity:\\n\\n{question}\"\n    }\n}\nagent.set_prompts(prompts_dict)\n</code></pre>"},{"location":"modules/agent.html#saving-and-loading-agents","title":"Saving and Loading Agents","text":"<p>Agents can be persisted and reloaded:</p> <pre><code># Save agent\nagent.save_module(\"./agents/my_agent.json\")\n\n# Load agent (requires providing llm_config again)\nloaded_agent = Agent.from_file(\n    \"./agents/my_agent.json\", \n    llm_config=llm_config\n)\n</code></pre>"},{"location":"modules/agent.html#context-extraction","title":"Context Extraction","text":"<p>The Agent includes a built-in context extraction mechanism that automatically derives appropriate inputs for actions from conversation history:</p> <pre><code># Context is automatically extracted when executing without explicit input data\nresponse = agent.execute(\n    action_name=\"action_name\",\n    msgs=conversation_history\n)\n\n# Get action inputs manually\naction = agent.get_action(\"action_name\")\ninputs = agent.get_action_inputs(action)\n</code></pre>"},{"location":"modules/benchmark.html","title":"Benchmark","text":""},{"location":"modules/benchmark.html#benchmark-overview","title":"Benchmark Overview","text":"<p>EvoAgentX provides a set of benchmarks to facilitate the evaluation of different agent-based systems. Below is a summary of the benchmarks currently included, along with basic dataset statistics: </p> Task Dataset Name # Train # Dev # Test QA NQ 79,168 8,757 3,610 Multi-Hop QA HotPotQA 90,447 7,405 / Math GSM8K 7,473 / 1,319 Math MATH 7,500 / 5,000 Code Generation HumanEval / / 164 Code Generation MBPP / / 427 Code Generation LiveCodeBench(v1~v5) / / 400~880 Code Execution LiveCodeBench / / 479 Test Output Prediction LiveCodeBench / / 442 <p>Our framework provides automatic dataset downloading capabilities, and all benchmarks have built-in evaluation methods. The framework is designed to allow users to easily load, use, and evaluate datasets for various tasks without manually handling data downloading and evaluation logic.  All datasets are automatically downloaded to the default path (~/.evoagentx/data/) when first used, or users can specify a custom path via parameter <code>path</code>. Each benchmark class implements a standardized interface, including methods for data loading, label retrieval, and prediction evaluation. </p> <p>Below, we introduce the preprocessing steps and evaluation metrics for each benchmark. </p> <ul> <li>Question Answering<ul> <li>NQ</li> <li>HotPotQA</li> </ul> </li> <li>Math<ul> <li>GSM8K</li> <li>MATH</li> </ul> </li> <li>Code Generation<ul> <li>HumanEval</li> <li>MBPP</li> <li>LiveCodeBench</li> </ul> </li> </ul>"},{"location":"modules/benchmark.html#preprocessing-and-evaluation-metrics","title":"Preprocessing and Evaluation Metrics","text":""},{"location":"modules/benchmark.html#question-answering","title":"Question Answering","text":"<p>For the QA datasets, we use Exact Match (EM), F1, and Accuracy (ACC) as evaluation metrics by default. EM requires the predicted answer to be exactly the same as the ground truth answer. ACC requires that the predicted answer contains the ground-truth answer, which is useful when the LLM is used to generate the answer. </p>"},{"location":"modules/benchmark.html#nq","title":"NQ","text":"<p>Natural Questions (NQ) contains questions from the Google search engine and the answers, annotated by human annotators, are paragraphs or entities in the Wikipedia page of the top 5 search results. We use the dataset splits provided by the DPR repository, which contains 79,168 training, 8,757 development, and 3,610 test examples. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import NQ\nnq_dataset = NQ() # optional: path=\"/path/to/save_data\"\ntest_data = nq_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format:  <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"the question\", \n    \"answers\": [\"possible answers\"]\n}\n</code></pre></p>"},{"location":"modules/benchmark.html#hotpotqa","title":"HotPotQA","text":"<p>HotPotQA is a multi-hop QA dataset that requires multi-step reasoning to answer the question. We use the distractor setting of the dataset. Each example contains a question, an answer, some context that contians both supporting and distractor information, and supporting facts. We only include the training and development sets, as the test set is not publicly available. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import HotPotQA\nhotpotqa_dataset = HotPotQA() # optional: path=\"/path/to/save_data\"\ntest_data = hotpotqa_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format, where the second element (int) of a supporting_fact is the index of the sentence in the context that supports the answer.  <pre><code>{\n        \"_id\": \"the id of the example\", \n        \"question\": \"the question\", \n        \"answer\": \"the answer\", \n        \"context\": [[\"context_title\", [\"context_sentence\", \"another_sentence\"]]],\n        \"supporting_facts\": [[\"supporting_title\", 0]]\n    }\n</code></pre></p>"},{"location":"modules/benchmark.html#math","title":"Math","text":"<p>For match datasets, we use the solve rate as the evaluation metric. The solve rate is the ratio of the number of examples that are solved correctly to the total number of examples.</p>"},{"location":"modules/benchmark.html#gsm8k","title":"GSM8K","text":"<p>GSM8K consists of high quality grade school math problems created by human problem writers. These problems require multi-step mathematical reasoning to solve. We use the dataset splits provided by the original repository, which contains 7.5K training problems and 1K test problems. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import GSM8K\ngsm8k_dataset = GSM8K() # optional: path=\"/path/to/save_data\"\ntest_data = gsm8k_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format:  <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"the question\", \n    \"answer\": \"the answer\"\n}\n</code></pre></p>"},{"location":"modules/benchmark.html#math_1","title":"MATH","text":"<p>The Mathematics Aptitude Test of Heuristics (MATH) dataset consists of problems from mathematics competitions, including the AMC 10, AMC 12, AIME, etc. Each problem in MATH has a step-by-step solution. We use the dataset splits provided by the original repository, which contains 7.5K training problems and 5K test problems. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import MATH\nmath_dataset = MATH() # optional: path=\"/path/to/save_data\"\ntest_data = math_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format. For the <code>level</code> field, valid values are: \"Level 1\", \"Level 2\", \"Level 3\", \"Level 4\", \"Level 5\", and \"Level ?\". The <code>type</code> field can be one of: \"Geometry\", \"Algebra\", \"Intermediate Algebra\", \"Counting &amp; Probability\", \"Precalculus\", \"Number Theory\", or \"Prealgebra\".</p> <pre><code>{\n    \"id\": \"test-1\", \n    \"problem\": \"the problem\", \n    \"solution\": \"the solution\",\n    \"level\": \"Level 1\",\n    \"type\": \"Algebra\"\n}\n</code></pre>"},{"location":"modules/benchmark.html#code-generation","title":"Code Generation","text":"<p>For the code generation benchmarks, we use pass@k as the evaluation metric, where k is the number of solutions for each problem. By default, k is set to 1. </p>"},{"location":"modules/benchmark.html#humaneval","title":"HumanEval","text":"<p>HumanEval is a dataset of 164 coding problems from the HumanEval benchmark. Each problem contains a function signature, a canonical solution, and a set of unit tests. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import HumanEval\nhumaneval_dataset = HumanEval() # optional: path=\"/path/to/save_data\"\ntest_data = humaneval_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format:  <pre><code>{\n    \"task_id\": \"HumanEval/0\", \n    \"prompt\": \"the prompt of the problem\", \n    \"entry_point\": \"the name of the function to be tested\",\n    \"canonical_solution\": \"the canonical solution of the problem\", \n    \"test\": \"the unit tests of the problem\"\n}\n</code></pre></p>"},{"location":"modules/benchmark.html#mbpp","title":"MBPP","text":"<p>Mostly Basic Python Problems (MBPP) consists of hundreds of entry-level Python programming problems. Each problem consists of a task description, code solution and 3 automated test cases. We use the sanitized subset of the MBPP dataset, which consists of 427 problems with data that are hand-verfied by the authors. To facilitate the evaluation, we convert the MBPP dataset into the HumanEval format. </p> <p>You can load the dataset using the following code:  <pre><code>from evoagentx.benchmark import MBPP\nmbpp_dataset = MBPP() # optional: path=\"/path/to/save_data\"\ntest_data = mbpp_dataset.get_test_data()\n</code></pre> Each example in the dataset is in the following format, where we keep the original MBPP <code>task_id</code>. <pre><code>{\n    \"task_id\": 2, \n    \"prompt\": \"the prompt of the problem\", \n    \"entry_point\": \"the name of the function to be tested\",\n    \"canonical_solution\": \"the canonical solution of the problem\", \n    \"test\": \"the unit tests of the problem\"\n}\n</code></pre> You can also access the original MBPP attributes such as \"code\", \"test_list\" in the example by using <code>example[\"code\"]</code>. </p>"},{"location":"modules/benchmark.html#livecodebench","title":"LiveCodeBench","text":"<p>LiveCodeBench is a contamination-free evaluation benchmark of LLMs for code that continuously collects new problems over time. Particularly, LiveCodeBench also focuses on broader code-related capabilities, such as code execution, and test output prediction, beyond mere code generation. Currently, LiveCodeBench hosts over three hundred high-quality coding problems published between May 2023 and February 2024. </p> <p>You can load the dataset using the following code, where <code>scenario</code> can be one of [<code>code_generation</code>, <code>test_output_prediction</code>, <code>code_execution</code>] indicating different tasks. <code>version</code> denotes different versions of the code generation datasets, which is only available for <code>code_generation</code> scenario, and can be one of <code>[\"release_v1\", \"release_v2\", \"release_v3\", \"release_v4\", \"release_v5\", \"release_latest\"]</code>. Please refer to the LiveCodeBench repository for more details. </p> <pre><code>from evoagentx.benchmark import LiveCodeBench\nlivecodebench_dataset = LiveCodeBench(scenario=\"code_generation\", version=\"release_v1\") # optional: path=\"/path/to/save_data\"\ntest_data = livecodebench_dataset.get_test_data()\n</code></pre>"},{"location":"modules/customize_agent.html","title":"CustomizeAgent","text":""},{"location":"modules/customize_agent.html#introduction","title":"Introduction","text":"<p>The <code>CustomizeAgent</code> class provides a flexible framework for creating specialized LLM-powered agents. It enables the definition of agents with well-defined inputs, outputs, custom prompt templates, and configurable parsing strategies, making it suitable for rapid prototyping and deployment of domain-specific agents.</p>"},{"location":"modules/customize_agent.html#key-features","title":"Key Features","text":"<ul> <li>No Custom Code Required: Create specialized agents through configuration rather than writing custom agent classes</li> <li>Flexible Input/Output Definitions: Define exactly what inputs your agent accepts and what outputs it produces</li> <li>Customizable Parsing Strategies: Multiple parsing modes to extract structured data from LLM responses</li> <li>Reusable Components: Save and load agent definitions for reuse across projects</li> </ul>"},{"location":"modules/customize_agent.html#basic-usage","title":"Basic Usage","text":""},{"location":"modules/customize_agent.html#simple-agent","title":"Simple Agent","text":"<p>The simplest way to create a <code>CustomizeAgent</code> is with just a name, description and prompt:</p> <p><pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\n\n# Create a simple agent\nsimple_agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"A basic agent that responds to queries\",\n    prompt=\"Answer the following question: {question}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"question\", \"type\": \"string\", \"description\": \"The question to answer\"}\n    ]\n)\n\n# Execute the agent\nresponse = simple_agent(inputs={\"question\": \"What is a language model?\"})\nprint(response.content.content)  # Access the raw response content\n</code></pre> In this example, 1. We specify the input information (including its name, type, and description) in the <code>inputs</code> parameter since the prompt requires an input. 2. Moreover, when executing the agent with <code>simple_agent(...)</code>, you should provide all the inputs in the <code>inputs</code> parameter. </p> <p>The output after executing the agent is a <code>Message</code> object, which contains the raw LLM response in <code>message.content.content</code>. </p> <p>Note</p> <p>All the input names specified in the <code>CustomizeAgent(inputs=[...])</code> should appear in the <code>prompt</code>. Otherwise, an error will be raised.</p>"},{"location":"modules/customize_agent.html#structured-outputs","title":"Structured Outputs","text":"<p>One of the most powerful features of <code>CustomizeAgent</code> is the ability to define structured outputs. This allows you to transform unstructured LLM responses into well-defined data structures that are easier to work with programmatically.</p>"},{"location":"modules/customize_agent.html#basic-structured-output","title":"Basic Structured Output","text":"<p>Here's a simple example of defining structured outputs:</p> <pre><code>from evoagentx.core.module_utils import extract_code_blocks\n\n# Create a CodeWriter agent with structured output\ncode_writer = CustomizeAgent(\n    name=\"CodeWriter\",\n    description=\"Writes Python code based on requirements\",\n    prompt=\"Write Python code that implements the following requirement: {requirement}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"}\n    ],\n    parse_mode=\"custom\",  # Use custom parsing function\n    parse_func=lambda content: {\"code\": extract_code_blocks(content)[0]}  # Extract first code block\n)\n\n# Execute the agent\nmessage = code_writer(\n    inputs={\"requirement\": \"Write a function that returns the sum of two numbers\"}\n)\nprint(message.content.code)  # Access the parsed code directly\n</code></pre> <p>In this example: 1. We define an output field named <code>code</code> in the <code>outputs</code> parameter. 2. We set <code>parse_mode=\"custom\"</code> to use a custom parsing function. 3. The <code>parse_func</code> extracts the first code block from the LLM response. 4. We can directly access the parsed code with <code>message.content.code</code>.</p> <p>You can also access the raw LLM response by <code>message.content.content</code>. </p> <p>Note</p> <ol> <li> <p>If the <code>outputs</code> parameter is set in <code>CustomizeAgent</code>, the agent will try to parse the LLM response based on the output field names. If you don't want to parse the LLM response, you should not set the <code>outputs</code> parameter. The raw LLM response can be accessed by <code>message.content.content</code>. </p> </li> <li> <p>CustomizeAgent supports different parsing modes, such as `['str', 'json', 'xml', 'title', 'custom']. Please refer to the Parsing Modes section for more details. </p> </li> </ol>"},{"location":"modules/customize_agent.html#multiple-structured-outputs","title":"Multiple Structured Outputs","text":"<p>You can define multiple output fields to create more complex structured data:</p> <pre><code># Agent that generates both code and explanation\nanalyzer = CustomizeAgent(\n    name=\"CodeAnalyzer\",\n    description=\"Generates and explains Python code\",\n    prompt=\"\"\"\n    Write Python code for: {requirement}\n\n    Provide your response in the following format:\n\n    ## code\n    [Your code implementation here]\n\n    ## explanation\n    [A brief explanation of how the code works]\n\n    ## complexity\n    [Time and space complexity analysis]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"explanation\", \"type\": \"string\", \"description\": \"Explanation of the code\"},\n        {\"name\": \"complexity\", \"type\": \"string\", \"description\": \"Complexity analysis\"}\n    ],\n    parse_mode=\"title\"  # Use default title parsing mode\n)\n\n# Execute the agent\nresult = analyzer(inputs={\"requirement\": \"Write a binary search algorithm\"})\n\n# Access each structured output separately\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nEXPLANATION:\")\nprint(result.content.explanation)\nprint(\"\\nCOMPLEXITY:\")\nprint(result.content.complexity)\n</code></pre>"},{"location":"modules/customize_agent.html#prompt-template-usage","title":"Prompt Template Usage","text":"<p>The <code>CustomizeAgent</code> also supports using <code>PromptTemplate</code> for more flexible prompt templating. For detailed information about prompt templates and their advanced features, please refer to the PromptTemplate Tutorial.</p>"},{"location":"modules/customize_agent.html#simple-prompt-template","title":"Simple Prompt Template","text":"<p>Here's a basic example using a prompt template:</p> <pre><code>from evoagentx.prompts import StringTemplate\n\nagent = CustomizeAgent(\n    name=\"FirstAgent\",\n    description=\"A simple agent that prints hello world\",\n    prompt_template=StringTemplate(\n        instruction=\"Print 'hello world'\",\n    ),\n    llm_config=openai_config\n)\n\nmessage = agent()\nprint(message.content.content)\n</code></pre>"},{"location":"modules/customize_agent.html#prompt-template-with-inputs-and-outputs","title":"Prompt Template with Inputs and Outputs","text":"<p>You can combine prompt templates with structured inputs and outputs:</p> <pre><code>code_writer = CustomizeAgent(\n    name=\"CodeWriter\",\n    description=\"Writes Python code based on requirements\",\n    prompt_template=StringTemplate(\n        instruction=\"Write Python code that implements the provided `requirement`\",\n        # You can optionally add demonstrations:\n        # demonstrations=[\n        #     {\n        #         \"requirement\": \"print 'hello world'\",\n        #         \"code\": \"print('hello world')\"\n        #     }, \n        #     {\n        #         \"requirement\": \"print 'Test Demonstration'\",\n        #         \"code\": \"print('Test Demonstration')\"\n        #     }\n        # ]\n    ), # no need to specify input placeholders in the instruction of the prompt template\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n    ],\n    parse_mode=\"custom\", \n    parse_func=lambda content: {\"code\": extract_code_blocks(content)[0]}\n)\n\nmessage = code_writer(\n    inputs={\"requirement\": \"Write a function that returns the sum of two numbers\"}\n)\nprint(message.content.code)\n</code></pre> <p>The <code>PromptTemplate</code> provides a more structured way to define prompts and can include: - A main instruction - Optional context that can be used to provide additional information - Optional constraints that the LLM should follow  - Optional demonstrations for few-shot learning - Optional tools information that the LLM can use  etc. </p> <p>Note</p> <ol> <li> <p>When using <code>prompt_template</code>, you don't need to explicitly include input placeholders in the instruction string like <code>{input_name}</code>. The template will automatically handle the mapping of inputs. </p> </li> <li> <p>Also, you don't need to explicitly specify the output format in the <code>instruction</code> field of the <code>PromptTemplate</code>. The template will automatically formulate the output format based on the <code>outputs</code> parameter and the <code>parse_mode</code> parameter. However, <code>PromptTemplate</code> also supports explicitly specifying the output format by specifying <code>PromptTemplate.format(custom_output_format=\"...\")</code>. </p> </li> </ol>"},{"location":"modules/customize_agent.html#parsing-modes","title":"Parsing Modes","text":"<p>CustomizeAgent supports different ways to parse the LLM output:</p>"},{"location":"modules/customize_agent.html#1-string-mode-parse_modestr","title":"1. String Mode (<code>parse_mode=\"str\"</code>)","text":"<p>Uses the raw LLM output as the value for each output field. Useful for simple agents where structured parsing isn't needed.</p> <pre><code>agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"Returns raw output\",\n    prompt=\"Generate a greeting for {name}\",\n    inputs=[{\"name\": \"name\", \"type\": \"string\", \"description\": \"The name to greet\"}],\n    outputs=[{\"name\": \"greeting\", \"type\": \"string\", \"description\": \"The generated greeting\"}],\n    parse_mode=\"str\",\n    # other parameters...\n)\n</code></pre> <p>After executing the agent, you can access the raw LLM response by <code>message.content.content</code> or <code>message.content.greeting</code>.  </p>"},{"location":"modules/customize_agent.html#2-title-mode-parse_modetitle-default","title":"2. Title Mode (<code>parse_mode=\"title\"</code>, default)","text":"<p>Extracts content between titles matching output field names. This is the default parsing mode.</p> <p><pre><code>agent = CustomizeAgent(\n    name=\"ReportGenerator\",\n    description=\"Generates a structured report\",\n    prompt=\"Create a report about {topic}\",\n    outputs=[\n        {\"name\": \"summary\", \"type\": \"string\", \"description\": \"Brief summary\"},\n        {\"name\": \"analysis\", \"type\": \"string\", \"description\": \"Detailed analysis\"}\n    ],\n    # Default title pattern is \"## {title}\"\n    title_format=\"### {title}\",  # Optional: customize title format\n    # other parameters...\n)\n</code></pre> With this configuration, the LLM should be instructed to format its response like (only required when passing the complete prompt with <code>prompt</code> parameter to instantiate the <code>CustomizeAgent</code>. If using <code>prompt_template</code>, you don't need to specify this):</p> <pre><code>### summary\nBrief summary of the topic here.\n\n### analysis\nDetailed analysis of the topic here.\n</code></pre> <p>Note</p> <p>The section titles output by the LLM should be exactly the same as the output field names. Otherwise, the parsing will fail. For instance, in above example, if the LLM outputs <code>### Analysis</code>, which is different from the output field name <code>analysis</code>, the parsing will fail. </p>"},{"location":"modules/customize_agent.html#3-json-mode-parse_modejson","title":"3. JSON Mode (<code>parse_mode=\"json\"</code>)","text":"<p>Parse the JSON string output by the LLM. The keys of the JSON string should be exactly the same as the output field names. </p> <p><pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n        {\"name\": \"places\", \"type\": \"string\", \"description\": \"Locations mentioned\"},\n        {\"name\": \"dates\", \"type\": \"string\", \"description\": \"Dates mentioned\"}\n    ],\n    parse_mode=\"json\",\n    # other parameters...\n)\n</code></pre> When using this mode, the LLM should output a valid JSON string with keys matching the output field names. For instance, you should instruct the LLM to output (only required when passing the complete prompt with <code>prompt</code> parameter to instantiate the <code>CustomizeAgent</code>. If using <code>prompt_template</code>, you don't need to specify this):</p> <p><pre><code>{\n    \"people\": \"extracted people\",\n    \"places\": \"extracted places\",\n    \"dates\": \"extracted dates\"\n}\n</code></pre> If there are multiple JSON string in the LLM response, only the first one will be used. </p>"},{"location":"modules/customize_agent.html#4-xml-mode-parse_modexml","title":"4. XML Mode (<code>parse_mode=\"xml\"</code>)","text":"<p>Parse the XML string output by the LLM. The keys of the XML string should be exactly the same as the output field names.  </p> <pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n    ],\n    parse_mode=\"xml\",\n    # other parameters...\n)\n</code></pre> <p>When using this mode, the LLM should generte texts containing xml tags with keys matching the output field names. For instance, you should instruct the LLM to output (only required when passing the complete prompt with <code>prompt</code> parameter to instantiate the <code>CustomizeAgent</code>. If using <code>prompt_template</code>, you don't need to specify this):</p> <pre><code>The people mentioned in the text are: &lt;people&gt;John Doe and Jane Smith&lt;/people&gt;.\n</code></pre> <p>If the LLM output contains multiple xml tags with the same name, only the first one will be used. </p>"},{"location":"modules/customize_agent.html#5-custom-parsing-parse_modecustom","title":"5. Custom Parsing (<code>parse_mode=\"custom\"</code>)","text":"<p>For maximum flexibility, you can define a custom parsing function:</p> <pre><code>from evoagentx.core.registry import register_parse_function\n\n@register_parse_function  # Register the function for serialization\ndef extract_python_code(content: str) -&gt; dict:\n    \"\"\"Extract Python code from LLM response\"\"\"\n    code_blocks = extract_code_blocks(content)\n    return {\"code\": code_blocks[0] if code_blocks else \"\"}\n\nagent = CustomizeAgent(\n    name=\"CodeExplainer\",\n    description=\"Generates and explains code\",\n    prompt=\"Write a Python function that {requirement}\",\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The requirement to generate code for\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated code\"},\n    ],\n    parse_mode=\"custom\",\n    parse_func=extract_python_code,\n    # other parameters...\n)\n</code></pre> <p>Note</p> <ol> <li> <p>The parsing function should have an input parameter <code>content</code> that takes the raw LLM response as input, and return a dictionary with keys matching the output field names. </p> </li> <li> <p>It is recommended to use the <code>@register_parse_function</code> decorator to register the parsing function for serialization, so that you can save the agent and load it later. </p> </li> </ol>"},{"location":"modules/customize_agent.html#saving-and-loading-agents","title":"Saving and Loading Agents","text":"<p>You can save agent definitions to reuse them later:</p> <pre><code># Save agent configuration. By default, the `llm_config` will not be saved. \ncode_writer.save_module(\"./agents/code_writer.json\")\n\n# Load agent from file (requires providing llm_config again)\nloaded_agent = CustomizeAgent.from_file(\n    \"./agents/code_writer.json\", \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"modules/customize_agent.html#advanced-example-multi-step-code-generator","title":"Advanced Example: Multi-Step Code Generator","text":"<p>Here's a more advanced example that demonstrates creating a specialized code generation agent with multiple structured outputs:</p> <pre><code>from pydantic import Field\nfrom evoagentx.actions import ActionOutput\nfrom evoagentx.core.registry import register_parse_function\n\nclass CodeGeneratorOutput(ActionOutput):\n    code: str = Field(description=\"The generated Python code\")\n    documentation: str = Field(description=\"Documentation for the code\")\n    tests: str = Field(description=\"Unit tests for the code\")\n\n@register_parse_function\ndef parse_code_documentation_tests(content: str) -&gt; dict:\n    \"\"\"Parse LLM output into code, documentation, and tests sections\"\"\"\n    sections = content.split(\"## \")\n    result = {\"code\": \"\", \"documentation\": \"\", \"tests\": \"\"}\n\n    for section in sections:\n        if not section.strip():\n            continue\n\n        lines = section.strip().split(\"\\n\")\n        section_name = lines[0].lower()\n        section_content = \"\\n\".join(lines[1:]).strip()\n\n        if \"code\" in section_name:\n            # Extract code from code blocks\n            code_blocks = extract_code_blocks(section_content)\n            result[\"code\"] = code_blocks[0] if code_blocks else section_content\n        elif \"documentation\" in section_name:\n            result[\"documentation\"] = section_content\n        elif \"test\" in section_name:\n            # Extract code from code blocks if present\n            code_blocks = extract_code_blocks(section_content)\n            result[\"tests\"] = code_blocks[0] if code_blocks else section_content\n\n    return result\n\n# Create the advanced code generator agent\nadvanced_generator = CustomizeAgent(\n    name=\"AdvancedCodeGenerator\",\n    description=\"Generates complete code packages with documentation and tests\",\n    prompt=\"\"\"\n    Create a complete implementation based on this requirement:\n    {requirement}\n\n    Provide your response in the following format:\n\n    ## Code\n    [Include the Python code implementation here]\n\n    ## Documentation\n    [Include clear documentation explaining the code]\n\n    ## Tests\n    [Include unit tests that verify the code works correctly]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"documentation\", \"type\": \"string\", \"description\": \"Documentation for the code\"},\n        {\"name\": \"tests\", \"type\": \"string\", \"description\": \"Unit tests for the code\"}\n    ],\n    output_parser=CodeGeneratorOutput,\n    parse_mode=\"custom\",\n    parse_func=parse_code_documentation_tests,\n    system_prompt=\"You are an expert Python developer specialized in writing clean, efficient code with comprehensive documentation and tests.\"\n)\n\n# Execute the agent\nresult = advanced_generator(\n    inputs={\n        \"requirement\": \"Create a function to validate if a string is a valid email address\"\n    }\n)\n\n# Access the structured outputs\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nDOCUMENTATION:\")\nprint(result.content.documentation)\nprint(\"\\nTESTS:\")\nprint(result.content.tests)\n</code></pre> <p>This advanced example demonstrates how to create a specialized agent that produces multiple structured outputs from a single LLM call, providing a complete code package with implementation, documentation, and tests.</p>"},{"location":"modules/evaluator.html","title":"Evaluator","text":""},{"location":"modules/evaluator.html#introduction","title":"Introduction","text":"<p>The <code>Evaluator</code> class is a fundamental component in the EvoAgentX framework for evaluating the performance of workflows and action graphs against benchmarks. It provides a structured way to measure how well AI agents perform on specific tasks by running evaluations on test data and computing metrics.</p>"},{"location":"modules/evaluator.html#architecture","title":"Architecture","text":""},{"location":"modules/evaluator.html#evaluator-architecture","title":"Evaluator Architecture","text":"<p>An <code>Evaluator</code> consists of several key components:</p> <ol> <li> <p>LLM Instance: </p> <p>The language model used for executing workflows during evaluation:</p> <ul> <li>Provides the reasoning and generation capabilities needed for workflow execution</li> <li>Can be any implementation that follows the <code>BaseLLM</code> interface</li> </ul> </li> <li> <p>Agent Manager: </p> <p>Manages the agents used by workflow graphs during evaluation:</p> <ul> <li>Provides access to agents needed for workflow execution</li> <li>Only required when evaluating <code>WorkFlowGraph</code> instances, and can be ignored when evaluating <code>ActionGraph</code> instances </li> </ul> </li> <li> <p>Data Processing Functions:</p> <p>Functions that prepare and process data during evaluation:</p> <ul> <li><code>collate_func</code>: Prepares benchmark examples for workflow execution</li> <li><code>output_postprocess_func</code>: Processes workflow outputs before evaluation</li> </ul> </li> </ol>"},{"location":"modules/evaluator.html#evaluation-process","title":"Evaluation Process","text":"<p>The evaluation process follows these steps:</p> <ol> <li>Data Processing: Obtain examples from the benchmark dataset and process them into the format expected by the workflow graph or action graph</li> <li>Workflow Execution: Run each example through the workflow graph or action graph</li> <li>Output Processing: Process the outputs into the format expected by the benchmark</li> <li>Metric Calculation: Compute performance metrics by comparing outputs to ground truth</li> <li>Result Aggregation: Aggregate individual metrics into overall performance scores</li> </ol>"},{"location":"modules/evaluator.html#usage","title":"Usage","text":""},{"location":"modules/evaluator.html#basic-evaluator-creation--execution","title":"Basic Evaluator Creation &amp; Execution","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# Initialize agent manager\nagent_manager = AgentManager()\n\n# Load your workflow graph\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\n\n# Add agents to the agent manager\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# Create benchmark\nbenchmark = SomeBenchmark()\n\n# Create evaluator\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4,  # Use 4 parallel workers\n    verbose=True    # Show progress bars\n)\n\n# Run evaluation with suppressed logging\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow_graph,\n        benchmark=benchmark,\n        eval_mode=\"test\",    # Evaluate on test split (default)\n        sample_k=100         # Use 100 random examples\n    )\n\nprint(f\"Evaluation results: {results}\")\n</code></pre>"},{"location":"modules/evaluator.html#customizing-data-processing","title":"Customizing Data Processing","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Custom collate function to prepare inputs. The keys should match the input parameters of the workflow graph or action graph. The return value will be directly passed to the `execute` method of the workflow graph or action graph. \ndef custom_collate(example):\n    return {\n        \"input_text\": example[\"question\"],\n        \"context\": example.get(\"context\", \"\")\n    }\n\n# Custom output processing, `output` is the output of the workflow and the return value will be passed to the `evaluate` method of the benchmark.  \ndef custom_postprocess(output):\n    if isinstance(output, dict):\n        return output.get(\"answer\", \"\")\n    return output\n\n# Create evaluator with custom functions\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    collate_func=custom_collate,\n    output_postprocess_func=custom_postprocess,\n    num_workers=4,  # Use 4 parallel workers\n    verbose=True    # Show progress bars\n)\n</code></pre>"},{"location":"modules/evaluator.html#evaluating-an-action-graph","title":"Evaluating an Action Graph","text":"<pre><code>from evoagentx.workflow.action_graph import ActionGraph\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# Load your action graph\naction_graph = ActionGraph.from_file(\"path/to/action_graph.json\", llm_config=llm_config)\n\n# Create evaluator (no agent_manager needed for action graphs)\nevaluator = Evaluator(llm=llm, num_workers=4, verbose=True)\n\n# Run evaluation with suppressed logging\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=action_graph,\n        benchmark=benchmark\n    )\n</code></pre>"},{"location":"modules/evaluator.html#asynchronous-evaluation","title":"Asynchronous Evaluation","text":"<pre><code>import asyncio\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize LLM and components\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nagent_manager = AgentManager()\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\nbenchmark = SomeBenchmark()\n\n# Create evaluator\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4\n)\n\n# Run async evaluation\nasync def run_async_eval():\n    with suppress_logger_info():\n        results = await evaluator.async_evaluate(\n            graph=workflow_graph,\n            benchmark=benchmark\n        )\n    return results\n\n# Execute async evaluation\nresults = asyncio.run(run_async_eval())\n</code></pre>"},{"location":"modules/evaluator.html#accessing-evaluation-records","title":"Accessing Evaluation Records","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# Initialize components\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nbenchmark = SomeBenchmark()\nevaluator = Evaluator(llm=llm)\n\n# Run evaluation with suppressed logging\nwith suppress_logger_info():\n    evaluator.evaluate(graph=graph, benchmark=benchmark)\n\n# Get all evaluation records\nall_records = evaluator.get_all_evaluation_records()\n\n# Get record for a specific example\nexample = benchmark.get_test_data()[0]\nrecord = evaluator.get_example_evaluation_record(benchmark, example)\n\n# Get record by example ID\nrecord_by_id = evaluator.get_evaluation_record_by_id(\n    benchmark=benchmark,\n    example_id=\"example-123\",\n    eval_mode=\"test\"\n)\n\n# Access trajectory for workflow graph evaluations\nif \"trajectory\" in record:\n    for message in record[\"trajectory\"]:\n        print(f\"{message.role}: {message.content}\")\n</code></pre> <p>The <code>Evaluator</code> class provides a powerful way to assess the performance of workflows and action graphs, enabling quantitative comparison and improvement tracking in the EvoAgentX framework.</p>"},{"location":"modules/llm.html","title":"LLM","text":""},{"location":"modules/llm.html#introduction","title":"Introduction","text":"<p>The LLM (Large Language Model) module provides a unified interface for interacting with various language model providers in the EvoAgentX framework. It abstracts away provider-specific implementation details, offering a consistent API for generating text, managing costs, and handling responses.</p>"},{"location":"modules/llm.html#supported-llm-providers","title":"Supported LLM Providers","text":"<p>EvoAgentX currently supports the following LLM providers:</p>"},{"location":"modules/llm.html#openaillm","title":"OpenAILLM","text":"<p>The primary implementation for accessing OpenAI's language models. It handles authentication, request formatting, and response parsing for models like GPT-4, GPT-3.5-Turbo, and other OpenAI models.</p> <p>Basic Usage:</p> <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# Configure the model\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",  \n    openai_key=\"your-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = OpenAILLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Explain quantum computing in simple terms.\",\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)\n</code></pre>"},{"location":"modules/llm.html#litellm","title":"LiteLLM","text":"<p>LiteLLM is an adapter for the LiteLLM project, which provides a unified Python SDK and proxy server for calling over 100 LLM APIs using the OpenAI API format. It supports providers such as Bedrock, Azure, OpenAI, VertexAI, Cohere, Anthropic, Sagemaker, HuggingFace, Replicate, and Groq. Thanks to this project, the <code>LiteLLM</code> model class in EvoAgentX can be used to seamlessly access a wide range of LLM providers through a single interface. </p> <p>Basic Usage:</p> <p>To faciliate seamless integration with LiteLLM, you should specify the model name using the naming convention defied in the LiteLLM platform. For example, you need to specify <code>anthropic/claude-3-opus-20240229</code> for Claude 3.0 Opus. You can find a full list of supported providers and model names in their official documentation: https://docs.litellm.ai/docs/providers.</p> <pre><code>from evoagentx.models import LiteLLMConfig, LiteLLM\n\n# Configure the model\nconfig = LiteLLMConfig(\n    model=\"anthropic/claude-3-opus-20240229\", \n    anthropic_key=\"your-anthropic-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = LiteLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Design a system for autonomous vehicles.\",\n    system_message=\"You are an expert in autonomous systems design.\"\n)\n</code></pre>"},{"location":"modules/llm.html#siliconflowllm","title":"SiliconFlowLLM","text":"<p>SiliconFlowLLM is an adapter for models hosted on the SiliconFlow platform, which offers access to both open-source and proprietary models via an OpenAI-compatible API. It enables you to integrate models like Qwen, DeepSeek, or Mixtral by specifying their names using the SiliconFlow platform's naming conventions.</p> <p>Thanks to SiliconFlow's unified interface, the <code>SiliconFlowLLM</code> model class in EvoAgentX allows seamless switching between a variety of powerful LLMs hosted on SiliconFlow using the same API format.</p> <p>Basic Usage:</p> <pre><code>from evoagentx.models import SiliconFlowConfig, SiliconFlowLLM\n\n# Configure the model\nconfig = SiliconFlowConfig(\n    model=\"deepseek-ai/DeepSeek-V3\",\n    siliconflow_key=\"your-siliconflow-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = SiliconFlowLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Write a poem about artificial intelligence.\",\n    system_message=\"You are a creative poet.\"\n)\n</code></pre>"},{"location":"modules/llm.html#openrouterllm","title":"OpenRouterLLM","text":"<p>OpenRouterLLM is an adapter for the OpenRouter platform, which provides access to a wide range of language models from various providers through a unified API. It supports models from providers like Anthropic, Google, Meta, Mistral AI, and more, all accessible through a single interface.</p> <p>The <code>OpenRouterLLM</code> model class in EvoAgentX enables you to easily switch between different models hosted on OpenRouter while maintaining a consistent API format. This makes it simple to experiment with different models and find the best one for your specific use case.</p> <p>Basic Usage:</p> <pre><code>from evoagentx.models import OpenRouterConfig, OpenRouterLLM\n\n# Configure the model\nconfig = OpenRouterConfig(\n    model=\"openai/gpt-4o-mini\",  # or any other model supported by OpenRouter\n    openrouter_key=\"your-openrouter-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = OpenRouterLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Analyze the impact of artificial intelligence on healthcare.\",\n    system_message=\"You are an AI ethics expert specializing in healthcare applications.\"\n)\n</code></pre>"},{"location":"modules/llm.html#aliyun-llm","title":"Aliyun LLM","text":"<p>AliyunLLM is an implementation of the EvoAgentX framework for accessing the Aliyun Tongyi Qianqian family of models. It provides seamless integration with Aliyun DashScope API and supports various models of Tongyiqianqian, including qwen-turbo, qwen-plus, qwen-max and so on. We have included reference costs for your consideration; however, please note that actual expenses should be regarded as the definitive amount.</p> <p>To utilize the DashScope API with AliyunLLM, an API key from Bailian is required. The following steps outline the process:</p> <p>Basic Usage:</p> <p>Execute the following command in your bash terminal to set the API key:</p> <pre><code>export DASHSCOPE_API_KEY=\"your-api-key-here\"\n</code></pre> <p>You can use python to call the model as below template.</p> <pre><code>from evoagentx.models import AliyunLLM, AliyunLLMConfig\n\n# Configure the model\nconfig = AliyunLLMConfig(\n    model=\"qwen-turbo\",  # you can use qwen-turbo, qwen-plus, qwen-max and so on.\n    aliyun_api_key=\"Your DASHSCOPE_API_KEY\",\n    temperature=0.7,\n    max_tokens=2000,\n    stream=False,\n    output_response=True\n)\n\n# Initialize the model\nllm = AliyunLLM(config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Explain quantum computing in simple terms.\",\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)\n</code></pre>"},{"location":"modules/llm.html#local-llm","title":"Local LLM","text":"<p>We now support calling local models for your tasks, built on the LiteLLM framework for a familiar user experience. For example, to use Ollama, follow these steps:</p> <ol> <li>Download the desired model, such as <code>ollama3</code>.</li> <li>Run the model locally.</li> <li>Configure the settings by specifying <code>api_base</code> (typically port <code>11434</code>) and setting <code>is_local</code> to <code>True</code>.</li> </ol> <p>You're now ready to leverage your local model seamlessly!</p> <p>Basic Usage:</p> <pre><code>from evoagentx.models.model_configs import LiteLLMConfig\nfrom evoagentx.models import LiteLLM\n\n# use local model\nconfig = LiteLLMConfig(\n    model=\"ollama/llama3\",\n    api_base=\"http://localhost:11434\",\n    is_local=True,\n    temperature=0.7,\n    max_tokens=1000,\n    output_response=True\n)\n\n# Generate \nllm = LiteLLM(config)\nresponse = llm.generate(prompt=\"What is Agentic Workflow?\")\n</code></pre>"},{"location":"modules/llm.html#core-functions","title":"Core Functions","text":"<p>All LLM implementations in EvoAgentX provide a consistent set of core functions for generating text and managing the generation process.</p>"},{"location":"modules/llm.html#generate-function","title":"Generate Function","text":"<p>The <code>generate</code> function is the primary method for producing text with language models:</p> <pre><code>def generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    Generate text based on the prompt and optional system message.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output into a structured format.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        parse_func: A function to apply to the parsed output.\n        **kwargs: Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n    \"\"\"\n</code></pre>"},{"location":"modules/llm.html#inputs","title":"Inputs","text":"<p>In EvoAgentX, there are several ways to provide inputs to LLMs using the <code>generate</code> function:</p> <p>Method 1: Prompt and System Message</p> <ol> <li> <p>Prompt: The specific query or instruction for which you want a response. </p> </li> <li> <p>System Message (optional): Instructions that guide the model's overall behavior and role. This sets the context for how the model should respond.</p> </li> </ol> <p>Together, these components are converted into a standardized message format that the language model can understand:</p> <pre><code># Simple example with prompt and system message\nresponse = llm.generate(\n    prompt=\"What are three ways to improve productivity?\",\n    system_message=\"You are a productivity expert providing concise, actionable advice.\"\n)\n</code></pre> <p>Behind the scenes, this gets converted into messages with appropriate roles:</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a productivity expert providing concise, actionable advice.\"},\n    {\"role\": \"user\", \"content\": \"What are three ways to improve productivity?\"}\n]\n</code></pre> <p>Method 2: Using Messages Directly</p> <p>For more complex conversations or when you need precise control over the message format, you can use the <code>messages</code> parameter directly:</p> <pre><code># Using messages directly for a multi-turn conversation\nresponse = llm.generate(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm an AI assistant designed to help with various tasks.\"},\n        {\"role\": \"user\", \"content\": \"Can you help me with programming?\"}\n    ]\n)\n</code></pre>"},{"location":"modules/llm.html#batch-generation","title":"Batch Generation","text":"<p>For batch processing, you can provide lists of prompts/system messages or list of messages. For example: </p> <pre><code># Batch processing example\nresponses = llm.generate(\n    prompt=[\"What is machine learning?\", \"Explain neural networks.\"],\n    system_message=[\"You are a data scientist.\", \"You are an AI researcher.\"]\n)\n</code></pre>"},{"location":"modules/llm.html#parse-modes","title":"Parse Modes","text":"<p>EvoAgentX supports several parsing strategies:</p> <ol> <li>\"str\": Uses the raw output as-is for each field defined in the parser.</li> <li>\"json\" (default): Extracts fields from a JSON string in the output.</li> <li>\"xml\": Extracts content from XML tags matching field names.</li> <li>\"title\": Extracts content from markdown sections (default format: \"## {title}\").</li> <li>\"custom\": Uses a custom parsing function specified by <code>parse_func</code>.</li> </ol> <p>Note</p> <p>For <code>'json'</code>, <code>'xml'</code> and <code>'title'</code>, you should instruct the LLM (through the <code>prompt</code>) to output the content in the specified format that can be parsed by the parser. Otherwise, the parsing will fail. </p> <ol> <li> <p>For <code>'json'</code>, you should instruct the LLM to output a valid JSON string containing keys that match the field names in the parser class. If there are multiple JSON string in the raw LLM output, only the first one will be parsed.  </p> </li> <li> <p>For <code>xml</code>, you should instruct the LLM to output content that contains XML tags matching the field names in the parser class, e.g., <code>&lt;{field_name}&gt;...&lt;/{field_name}&gt;</code>. If there are multiple XML tags with the same field name, only the first one will be used. </p> </li> <li> <p>For <code>title</code>, you should instruct the LLM to output content that contains markdown sections with the title exactly matching the field names in the parser class. The default title format is \"## {title}\". You can change it by setting the <code>title_format</code> parameter in the <code>generate</code> function, e.g., <code>generate(..., title_format=\"### {title}\")</code>. The <code>title_format</code> must contain <code>{title}</code> as a placeholder for the field name.  </p> </li> </ol>"},{"location":"modules/llm.html#custom-parsing-function","title":"Custom Parsing Function","text":"<p>For maximum flexibility, you can define a custom parsing function with <code>parse_func</code>:</p> <pre><code>from evoagentx.models import LLMOutputParser\nfrom evoagentx.core.module_utils import extract_code_block\n\nclass CodeOutput(LLMOutputParser):\n    code: str = Field(description=\"The generated code\")\n\n# Use custom parsing\nresponse = llm.generate(\n    prompt=\"Write a Python function to calculate Fibonacci numbers.\",\n    parser=CodeOutput,\n    parse_mode=\"custom\",\n    parse_func=lambda content: {\"code\": extract_code_block(content)[0]}\n)\n</code></pre> <p>Note</p> <p>The <code>parse_func</code> should have an input parameter <code>content</code> that receives the raw LLM output, and return a dictionary with keys matching the field names in the parser class.  </p>"},{"location":"modules/llm.html#async-generate-function","title":"Async Generate Function","text":"<p>For applications requiring asynchronous operation, the <code>async_generate</code> function provides the same functionality as the <code>generate</code> function, but in a non-blocking manner:</p> <pre><code>async def async_generate(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        system_message: Optional[Union[str, List[str]]] = None,\n        messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n        parser: Optional[Type[LLMOutputParser]] = None,\n        parse_mode: Optional[str] = \"json\", \n        parse_func: Optional[Callable] = None,\n        **kwargs\n    ) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    Asynchronously generate text based on the prompt and optional system message.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output into a structured format.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        parse_func: A function to apply to the parsed output.\n        **kwargs: Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n    \"\"\"\n</code></pre>"},{"location":"modules/llm.html#streaming-responses","title":"Streaming Responses","text":"<p>EvoAgentX supports streaming responses from LLMs, which allows you to see the model's output as it's being generated token by token, rather than waiting for the complete response. This is especially useful for long-form content generation or providing a more interactive experience.</p> <p>There are two ways to enable streaming:</p>"},{"location":"modules/llm.html#configure-streaming-in-the-llm-config","title":"Configure Streaming in the LLM Config","text":"<p>You can enable streaming when initializing the LLM by setting appropriate parameters in the config:</p> <pre><code># Enable streaming at initialization time\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\",\n    stream=True,  # Enable streaming\n    output_response=True  # Print tokens to console in real-time\n)\n\nllm = OpenAILLM(config=config)\n\n# All calls to generate() will now stream by default\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\"\n)\n</code></pre>"},{"location":"modules/llm.html#enable-streaming-in-the-generate-method","title":"Enable Streaming in the Generate Method","text":"<p>Alternatively, you can enable streaming for specific generate calls:</p> <pre><code># LLM initialized with default non-streaming behavior\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\"\n)\n\nllm = OpenAILLM(config=config)\n\n# Override for this specific call\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\",\n    stream=True,  # Enable streaming for this call only\n    output_response=True  # Print tokens to console in real-time\n)\n</code></pre>"},{"location":"modules/prompt_template.html","title":"PromptTemplate","text":""},{"location":"modules/prompt_template.html#introduction","title":"Introduction","text":"<p>The <code>PromptTemplate</code> class provides a flexible and structured way to define prompts for language models. It supports various components like instructions, context, constraints, tools, and demonstrations, making it easier to create consistent and well-formatted prompts.</p>"},{"location":"modules/prompt_template.html#key-features","title":"Key Features","text":"<ul> <li>Structured Prompt Components: Define prompts with clear sections for instruction, context, constraints, and more</li> <li>Flexible Output Formats: Support for multiple output parsing modes (JSON, XML, Title format)</li> <li>Few-Shot Learning Support: Easy integration of demonstrations for few-shot learning</li> <li>Input/Output Validation: Automatic validation of required inputs and outputs</li> <li>Chat Format Support: Special support for chat-based interactions via <code>ChatTemplate</code></li> </ul>"},{"location":"modules/prompt_template.html#basic-usage","title":"Basic Usage","text":""},{"location":"modules/prompt_template.html#simple-template","title":"Simple Template","text":"<p>The simplest way to create a <code>PromptTemplate</code> is with just an instruction:</p> <pre><code>from evoagentx.prompts import StringTemplate\n\ntemplate = StringTemplate(\n    instruction=\"Write a function that calculates the factorial of a number\"\n)\n\n# Format the template into a prompt string\nprompt = template.format()\n</code></pre>"},{"location":"modules/prompt_template.html#template-with-context-and-constraints","title":"Template with Context and Constraints","text":"<p>You can add context and constraints to provide more guidance:</p> <pre><code>template = StringTemplate(\n    instruction=\"Write a function that calculates the factorial of a number\",\n    context=\"The factorial of a number n is the product of all positive integers less than or equal to n\",\n    constraints=[\n        \"Use a recursive implementation\",\n        \"Include input validation\",\n        \"Add docstring with examples\"\n    ]\n)\n\nprompt = template.format()\n</code></pre>"},{"location":"modules/prompt_template.html#template-with-demonstrations","title":"Template with Demonstrations","text":"<p>You can add examples for few-shot learning:</p> <pre><code>from evoagentx.models import OpenAILLMConfig\nfrom evoagentx.actions import ActionInput, ActionOutput\n\ntemplate = StringTemplate(\n    instruction=\"Write a function that implements the provided requirement\",\n    demonstrations=[\n        {\n            \"requirement\": \"Write a function that returns the sum of two numbers\",\n            \"code\": \"def add_numbers(a: int, b: int) -&gt; int:\\n    return a + b\"\n        },\n        {\n            \"requirement\": \"Write a function that checks if a number is even\",\n            \"code\": \"def is_even(n: int) -&gt; bool:\\n    return n % 2 == 0\"\n        }\n    ]\n)\n\nclass InputFormat(ActionInput):\n    requirement: str\n\nclass OutputFormat(ActionOutput):\n    code: str\n\nprompt = template.format(\n    values={\"requirement\": \"Write a function that return the factorial of a number\"}, \n    inputs_format=InputFormat,\n    outputs_format=OutputFormat,\n)\n</code></pre> <p>Note</p> <p><code>inputs_format</code> and <code>outputs_format</code> are required when using <code>demonstrations</code> to correctly map the inputs and outputs to the demonstrations. </p>"},{"location":"modules/prompt_template.html#structured-output-formats","title":"Structured Output Formats","text":"<p>By default, the template automatically generate the output format based on the <code>outputs_format</code> and <code>parse_mode</code>. </p>"},{"location":"modules/prompt_template.html#title-format-default","title":"Title Format (Default)","text":"<p><pre><code>template = StringTemplate(\n    instruction=\"Analyze the given text\",\n    inputs_format=TextAnalysisInput, # A Pydantic model with text field\n    outputs_format=TextAnalysisOutput,  # A Pydantic model with summary and sentiment fields\n    parse_mode=\"title\"\n)\n</code></pre> The above <code>template</code> will generate output in format: <pre><code>## summary\n[Summary content]\n\n## sentiment\n[Sentiment analysis]\n</code></pre></p>"},{"location":"modules/prompt_template.html#json-format","title":"JSON Format","text":"<p><pre><code>template = StringTemplate(\n    instruction=\"Analyze the given text\",\n    inputs_format=TextAnalysisInput,\n    outputs_format=TextAnalysisOutput, \n    parse_mode=\"json\"\n)\n</code></pre> The above <code>template</code> will generate output in format: <pre><code>{\n    \"summary\": \"[Summary content]\",\n    \"sentiment\": \"[Sentiment analysis]\"\n}\n</code></pre></p>"},{"location":"modules/prompt_template.html#xml-format","title":"XML Format","text":"<p><pre><code>template = StringTemplate(\n    instruction=\"Analyze the given text\",\n    inputs_format=TextAnalysisInput,\n    outputs_format=TextAnalysisOutput,\n    parse_mode=\"xml\"\n)\n</code></pre> The above <code>template</code> will generate output in format: <pre><code>&lt;summary&gt;\n[Summary content]\n&lt;/summary&gt;\n&lt;sentiment&gt;\n[Sentiment analysis]\n&lt;/sentiment&gt;\n</code></pre></p> <p>Note</p> <ol> <li> <p>For <code>parse_mode=\"str\" or \"custom\"</code>, the model will follow the instruction to generate the response. </p> </li> <li> <p>You can override the output format by setting <code>template.format(custom_output_format=...)</code>, see Custom Output Format. </p> </li> </ol>"},{"location":"modules/prompt_template.html#chat-template","title":"Chat Template","text":"<p>For chat-based interactions, you can use the <code>ChatTemplate</code> class:</p> <pre><code>from evoagentx.prompts import ChatTemplate\n\ntemplate = ChatTemplate(\n    instruction=\"You are a helpful coding assistant\",\n    context=\"You help users write Python code\",\n    constraints=[\"Always include comments\", \"Follow PEP 8 style guide\"]\n)\n\n# Format will return a list of chat messages\nmessages = template.format(\n    inputs_format=CodeInputs,\n    outputs_format=CodeOutputs,\n    values={\"requirement\": \"Write a sorting function\"}\n)\n</code></pre> <p>The formatted output will be a list of messages suitable for chat-based models:</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"...\"},\n    {\"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"}\n]\n</code></pre>"},{"location":"modules/prompt_template.html#best-practices","title":"Best Practices","text":"<ol> <li>Clear Instructions: Make your instruction specific and unambiguous</li> <li>Relevant Context: Include only context that's directly relevant to the task</li> <li>Specific Constraints: List constraints that meaningfully guide the output</li> <li>Representative Demonstrations: Choose examples that cover different cases</li> <li>Appropriate Output Format: Choose the parsing mode that best fits your needs</li> </ol>"},{"location":"modules/prompt_template.html#advanced-features","title":"Advanced Features","text":""},{"location":"modules/prompt_template.html#custom-output-format","title":"Custom Output Format","text":"<p>You can specify a custom output format:</p> <pre><code>template = StringTemplate(\n    instruction=\"Generate code documentation\"\n)\n\nprompt = template.format(\n    values={\"code\": \"...\"},\n    custom_output_format=\"\"\"\n    Please provide your response in the following format:\n\n    # USAGE\n    [Code usage examples]\n\n    # API\n    [API documentation]\n\n    # NOTES\n    [Additional notes and warnings]\n    \"\"\"\n)\n</code></pre>"},{"location":"modules/rag.html","title":"RAGEngine Module Documentation","text":""},{"location":"modules/rag.html#overview","title":"Overview","text":"<p>The <code>RAGEngine</code> module is the core component of the Retrieval-Augmented Generation (RAG) system, designed to manage document indexing, storage, and retrieval for efficient information access. Built on top of LlamaIndex, it integrates with various storage backends (e.g., SQLite, FAISS, Neo4j) and supports multiple index types (e.g., vector, graph). This module is part of a long-term memory management framework, enabling agents to process and query large datasets effectively.</p>"},{"location":"modules/rag.html#purpose","title":"Purpose","text":"<p>The <code>RAGEngine</code> serves as the central interface for: - Document Processing: Loading and chunking documents from various formats (e.g., PDF, text). - Indexing: Creating and managing indices for efficient retrieval. - Retrieval: Querying indexed data with support for metadata filtering and similarity-based search. - Storage Management: Persisting indices to files or databases for scalability.</p> <p>It is designed to integrate with <code>StorageHandler</code> and <code>MemoryManager</code> components, making it suitable for agent systems requiring contextual knowledge retrieval.</p>"},{"location":"modules/rag.html#key-features","title":"Key Features","text":"<ul> <li>Flexible Document Loading: Supports loading files from directories with customizable filters (e.g., file suffixes, exclusion lists).</li> <li>Chunking and Embedding: Automatically chunks documents and generates embeddings using configurable models (e.g., OpenAI\u2019s <code>text-embedding-ada-002</code>).</li> <li>Multi-Index Support: Handles different index types (vector, graph) for diverse retrieval needs.</li> <li>Advanced Retrieval: Supports metadata filters, similarity cutoffs, and keyword-based queries, with asynchronous and multi-threaded retrieval.</li> <li>Storage Integration: Seamlessly integrates with SQLite for metadata, vector stores like FAISS for embeddings and graph stores like neo4j for relations of the entity.</li> <li>Persistence: Saves and loads indices to/from files or databases, ensuring data durability.</li> <li>Error Handling: Robust logging and exception handling for reliable operation.</li> </ul>"},{"location":"modules/rag.html#rag-pipeline-overview","title":"RAG Pipeline Overview","text":"<p>The RAG pipeline in <code>RAGEngine</code> consists of four main stages, ensuring efficient processing and retrieval of information:</p> <ol> <li>Document Reading: Loads documents from specified file paths or directories using <code>LLamaIndexReader</code>. Files (e.g., PDF, text, Markdown) are processed with configurable options like recursive directory reading, file suffix filtering, and custom metadata extraction.</li> <li>Chunking: Splits documents into smaller chunks using a specified chunking strategy (e.g., simple, semantic, hierarchical). This ensures manageable text segments for embedding and retrieval.</li> <li>Vector Index Construction: Generates embeddings for chunks using an embedding model (e.g., OpenAI, ollama or Hugging Face) and builds indices (e.g., vector/graph index) for efficient storage and retrieval. Indices are stored in a backend like FAISS, Neo4j and SQLite.</li> <li>Retrieval: Processes queries using a retriever (e.g., vector or graph retriever), applies preprocessing (e.g., HyDE query transform), retrieves relevant chunks, and post-processes results (e.g., reranking or filtering by metadata).</li> </ol> <p> </p>"},{"location":"modules/rag.html#configuration-details","title":"Configuration Details","text":"<p>The <code>RAGEngine</code> relies on a <code>RAGConfig</code> object that encapsulates configurations for each stage of the RAG pipeline. Below are the detailed configurations and their parameters.</p>"},{"location":"modules/rag.html#readerconfig","title":"ReaderConfig","text":"<p>Controls the document reading stage.</p> Parameter Type Default Description <code>recursive</code> <code>bool</code> <code>False</code> Whether to recursively read directories. Set to <code>True</code> to include subdirectories. <code>exclude_hidden</code> <code>bool</code> <code>True</code> Excludes hidden files and directories (e.g., those starting with a dot). <code>num_files_limit</code> <code>Optional[int]</code> <code>None</code> Maximum number of files to read. Set to <code>None</code> for no limit. <code>custom_metadata_function</code> <code>Optional[Callable]</code> <code>None</code> Custom function to extract metadata from files, allowing user-defined metadata fields. <code>extern_file_extractor</code> <code>Optional[Dict[str, Any]]</code> <code>None</code> External file extractors for specific file types (e.g., PDF, Word). <code>errors</code> <code>str</code> <code>\"ignore\"</code> Error handling strategy: <code>\"ignore\"</code> skips invalid files, <code>\"strict\"</code> raises exceptions. <code>encoding</code> <code>str</code> <code>\"utf-8\"</code> File encoding for reading text files."},{"location":"modules/rag.html#chunkerconfig","title":"ChunkerConfig","text":"<p>Configures the document chunking stage.</p> Parameter Type Default Description <code>strategy</code> <code>str</code> <code>\"simple\"</code> Chunking strategy: <code>\"simple\"</code>, <code>\"semantic\"</code>, or <code>\"hierarchical\"</code>. Determines how documents are split. <code>chunk_size</code> <code>int</code> <code>1024</code> Maximum size of each chunk in characters. Smaller sizes improve granularity but increase processing time. <code>chunk_overlap</code> <code>int</code> <code>20</code> Number of overlapping characters between chunks to maintain context. <code>max_chunks</code> <code>Optional[int]</code> <code>None</code> Maximum number of chunks per document. Set to <code>None</code> for no limit."},{"location":"modules/rag.html#embeddingconfig","title":"EmbeddingConfig","text":"<p>Manages the embedding generation stage.</p> Parameter Type Default Description <code>provider</code> <code>str</code> <code>\"openai\"</code> Embedding provider: <code>\"openai\"</code>, <code>ollama</code>, <code>\"huggingface\"</code>. Determines the source of the embedding model. <code>model_name</code> <code>str</code> <code>\"text-embedding-ada-002\"</code> Name of the embedding model (e.g., <code>\"text-embedding-ada-002\"</code> for OpenAI). <code>api_key</code> <code>Optional[str]</code> <code>None</code> API key for the embedding provider, required for providers like OpenAI. <code>api_url</code> <code>str</code> <code>\"https://api.openai.com/v1\"</code> API URL for the embedding model, used for custom endpoints. <code>dimensions</code> <code>Optional[int]</code> <code>None</code> Dimensions of the embedding model. Must match the vector store configuration. <code>normalize</code> <code>Optional[bool]</code> <code>True</code> Whether to normalize embeddings (applies to Hugging Face models). <code>device</code> <code>Optional[str]</code> <code>None</code> Device for embedding computation (e.g., <code>\"cuda\"</code>, <code>\"cpu\"</code>) for Hugging Face models."},{"location":"modules/rag.html#indexconfig","title":"IndexConfig","text":"<p>Configures the indexing stage.</p> Parameter Type Default Description <code>index_type</code> <code>str</code> <code>\"vector\"</code> Index type: <code>\"vector\"</code>, <code>\"graph\"</code>, <code>\"summary\"</code>, or <code>\"tree\"</code>. Determines the structure for storing and retrieving data."},{"location":"modules/rag.html#retrievalconfig","title":"RetrievalConfig","text":"<p>Controls the retrieval stage, including preprocessing, retrieval, and post-processing.</p> Parameter Type Default Description <code>retrivel_type</code> <code>str</code> <code>\"vector\"</code> Retriever type: <code>\"vector\"</code> or <code>\"graph\"</code>. Specifies the retrieval mechanism. <code>postprocessor_type</code> <code>str</code> <code>\"simple\"</code> Postprocessor type for reranking or filtering results (e.g., <code>\"simple\"</code>). <code>top_k</code> <code>int</code> <code>5</code> Number of top results to retrieve in a query. <code>similarity_cutoff</code> <code>Optional[float]</code> <code>0.7</code> Minimum similarity score for retrieved chunks. Filters out low-relevance results. <code>keyword_filters</code> <code>Optional[List[str]]</code> <code>None</code> Keywords to filter retrieved chunks, ensuring relevance to specific terms. <code>metadata_filters</code> <code>Optional[Dict[str, Any]]</code> <code>None</code> Metadata filters to refine retrieval (e.g., <code>{\"file_name\": \"doc.txt\"}</code>)."},{"location":"modules/rag.html#usage-instructions","title":"Usage Instructions","text":""},{"location":"modules/rag.html#prerequisites","title":"Prerequisites","text":"<ul> <li>Install dependencies: <code>llama_index</code>, <code>pydantic</code>, and other required libraries.</li> <li>Configure environment variables (e.g., <code>OPENAI_API_KEY</code> for embedding models). See Installation Guide for EvoAgentX.</li> <li>Ensure a <code>StorageHandler</code> instance is configured with appropriate storage backends (e.g., SQLite, FAISS, Neo4j).</li> </ul>"},{"location":"modules/rag.html#initialization","title":"Initialization","text":"<p>Initialize <code>RAGEngine</code> with a <code>RAGConfig</code> and <code>StorageHandler</code>.</p> <pre><code>from evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# Configure storage\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# Configure RAG\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=0),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=\"your-api-key\"),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrivel_type=\"vector\", postprocessor_type=\"simple\", top_k=10, similarity_cutoff=0.3)\n)\n\n# Initialize RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n</code></pre>"},{"location":"modules/rag.html#core-functionality-usage","title":"Core Functionality Usage","text":"<ol> <li>Loading Documents:</li> <li>Use the <code>read</code> method to load and chunk documents into a corpus.</li> <li> <p>Example:      <pre><code>corpus = rag_engine.read(\n    file_paths=\"./data/docs\",\n    filter_file_by_suffix=[\".txt\", \".pdf\"],\n    merge_by_file=False,\n    show_progress=True,\n    corpus_id=\"doc_corpus\"\n)\n</code></pre></p> </li> <li> <p>Indexing Documents:</p> </li> <li>Use the <code>add</code> method to index a corpus or node list into a specified index type.</li> <li> <p>Example:      <pre><code>rag_engine.add(index_type=\"vector\", nodes=corpus, corpus_id=\"doc_corpus\")\n</code></pre></p> </li> <li> <p>Querying:</p> </li> <li>Use the <code>query</code> method to retrieve relevant chunks based on a query string or <code>Query</code> object.</li> <li> <p>Example:      <pre><code>from evoagentx.rag.schema import Query\nquery = Query(query_str=\"What is the capital of France?\", top_k=5)\nresult = rag_engine.query(query, corpus_id=\"doc_corpus\")\nprint(result.corpus.chunks)  # Retrieved chunks\n</code></pre></p> </li> <li> <p>Deleting Nodes or Indices:</p> </li> <li>Use the <code>delete</code> method to remove specific nodes or an entire index.</li> <li> <p>Example:      <pre><code>rag_engine.delete(corpus_id=\"doc_corpus\", node_ids=[\"node_1\", \"node_2\"])\nrag_engine.delete(corpus_id=\"doc_corpus\", index_type=\"vector\")  # Delete entire index\n</code></pre></p> </li> <li> <p>Clearing Indices:</p> </li> <li>Use the <code>clear</code> method to remove indices for a specific corpus or all corpora.</li> <li> <p>Example:      <pre><code>rag_engine.clear(corpus_id=\"doc_corpus\")  # Clear specific corpus\nrag_engine.clear()  # Clear all corpora\n</code></pre></p> </li> <li> <p>Saving Indices:</p> </li> <li>Use the <code>save</code> method to persist indices to files or a database.</li> <li> <p>Example:      <pre><code>rag_engine.save(output_path=\"./data/indexing\", corpus_id=\"doc_corpus\", index_type=\"vector\")\nrag_engine.save(corpus_id=\"doc_corpus\", table=\"indexing\")  # Save to database\n</code></pre></p> </li> <li> <p>Loading Indices:</p> </li> <li>Use the <code>load</code> method to reconstruct indices from files or a database.</li> <li>Example:      <pre><code>rag_engine.load(source=\"./data/indexing\", corpus_id=\"doc_corpus\", index_type=\"vector\")\nrag_engine.load(corpus_id=\"doc_corpus\", table=\"indexing\")  # Load from database\n</code></pre></li> </ol>"},{"location":"modules/rag.html#usage-example","title":"Usage Example","text":"<p>The following example demonstrates using <code>RAGEngine</code> to process a HotPotQA dataset, index documents, query, and evaluate retrieval performance. examples/rag_engine.py</p> <pre><code>import os\nimport json\nimport logging\nfrom typing import List, Dict\nfrom collections import defaultdict\nfrom dotenv import load_dotenv\n\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.core.logging import logger\nfrom evoagentx.storages.storages_config import VectorStoreConfig, DBConfig, StoreConfig\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, IndexConfig, EmbeddingConfig, RetrievalConfig\nfrom evoagentx.rag.schema import Query, Corpus, Chunk, ChunkMetadata\nfrom evoagentx.benchmark.hotpotqa import HotPotQA, download_raw_hotpotqa_data\n\n# Load environment\nload_dotenv()\n\n# Download datasets\ndownload_raw_hotpotqa_data(\"hotpot_dev_distractor_v1.json\", \"./debug/data/hotpotqa\")\ndatasets = HotPotQA(\"./debug/data/hotpotqa\")\n\n# Initialize StorageHandler\nstore_config = StoreConfig(\n    dbConfig=DBConfig(\n        db_name=\"sqlite\",\n        path=\"./debug/data/hotpotqa/cache/test_hotpotQA.sql\"\n    ),\n    vectorConfig=VectorStoreConfig(\n        vector_name=\"faiss\",\n        dimensions=1536,\n        index_type=\"flat_l2\",\n    ),\n    graphConfig=None,\n    path=\"./debug/data/hotpotqa/cache/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# Initialize RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(\n        recursive=False, exclude_hidden=True,\n        num_files_limit=None, custom_metadata_function=None,\n        extern_file_extractor=None,\n        errors=\"ignore\", encoding=\"utf-8\"\n    ),\n    chunker=ChunkerConfig(\n        strategy=\"simple\",\n        chunk_size=512,\n        chunk_overlap=0,\n        max_chunks=None\n    ),\n    embedding=EmbeddingConfig(\n        provider=\"openai\",\n        model_name=\"text-embedding-ada-002\",\n        api_key=os.environ[\"OPENAI_API_KEY\"],\n    ),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(\n        retrivel_type=\"vector\",\n        postprocessor_type=\"simple\",\n        top_k=10,  # Retrieve top-10 contexts\n        similarity_cutoff=0.3,\n        keyword_filters=None,\n        metadata_filters=None\n    )\n)\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\ndef create_corpus_from_context(context: List[List], corpus_id: str) -&gt; Corpus:\n    \"\"\"Convert HotPotQA context into a Corpus for indexing.\"\"\"\n    chunks = []\n    for title, sentences in context:\n        for idx, sentence in enumerate(sentences):\n            chunk = Chunk(\n                chunk_id=f\"{title}_{idx}\",\n                text=sentence,\n                metadata=ChunkMetadata(\n                    doc_id=str(idx),\n                    corpus_id=corpus_id\n                ),\n                start_char_idx=0,\n                end_char_idx=len(sentence),\n                excluded_embed_metadata_keys=[],\n                excluded_llm_metadata_keys=[],\n                relationships={}\n            )\n            chunk.metadata.title = title\n            chunks.append(chunk)\n    return Corpus(chunks=chunks, corpus_id=corpus_id)\n\ndef evaluate_retrieval(retrieved_chunks: List[Chunk], supporting_facts: List[List], top_k: int) -&gt; Dict[str, float]:\n    \"\"\"Evaluate retrieved chunks against supporting facts.\"\"\"\n    relevant = {(fact[0], fact[1]) for fact in supporting_facts}\n    retrieved = []\n    for chunk in retrieved_chunks[:top_k]:\n        title = chunk.metadata.title\n        sentence_idx = int(chunk.metadata.doc_id)\n        retrieved.append((title, sentence_idx))\n    hits = sum(1 for r in retrieved if r in relevant)\n    precision = hits / top_k if top_k &gt; 0 else 0.0\n    recall = hits / len(relevant) if len(relevant) &gt; 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0.0\n    mrr = 0.0\n    for rank, r in enumerate(retrieved, 1):\n        if r in relevant:\n            mrr = 1.0 / rank\n            break\n    hit = 1.0 if hits &gt; 0 else 0.0\n    intersection = set((r[0], r[1]) for r in retrieved) &amp; relevant\n    union = set((r[0], r[1]) for r in retrieved) | relevant\n    jaccard = len(intersection) / len(union) if union else 0.0\n    return {\n        \"precision@k\": precision,\n        \"recall@k\": recall,\n        \"f1@k\": f1,\n        \"mrr\": mrr,\n        \"hit@k\": hit,\n        \"jaccard\": jaccard\n    }\n\ndef run_evaluation(samples: List[Dict], top_k: int = 5) -&gt; Dict[str, float]:\n    \"\"\"Run evaluation on HotPotQA samples.\"\"\"\n    metrics = defaultdict(list)\n    for sample in samples:\n        question = sample[\"question\"]\n        context = sample[\"context\"]\n        supporting_facts = sample[\"supporting_facts\"]\n        corpus_id = sample[\"_id\"]\n        logger.info(f\"Processing sample: {corpus_id}, question: {question}\")\n        corpus = create_corpus_from_context(context, corpus_id)\n        logger.info(f\"Created corpus with {len(corpus.chunks)} chunks\")\n        rag_engine.add(index_type=\"vector\", nodes=corpus, corpus_id=corpus_id)\n        query = Query(query_str=question, top_k=top_k)\n        result = rag_engine.query(query, corpus_id=corpus_id)\n        retrieved_chunks = result.corpus.chunks\n        logger.info(f\"Retrieved {len(retrieved_chunks)} chunks for query\")\n        sample_metrics = evaluate_retrieval(retrieved_chunks, supporting_facts, top_k)\n        for metric_name, value in sample_metrics.items():\n            metrics[metric_name].append(value)\n        logger.info(f\"Metrics for sample {corpus_id}: {sample_metrics}\")\n        rag_engine.clear(corpus_id=corpus_id)\n    avg_metrics = {name: sum(values) / len(values) for name, values in metrics.items()}\n    return avg_metrics\n\nif __name__ == \"__main__\":\n    samples = datasets._dev_data[:20]\n    print(len(datasets._dev_data))\n    avg_metrics = run_evaluation(samples, top_k=5)\n    logger.info(\"Average Metrics:\")\n    for metric_name, value in avg_metrics.items():\n        logger.info(f\"{metric_name}: {value:.4f}\")\n    with open(\"./debug/data/hotpotqa/evaluation_results.json\", \"w\") as f:\n        json.dump(avg_metrics, f, indent=2)\n</code></pre>"},{"location":"modules/rag.html#interface-list","title":"Interface List","text":"Method Description Parameters Return Value <code>__init__</code> Initializes the RAGEngine with RAG and storage configurations. <code>config: RAGConfig</code>, <code>storage_handler: StorageHandler</code> None <code>read</code> Loads and chunks documents into a corpus. <code>file_paths</code>, <code>exclude_files</code>, <code>filter_file_by_suffix</code>, <code>merge_by_file</code>, <code>show_progress</code>, <code>corpus_id</code> <code>Corpus</code> <code>add</code> Adds nodes to an index for a specific corpus. <code>index_type</code>, <code>nodes</code>, <code>corpus_id</code> None <code>delete</code> Deletes nodes or an entire index from a corpus. <code>corpus_id</code>, <code>index_type</code>, <code>node_ids</code>, <code>metadata_filters</code> None <code>clear</code> Clears indices for a specific corpus or all corpora. <code>corpus_id</code> None <code>save</code> Saves indices to files or a database. <code>output_path</code>, <code>corpus_id</code>, <code>index_type</code>, <code>table</code> None <code>load</code> Loads indices from files or a database. <code>source</code>, <code>corpus_id</code>, <code>index_type</code>, <code>table</code> None <code>query</code> Executes a query and returns processed results. <code>query</code>, <code>corpus_id</code>, <code>query_transforms</code> <code>RagResult</code>"},{"location":"modules/rag.html#notes","title":"Notes","text":"<ul> <li>Query Transforms: The <code>query</code> method supports optional query transforms for preprocessing, which can be customized to enhance retrieval (e.g., HyDE, query decomposition).</li> <li>Storage Backend: Ensure <code>StorageHandler</code> is properly configured to handle vector and metadata storage.</li> <li>Warning: Loading indices multiple times may cause issues (e.g., duplicate node insertion in vector stores). Clear indices before reloading.</li> </ul>"},{"location":"modules/storages.html","title":"StorageHandler Documentation","text":""},{"location":"modules/storages.html#overview","title":"Overview","text":"<p>The <code>StorageHandler</code> class is a critical component designed to manage multiple storage backends for storing and retrieving various types of data, such as agent configurations, workflows, memory entries, and index data. It provides a unified interface to interact with different storage systems, including relational databases (e.g., SQLite), vector databases (e.g., FAISS), and graph databases (e.g., Neo4j). The class leverages the Pydantic library for configuration validation and uses factory patterns to initialize storage backends.</p> <p>The <code>StorageHandler</code> is tightly integrated with the <code>RAGEngine</code> class to support retrieval-augmented generation (RAG) functionality by managing the storage of indexed documents, embeddings, and associated metadata. It abstracts the complexity of interacting with different storage systems, ensuring seamless data operations for applications like long-term memory management and RAG pipelines.</p>"},{"location":"modules/storages.html#class-structure","title":"Class Structure","text":"<p>The <code>StorageHandler</code> class inherits from <code>BaseModule</code> and uses Pydantic's <code>Field</code> for configuration and type validation. It supports three types of storage backends: - Database Storage (<code>storageDB</code>): Manages relational database operations, such as SQLite, for structured data storage. - Vector Storage (<code>vector_store</code>): Handles vector embeddings for semantic search, supporting providers like FAISS. - Graph Storage (<code>graph_store</code>): Manages graph-based data, such as Neo4j, for relational or networked data structures.</p>"},{"location":"modules/storages.html#key-attributes","title":"Key Attributes","text":"<ul> <li><code>storageConfig: StoreConfig</code>: Configuration object for all storage backends, defined in <code>storages_config.py</code>. It includes settings for database, vector, and graph stores.</li> <li><code>storageDB: Optional[Union[DBStoreBase, Any]]</code>: Instance of the database storage backend, initialized via <code>DBStoreFactory</code>.</li> <li><code>vector_store: Optional[Union[VectorStoreBase, Any]]</code>: Instance of the vector storage backend, initialized via <code>VectorStoreFactory</code>.</li> <li><code>graph_store: Optional[Union[GraphStoreBase, Any]]</code>: Instance of the graph storage backend, initialized via <code>GraphStoreFactory</code>.</li> </ul>"},{"location":"modules/storages.html#dependencies","title":"Dependencies","text":"<ul> <li>Pydantic: For configuration validation and type checking.</li> <li>Factory Patterns: <code>DBStoreFactory</code>, <code>VectorStoreFactory</code>, and <code>GraphStoreFactory</code> for creating storage backend instances.</li> <li>Configuration: <code>StoreConfig</code>, <code>DBConfig</code>, <code>VectorStoreConfig</code>, and <code>GraphStoreConfig</code> from <code>storages_config.py</code> for defining storage settings.</li> <li>Schema: <code>TableType</code>, <code>AgentStore</code>, <code>WorkflowStore</code>, <code>MemoryStore</code>, <code>HistoryStore</code>, and <code>IndexStore</code> for data validation and structure.</li> </ul>"},{"location":"modules/storages.html#key-methods","title":"Key Methods","text":""},{"location":"modules/storages.html#initialization","title":"Initialization","text":"<ul> <li><code>init_module(self)</code></li> <li>Initializes all storage backends based on the provided <code>storageConfig</code>.</li> <li> <p>Creates the storage directory if specified and initializes database, vector, and graph stores by calling their respective initialization methods.</p> </li> <li> <p><code>_init_db_store(self)</code></p> </li> <li>Initializes the database storage backend using <code>DBStoreFactory</code> with the <code>dbConfig</code> from <code>storageConfig</code>.</li> <li> <p>Sets the <code>storageDB</code> attribute.</p> </li> <li> <p><code>_init_vector_store(self)</code></p> </li> <li>Initializes the vector storage backend using <code>VectorStoreFactory</code> if <code>vectorConfig</code> is provided.</li> <li> <p>Sets the <code>vector_store</code> attribute.</p> </li> <li> <p><code>_init_graph_store(self)</code></p> </li> <li>Initializes the graph storage backend using <code>GraphStoreFactory</code> if <code>graphConfig</code> is provided.</li> <li>Sets the <code>graph_store</code> attribute.</li> </ul>"},{"location":"modules/storages.html#data-operations","title":"Data Operations","text":"<ul> <li><code>load(self, tables: Optional[List[str]] = None, *args, **kwargs) -&gt; Dict[str, Any]</code></li> <li>Loads data from the database storage for specified tables or all tables defined in <code>TableType</code>.</li> <li>Returns a dictionary with table names as keys and lists of records as values.</li> <li> <p>Each record is a dictionary mapping column names to values, requiring manual parsing for JSON fields.</p> </li> <li> <p><code>save(self, data: Dict[str, Any], *args, **kwargs)</code></p> </li> <li>Saves data to the database storage.</li> <li>Takes a dictionary with table names as keys and lists of records to save.</li> <li> <p>Validates table names against <code>TableType</code> and inserts records using <code>storageDB.insert</code>.</p> </li> <li> <p><code>parse_result(self, results: Dict[str, str], store: Union[AgentStore, WorkflowStore, MemoryStore, HistoryStore]) -&gt; Dict[str, Any]</code></p> </li> <li>Parses raw database results, deserializing JSON strings into Python objects based on the provided Pydantic model (<code>store</code>).</li> <li>Returns a dictionary with parsed results, handling non-string fields appropriately.</li> </ul>"},{"location":"modules/storages.html#entity-specific-operations","title":"Entity-Specific Operations","text":"<ul> <li><code>load_memory(self, memory_id: str, table: Optional[str]=None, **kwargs) -&gt; Dict[str, Any]</code></li> <li>Placeholder method for loading a single long-term memory entry by <code>memory_id</code>.</li> <li> <p>Defaults to the <code>memory</code> table if no table is specified.</p> </li> <li> <p><code>save_memory(self, memory_data: Dict[str, Any], table: Optional[str]=None, **kwargs)</code></p> </li> <li>Placeholder method for saving or updating a single memory entry.</li> <li> <p>Defaults to the <code>memory</code> table if no table is specified.</p> </li> <li> <p><code>load_agent(self, agent_name: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]</code></p> </li> <li>Loads a single agent's data by <code>agent_name</code> from the database.</li> <li>Defaults to the <code>agent</code> table if no table is specified.</li> <li>Parses the result using <code>parse_result</code> with <code>AgentStore</code> for validation.</li> <li> <p>Returns <code>None</code> if the agent is not found.</p> </li> <li> <p><code>remove_agent(self, agent_name: str, table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>Deletes an agent by <code>agent_name</code> from the specified table (defaults to <code>agent</code>).</li> <li> <p>Raises a <code>ValueError</code> if the agent does not exist.</p> </li> <li> <p><code>save_agent(self, agent_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>Saves or updates an agent's data in the database.</li> <li>Requires <code>agent_data</code> to include a <code>name</code> field.</li> <li> <p>Updates existing records or inserts new ones using <code>storageDB.update</code> or <code>storageDB.insert</code>.</p> </li> <li> <p><code>load_workflow(self, workflow_id: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]</code></p> </li> <li>Loads a single workflow's data by <code>workflow_id</code> from the database.</li> <li>Defaults to the <code>workflow</code> table if no table is specified.</li> <li>Parses the result using <code>parse_result</code> with <code>WorkflowStore</code> for validation.</li> <li> <p>Returns <code>None</code> if the workflow is not found.</p> </li> <li> <p><code>save_workflow(self, workflow_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>Saves or updates a workflow's data in the database.</li> <li>Requires <code>workflow_data</code> to include a <code>name</code> field.</li> <li> <p>Updates existing records or inserts new ones using <code>storageDB.update</code> or <code>storageDB.insert</code>.</p> </li> <li> <p><code>load_history(self, memory_id: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]</code></p> </li> <li>Loads a single history entry by <code>memory_id</code> from the database.</li> <li>Defaults to the <code>history</code> table if no table is specified.</li> <li>Parses the result using <code>parse_result</code> with <code>HistoryStore</code> for validation.</li> <li> <p>Returns <code>None</code> if the history entry is not found.</p> </li> <li> <p><code>save_history(self, history_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>Saves or updates a single history entry in the database.</li> <li>Requires <code>history_data</code> to include a <code>memory_id</code> field.</li> <li> <p>Updates existing records with <code>old_memory</code> preserved or inserts new ones.</p> </li> <li> <p><code>load_index(self, corpus_id: str, table: Optional[str]=None) -&gt; Optional[Dict[str, Any]]</code></p> </li> <li>Loads index data by <code>corpus_id</code> from the database.</li> <li>Parses the result using <code>parse_result</code> with <code>IndexStore</code> for validation.</li> <li> <p>Returns <code>None</code> if the index is not found.</p> </li> <li> <p><code>save_index(self, index_data: Dict[str, Any], table: Optional[str]=None)</code></p> </li> <li>Saves or updates index data in the database.</li> <li>Requires <code>index_data</code> to include a <code>corpus_id</code> field.</li> <li>Updates existing records or inserts new ones using <code>storageDB.update</code> or <code>storageDB.insert</code>.</li> </ul>"},{"location":"modules/storages.html#integration-with-ragengine","title":"Integration with RAGEngine","text":"<p>The <code>StorageHandler</code> is tightly integrated with the <code>RAGEngine</code> class to support RAG functionality. It is used to: - Initialize Vector Storage: The <code>RAGEngine</code> constructor checks the vector store's dimensions against the embedding model's dimensions and reinitializes the vector store if necessary. - Save Indices: The <code>save</code> method in <code>RAGEngine</code> uses <code>StorageHandler.save_index</code> to persist index data (e.g., corpus chunks and metadata) to the database when no file output path is specified. - Load Indices: The <code>load</code> method in <code>RAGEngine</code> uses <code>StorageHandler.load</code> and <code>StorageHandler.parse_result</code> to reconstruct indices from database records, ensuring compatibility with embedding models and dimensions.</p>"},{"location":"modules/storages.html#configuration","title":"Configuration","text":"<p>The <code>StorageHandler</code> relies on the <code>StoreConfig</code> class (defined in <code>storages_config.py</code>) to configure its backends: - <code>DBConfig</code>: Configures relational databases (e.g., SQLite) with settings like <code>db_name</code>, <code>path</code>, <code>ip</code>, and <code>port</code>. - <code>VectorStoreConfig</code>: Configures vector databases (e.g., FAISS, Qdrant) with settings like <code>vector_name</code>, <code>dimensions</code>, <code>index_type</code>, <code>qdrant_url</code>, and <code>qdrant_collection_name</code>. - <code>GraphStoreConfig</code>: Configures graph databases (e.g., Neo4j) with settings\u4e0a\u5348 like <code>graph_name</code>, <code>uri</code>, <code>username</code>, <code>password</code>, and <code>database</code>.</p> <p>The configuration is validated using Pydantic, ensuring robust type checking and default values.</p>"},{"location":"modules/storages.html#usage-example","title":"Usage Example","text":"<p>Below is an example of how to initialize and use <code>StorageHandler</code>:</p> <pre><code>from evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, DBConfig, VectorStoreConfig\n\n# Define configuration\nconfig = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"data/storage.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536),\n    path=\"data/index_cache\"\n)\n\n# Initialize StorageHandler\nstorage_handler = StorageHandler(storageConfig=config)\nstorage_handler.init_module()\n\n# Save agent data\nagent_data = {\"name\": \"agent1\", \"content\": {\"role\": \"analyst\", \"tasks\": [\"data analysis\"]}}\nstorage_handler.save_agent(agent_data)\n\n# Load agent data\nagent = storage_handler.load_agent(\"agent1\")\nprint(agent)  # {'name': 'agent1', 'content': {'role': 'analyst', 'tasks': ['data analysis']}}\n\n# Save index data (used in RAGEngine)\nindex_data = {\n    \"corpus_id\": \"corpus1\",\n    \"content\": {\"chunks\": [{\"chunk_id\": \"c1\", \"text\": \"Sample text\", \"metadata\": {}}]},\n    \"metadata\": {\"index_type\": \"VECTOR\", \"dimension\": 1536}\n}\nstorage_handler.save_index(index_data)\n\n# Load index data\nindex = storage_handler.load_index(\"corpus1\")\nprint(index)  # {'corpus_id': 'corpus1', 'content': {...}, 'metadata': {...}}\n</code></pre>"},{"location":"modules/storages.html#notes","title":"Notes","text":"<ul> <li>The <code>load_memory</code> and <code>save_memory</code> methods are not yet fully implemented and will be developed alongside <code>LongTermMemory</code>.</li> <li>The <code>StorageHandler</code> assumes the database schema is managed by <code>DBStoreBase</code> and its factory, ensuring compatibility with <code>TableType</code> enums.</li> <li>When used with <code>RAGEngine</code>, ensure the vector store's dimensions match the embedding model's dimensions to avoid reinitialization issues.</li> <li>Error handling is implemented throughout, with logs generated via the <code>evoagentx.core.logging.logger</code> module.</li> </ul>"},{"location":"modules/storages.html#conclusion","title":"Conclusion","text":"<p>The <code>StorageHandler</code> class provides a flexible and extensible interface for managing multiple storage backends in a unified manner. Its integration with <code>RAGEngine</code> makes it a key component for RAG pipelines, enabling efficient storage and retrieval of indexed data. By leveraging factory patterns and Pydantic validation, it ensures robustness and scalability for applications requiring complex data management.</p>"},{"location":"modules/workflow_graph.html","title":"Workflow Graph","text":""},{"location":"modules/workflow_graph.html#introduction","title":"Introduction","text":"<p>The <code>WorkFlowGraph</code> class is a fundamental component in the EvoAgentX framework for creating, managing, and executing complex AI agent workflows. It provides a structured way to define task dependencies, execution order, and the flow of data between tasks.</p> <p>A workflow graph represents a collection of tasks (nodes) and their dependencies (edges) that need to be executed in a specific order to achieve a goal. The <code>SequentialWorkFlowGraph</code> is a specialized implementation that focuses on linear workflows with a single path from start to end.</p>"},{"location":"modules/workflow_graph.html#architecture","title":"Architecture","text":""},{"location":"modules/workflow_graph.html#workflowgraph-architecture","title":"WorkFlowGraph Architecture","text":"<p>A <code>WorkFlowGraph</code> consists of several key components:</p> <ol> <li> <p>Nodes (WorkFlowNode): </p> <p>Each node represents a task or operation in the workflow, with the following properties:</p> <ul> <li><code>name</code>: A unique identifier for the task</li> <li><code>description</code>: Detailed description of what the task does</li> <li><code>inputs</code>: List of input parameters required by the task, each input parameter is an instance of <code>Parameter</code> class. </li> <li><code>outputs</code>: List of output parameters produced by the task, each output parameter is an instance of <code>Parameter</code> class. </li> <li><code>agents</code> (optional): List of agents that can execute this task, each agent should be a string that matches the name of the agent in the <code>agent_manager</code> or a dictionary that specifies the agent name and its configuration, which will be used to create a <code>CustomizeAgent</code> instance in the <code>agent_manager</code>.  Please refer to the Customize Agent documentation for more details about the agent configuration. </li> <li><code>action_graph</code> (optional): An instance of <code>ActionGraph</code> class, where each action is an instance of the <code>Operator</code> class. Please refer to the Action Graph documentation for more details about the action graph. </li> <li><code>status</code>: Current execution state of the task (PENDING, RUNNING, COMPLETED, FAILED).</li> </ul> <p>Note</p> <ol> <li> <p>You should provide either <code>agents</code> or <code>action_graph</code> to execute the task. If both are provided, <code>action_graph</code> will be used. </p> </li> <li> <p>If you provide a set of <code>agents</code>, these agents will work together to complete the task. When executing the task using <code>WorkFlow</code>, the system will automatically determine the execution sequence (actions) based on the agent information and execution history. Specifically, when executing the task, <code>WorkFlow</code> will analyze all the possible actions within these agents and repeatly select the best action to execute based on the task description and execution history. </p> </li> <li> <p>If you provide an <code>action_graph</code>, it will be directly used to complete the task. When executing the task with <code>WorkFlow</code>, the system will execute the actions in the order defined by the <code>action_graph</code> and return the results.  </p> </li> </ol> </li> <li> <p>Edges (WorkFlowEdge): </p> <p>Edges represent dependencies between tasks, defining execution order and data flow. Each edge has:</p> <ul> <li><code>source</code>: Name of the source node (where the edge starts)</li> <li><code>target</code>: Name of the target node (where the edge ends) </li> <li><code>priority</code> (optional): numeric priority to influence execution order</li> </ul> </li> <li> <p>Graph Structure:</p> <p>Internally, the workflow is represented as a directed graph where:</p> <ul> <li>Nodes represent tasks</li> <li>Edges represent dependencies and data flow between tasks</li> <li>The graph structure supports both linear sequences and more complex patterns:<ul> <li>Fork-join patterns (parallel execution paths that rejoin later)</li> <li>Conditional branches</li> <li>Potential cycles (loops) in the workflow</li> </ul> </li> </ul> </li> <li> <p>Node States:</p> <p>Each node in the workflow can be in one of the following states:</p> <ul> <li><code>PENDING</code>: The task is waiting to be executed</li> <li><code>RUNNING</code>: The task is currently being executed</li> <li><code>COMPLETED</code>: The task has been successfully executed</li> <li><code>FAILED</code>: The task execution has failed</li> </ul> </li> </ol>"},{"location":"modules/workflow_graph.html#sequentialworkflowgraph-architecture","title":"SequentialWorkFlowGraph Architecture","text":"<p>The <code>SequentialWorkFlowGraph</code> is a specialized implementation of <code>WorkFlowGraph</code> that automatically infers node connections to create a linear workflow. It's designed for simpler use cases where tasks need to be executed in sequence, with outputs from one task feeding into the next.</p>"},{"location":"modules/workflow_graph.html#input-format","title":"Input Format","text":"<p>The <code>SequentialWorkFlowGraph</code> accepts a simplified input format that makes it easy to define linear workflows. Instead of explicitly defining nodes and edges, you provide a list of tasks in the order they should be executed. Each task is defined as a dictionary with the following fields:</p> <ul> <li><code>name</code> (required): A unique identifier for the task</li> <li><code>description</code> (required): Detailed description of what the task does</li> <li><code>inputs</code> (required): List of input parameters for the task</li> <li><code>outputs</code> (required): List of output parameters produced by the task</li> <li><code>prompt</code> (required): The prompt template to guide the agent's behavior</li> <li><code>system_prompt</code> (optional): System message to provide context to the agent</li> <li><code>output_parser</code> (optional): The output parser to parse the output of the task </li> <li><code>parse_mode</code> (optional): Mode for parsing outputs, defaults to \"str\"</li> <li><code>parse_func</code> (optional): Custom function for parsing outputs</li> <li><code>parse_title</code> (optional): Title for the parsed output</li> </ul> <p>The parameters related to prompts and parsing will be used to create a <code>CustomizeAgent</code> instance in the <code>agent_manager</code>. Please refer to the Customize Agent documentation for more details about the agent configuration. </p>"},{"location":"modules/workflow_graph.html#internal-conversion-to-workflowgraph","title":"Internal Conversion to WorkFlowGraph","text":"<p>Internally, <code>SequentialWorkFlowGraph</code> automatically converts this simplified task list into a complete <code>WorkFlowGraph</code> by:</p> <ol> <li> <p>Creating WorkFlowNode instances: For each task in the input list, it creates a <code>WorkFlowNode</code> with appropriate properties. During this process:</p> <ul> <li>It converts the task definition into a node with inputs, outputs, and an associated agent.</li> <li>It automatically generates a unique agent name based on the task name.</li> <li>It configures the agent with the provided prompt, system_prompt, and parsing options.</li> </ul> </li> <li> <p>Inferring edge connections: It examines the input and output parameters of each task and automatically creates <code>WorkFlowEdge</code> instances to connect tasks where outputs from one task match the inputs of another.</p> </li> <li> <p>Building the graph structure: Finally, it constructs the complete directed graph representing the workflow, with all nodes and edges properly connected.</p> </li> </ol> <p>This automatic conversion process makes it significantly easier to define sequential workflows without needing to manually specify all the graph components.</p>"},{"location":"modules/workflow_graph.html#usage","title":"Usage","text":""},{"location":"modules/workflow_graph.html#basic-workflowgraph-creation--execution","title":"Basic WorkFlowGraph Creation &amp; Execution","text":"<pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph, WorkFlowEdge\nfrom evoagentx.workflow.workflow import WorkFlow \nfrom evoagentx.agents import AgentManager, CustomizeAgent \nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM \n\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\nagent_manager = AgentManager()\n\ndata_extraction_agent = CustomizeAgent(\n    name=\"DataExtractionAgent\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    prompt=\"Extract data from source: {data_source}\",\n    llm_config=llm_config\n)  \n\ndata_transformation_agent = CustomizeAgent(\n    name=\"DataTransformationAgent\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    prompt=\"Transform data: {extracted_data}\",\n    llm_config=llm_config\n)\n\n# add agents to the agent manager for workflow execution \ndata_extraction_agent = agent_manager.add_agents(agents = [data_extraction_agent, data_transformation_agent])\n\n# Create workflow nodes\ntask1 = WorkFlowNode(\n    name=\"Task1\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    agents=[\"DataExtractionAgent\"] # should match the name of the agent in the agent manager\n)\n\ntask2 = WorkFlowNode(\n    name=\"Task2\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Data to transform\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    agents=[\"DataTransformationAgent\"] # should match the name of the agent in the agent manager\n)\n\ntask3 = WorkFlowNode(\n    name=\"Task3\",\n    description=\"Analyze data and generate insights\",\n    inputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n    outputs=[{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n    agents=[\n        {\n            \"name\": \"DataAnalysisAgent\",\n            \"description\": \"Analyze data and generate insights\",\n            \"inputs\": [{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n            \"outputs\": [{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n            \"prompt\": \"Analyze data and generate insights: {transformed_data}\",\n            \"parse_mode\": \"str\",\n        } # will be used to create a `CustomizeAgent` instance in the `agent_manager`\n    ]\n)\n\n# Create workflow edges\nedge1 = WorkFlowEdge(source=\"Task1\", target=\"Task2\")\nedge2 = WorkFlowEdge(source=\"Task2\", target=\"Task3\")\n\n# Create the workflow graph\nworkflow_graph = WorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    nodes=[task1, task2, task3],\n    edges=[edge1, edge2]\n)\n\n# add agents to the agent manager for workflow execution \nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# create a workflow instance for execution \nworkflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\nworkflow.execute(inputs={\"data_source\": \"xxx\"})\n</code></pre>"},{"location":"modules/workflow_graph.html#creating-a-sequentialworkflowgraph","title":"Creating a SequentialWorkFlowGraph","text":"<pre><code>from evoagentx.workflow.workflow_graph import SequentialWorkFlowGraph\n\n# Define tasks with their inputs, outputs, and prompts\ntasks = [\n    {\n        \"name\": \"DataExtraction\",\n        \"description\": \"Extract data from the specified source\",\n        \"inputs\": [\n            {\"name\": \"data_source\", \"type\": \"string\", \"required\": True, \"description\": \"Source data location\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Extracted data\"}\n        ],\n        \"prompt\": \"Extract data from the following source: {data_source}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataTransformation\",\n        \"description\": \"Transform the extracted data\",\n        \"inputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to transform\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Transformed data\"}\n        ],\n        \"prompt\": \"Transform the following data: {extracted_data}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataAnalysis\",\n        \"description\": \"Analyze data and generate insights\",\n        \"inputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to analyze\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"insights\", \"type\": \"string\", \"required\": True, \"description\": \"Generated insights\"}\n        ],\n        \"prompt\": \"Analyze the following data and generate insights: {transformed_data}\", \n        \"parse_mode\": \"str\"\n    }\n]\n\n# Create the sequential workflow graph\nsequential_workflow_graph = SequentialWorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    tasks=tasks\n)\n</code></pre>"},{"location":"modules/workflow_graph.html#saving-and-loading-a-workflow","title":"Saving and Loading a Workflow","text":"<pre><code># Save workflow\nworkflow_graph.save_module(\"examples/output/my_workflow.json\")\n\n# For SequentialWorkFlowGraph, use save_module and get_graph_info\nsequential_workflow_graph.save_module(\"examples/output/my_sequential_workflow.json\")\n</code></pre>"},{"location":"modules/workflow_graph.html#visualizing-the-workflow","title":"Visualizing the Workflow","text":"<pre><code># Display the workflow graph with node statuses visually\nworkflow_graph.display()\n</code></pre> <p>The <code>WorkFlowGraph</code> and <code>SequentialWorkFlowGraph</code> classes provide a flexible and powerful way to design complex agent workflows, track their execution, and manage the flow of data between tasks. </p>"},{"location":"tutorial/aflow_optimizer.html","title":"AFlow Optimizer Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running the AFlow optimizer in EvoAgentX. We'll use the HumanEval benchmark as an example to demonstrate how to optimize a multi-agent workflow for code generation tasks.</p>"},{"location":"tutorial/aflow_optimizer.html#1-overview","title":"1. Overview","text":"<p>The AFlow optimizer in EvoAgentX enables you to:</p> <ul> <li>Automatically optimize multi-agent workflows for specific task types (code generation, QA, math, etc.)</li> <li>Support different types of operators (Custom, CustomCodeGenerate, Test, ScEnsemble, etc.)</li> <li>Evaluate optimization results on benchmark datasets</li> <li>Use different LLMs for optimization and execution</li> </ul>"},{"location":"tutorial/aflow_optimizer.html#2-setting-up-the-environment","title":"2. Setting Up the Environment","text":"<p>First, let's import the necessary modules for setting up the AFlow optimizer:</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom evoagentx.optimizers import AFlowOptimizer\nfrom evoagentx.models import LiteLLMConfig, LiteLLM, OpenAILLMConfig, OpenAILLM\nfrom evoagentx.benchmark import AFlowHumanEval\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nANTHROPIC_API_KEY = os.getenv(\"ANTHROPIC_API_KEY\")\n</code></pre>"},{"location":"tutorial/aflow_optimizer.html#configure-the-llm-models","title":"Configure the LLM Models","text":"<p>Following the settings in the original AFlow implementation, the AFlow optimizer uses two different LLMs: 1. An optimizer LLM (e.g., Claude 3.5 Sonnet) for workflow optimization 2. An executor LLM (e.g., GPT-4o-mini) for task execution</p> <pre><code># Configure the optimizer LLM (Claude 3.5 Sonnet)\nclaude_config = LiteLLMConfig(\n    model=\"anthropic/claude-3-5-sonnet-20240620\", \n    anthropic_key=ANTHROPIC_API_KEY\n)\noptimizer_llm = LiteLLM(config=claude_config)\n\n# Configure the executor LLM (GPT-4o-mini)\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\", \n    openai_key=OPENAI_API_KEY\n)\nexecutor_llm = OpenAILLM(config=openai_config)\n</code></pre>"},{"location":"tutorial/aflow_optimizer.html#3-setting-up-the-components","title":"3. Setting Up the Components","text":""},{"location":"tutorial/aflow_optimizer.html#step-1-define-task-configuration","title":"Step 1: Define Task Configuration","text":"<p>The AFlow optimizer requires a configuration that specifies the task type and available operators. Here's an example configuration for different task types:</p> <pre><code>EXPERIMENTAL_CONFIG = {\n    \"humaneval\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    }, \n    \"mbpp\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    },\n    \"hotpotqa\": {\n        \"question_type\": \"qa\", \n        \"operators\": [\"Custom\", \"AnswerGenerate\", \"QAScEnsemble\"]\n    },\n    \"gsm8k\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    },\n    \"math\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    }\n}\n</code></pre>"},{"location":"tutorial/aflow_optimizer.html#step-2-define-initial-workflow","title":"Step 2: Define initial workflow","text":"<p>The AFlow optimizer requires two files:  - <code>graph.py</code>: which defines the initial workflow graph in python code.  - <code>prompt.py</code>: which defines the prompts used in the workflow. </p> <p>Below is an example of the <code>graph.py</code> file for the HumanEval benchmark:</p> <pre><code>import evoagentx.workflow.operators as operator\nimport examples.aflow.code_generation.prompt as prompt_custom # noqa: F401\nfrom evoagentx.models.model_configs import LLMConfig\nfrom evoagentx.benchmark.benchmark import Benchmark\nfrom evoagentx.models.model_utils import create_llm_instance\n\nclass Workflow:\n\n    def __init__(\n        self,\n        name: str,\n        llm_config: LLMConfig,\n        benchmark: Benchmark\n    ):\n        self.name = name\n        self.llm = create_llm_instance(llm_config)\n        self.benchmark = benchmark \n        self.custom = operator.Custom(self.llm)\n        self.custom_code_generate = operator.CustomCodeGenerate(self.llm)\n\n    async def __call__(self, problem: str, entry_point: str):\n        \"\"\"\n        Implementation of the workflow\n        Custom operator to generate anything you want.\n        But when you want to get standard code, you should use custom_code_generate operator.\n        \"\"\"\n        # await self.custom(input=, instruction=\"\")\n        solution = await self.custom_code_generate(problem=problem, entry_point=entry_point, instruction=prompt_custom.GENERATE_PYTHON_CODE_PROMPT) # But When you want to get standard code ,you should use customcodegenerator.\n        return solution['response']\n</code></pre> <p>Note</p> <p>When defining your workflow, please pay attention to the following key points:</p> <ol> <li> <p>Prompt Import Path: Ensure the import path for <code>prompt.py</code> is correctly specified (e.g., <code>examples.aflow.code_generation.prompt</code>). This path should match your project structure to enable proper prompt loading.</p> </li> <li> <p>Operator Initialization: In the <code>__init__</code> function, you must initialize all operators that will be used in the workflow. Each operator should be instantiated with the appropriate LLM instance. </p> </li> <li> <p>Workflow Execution: The <code>__call__</code> function serves as the main entry point for workflow execution. It should define the complete execution logic of your workflow and return the final output that will be used for evaluation.</p> </li> </ol> <p>Below is an example of the <code>prompt.py</code> file for the HumanEval benchmark:</p> <pre><code>GENERATE_PYTHON_CODE_PROMPT = \"\"\"\nGenerate a functional and correct Python code for the given problem.\n\nProblem: \"\"\"\n</code></pre> <p>Note</p> <p>If the workflow does not require any prompts, the <code>prompt.py</code> file can be empty. </p>"},{"location":"tutorial/aflow_optimizer.html#step-3-prepare-the-benchmark","title":"Step 3: Prepare the Benchmark","text":"<p>For this tutorial, we'll use the AFlowHumanEval benchmark. It follows the exact same data split and format as used in the original AFlow implementation.</p> <pre><code># Initialize the benchmark\nhumaneval = AFlowHumanEval()\n</code></pre>"},{"location":"tutorial/aflow_optimizer.html#4-configuring-and-running-the-aflow-optimizer","title":"4. Configuring and Running the AFlow Optimizer","text":"<p>The AFlow optimizer can be configured with various parameters to control the optimization process:</p> <pre><code>optimizer = AFlowOptimizer(\n    graph_path=\"examples/aflow/code_generation\",  # Path to the initial workflow graph\n    optimized_path=\"examples/aflow/humaneval/optimized\",  # Path to save optimized workflows\n    optimizer_llm=optimizer_llm,  # LLM for optimization\n    executor_llm=executor_llm,    # LLM for execution\n    validation_rounds=3,          # Number of times to run validation on the development set during optimization\n    eval_rounds=3,               # Number of times to run evaluation on the test set during testing\n    max_rounds=20,               # Maximum optimization rounds\n    **EXPERIMENTAL_CONFIG[\"humaneval\"]  # Task-specific configuration, used to specify the task type and available operators\n)\n</code></pre>"},{"location":"tutorial/aflow_optimizer.html#running-the-optimization","title":"Running the Optimization","text":"<p>To start the optimization process:</p> <pre><code># Optimize the workflow\noptimizer.optimize(humaneval)\n</code></pre> <p>Note</p> <p>During optimization, the workflow will be validated on the development set for <code>validation_rounds</code> times at each step. Make sure the benchmark <code>humaneval</code> contains a development set (i.e., <code>self._dev_data</code> is not empty).</p>"},{"location":"tutorial/aflow_optimizer.html#test-the-optimized-workflow","title":"Test the Optimized Workflow","text":"<p>To test the optimized workflow:</p> <p><pre><code>optimizer.test(humaneval)\n</code></pre> By default, the optimizer will choose the workflow with the highest validation performance to test. You can also specify the test rounds using the <code>test_rounds: List[int]</code> parameter. For example, to evaluate the second round and the third round, you can use <code>optimizer.test(humaneval, test_rounds=[2, 3])</code>.</p> <p>Note</p> <p>During testing, the workflow will be evaluated on the test set for <code>eval_rounds</code> times. Make sure the benchmark <code>humaneval</code> contains a test set (i.e., <code>self._test_data</code> is not empty).</p> <p>For a complete working example, please refer to aflow_humaneval.py.</p>"},{"location":"tutorial/benchmark_and_evaluation.html","title":"Benchmark and Evaluation Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running benchmark evaluations using EvoAgentX. We'll use the HotpotQA dataset as an example to demonstrate how to set up and run the evaluation process.</p>"},{"location":"tutorial/benchmark_and_evaluation.html#1-overview","title":"1. Overview","text":"<p>EvoAgentX provides a flexible and modular evaluation framework that enables you to:</p> <ul> <li>Load and use predefined benchmark datasets</li> <li>Customize data loading, processing, and post-processing logic</li> <li>Evaluate the performance of your multi-agent workflows</li> <li>Process multiple evaluation tasks in parallel</li> </ul>"},{"location":"tutorial/benchmark_and_evaluation.html#2-setting-up-the-benchmark","title":"2. Setting Up the Benchmark","text":"<p>To get started, you need to import the relevant modules and set up the language model (LLM) that your agent will use during evaluation.</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAIConfig, OpenAI \nfrom evoagentx.benchmark import HotpotQA\nfrom evoagentx.workflow import QAActionGraph \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"tutorial/benchmark_and_evaluation.html#configure-the-llm-model","title":"Configure the LLM Model","text":"<p>You'll need a valid OpenAI API key to initialize the LLM. It is recommended to save your API key in the <code>.env</code> file and load it using the <code>load_dotenv</code> function:  <pre><code>import os \nfrom dotenv import load_dotenv\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre></p>"},{"location":"tutorial/benchmark_and_evaluation.html#3-initialize-the-benchmark","title":"3. Initialize the Benchmark","text":"<p>EvoAgentX includes several predefined benchmarks for tasks like Question Answering, Math, and Coding. Please refer to the Benchmark README for more details about existing benchmarks. You can also define your own benchmark class by extending the base <code>Benchmark</code> interface, and we provide an example in the Custom Benchmark section.</p> <p>In this example, we will use the <code>HotpotQA</code> benchmark.  <pre><code>benchmark = HotPotQA(mode=\"dev\")\n</code></pre> where <code>mode</code> parameter determines which split of the dataset is loaded. Options include:</p> <ul> <li><code>\"train\"</code>: Training data</li> <li><code>\"dev\"</code>: Development/validation data</li> <li><code>\"test\"</code>: Test data</li> <li><code>\"all\"</code> (default): Loads the entire dataset</li> </ul> <p>The data will be automatically downloaded to a default cache folder, but you can change this location by specifying the <code>path</code> parameter.</p>"},{"location":"tutorial/benchmark_and_evaluation.html#4-running-the-evaluation","title":"4. Running the Evaluation","text":"<p>Once you have your benchmark and LLM ready, the next step is to define your agent workflow and evaluation logic. EvoAgentX supports full customization of how benchmark examples are processed and how outputs are interpreted.</p> <p>Here's how to run an evaluation using the <code>HotpotQA</code> benchmark and a QA workflow.</p>"},{"location":"tutorial/benchmark_and_evaluation.html#step-1-define-the-agent-workflow","title":"Step 1: Define the Agent Workflow","text":"<p>You can use one of the predefined workflows or implement your own. In this example, we use the <code>QAActionGraph</code> designed for question answering, which simply use self-consistency to generate the final answer:</p> <pre><code>workflow = QAActionGraph(\n    llm_config=llm_config,\n    description=\"This workflow aims to address multi-hop QA tasks.\"\n)\n</code></pre>"},{"location":"tutorial/benchmark_and_evaluation.html#step-2-customize-data-preprocessing-and-post-processing","title":"Step 2: Customize Data Preprocessing and Post-processing","text":"<p>The next key aspect of evaluation is properly transforming data between your benchmark, workflow, and evaluation metrics.</p>"},{"location":"tutorial/benchmark_and_evaluation.html#why-preprocessing-and-postprocessing-are-needed","title":"Why Preprocessing and Postprocessing Are Needed","text":"<p>In EvoAgentX, preprocessing and postprocessing are essential steps to ensure smooth interaction between benchmark data, workflows, and evaluation logic:</p> <ul> <li> <p>Preprocessing (<code>collate_func</code>):  </p> <p>The raw examples from a benchmark like HotpotQA typically consist of structured fields such as questions, answer, and context. However, your agent workflow usually expects a single prompt string or other structured input. The <code>collate_func</code> is used to convert each raw example into a format that can be consumed by your (custom) workflow.</p> </li> <li> <p>Postprocessing (<code>output_postprocess_func</code>):</p> <p>The workflow output might include reasoning steps or additional formatting beyond just the final answer. Since the <code>Evaluator</code> internally calls the benchmark's <code>evaluate</code> method to compute metrics (e.g., exact match or F1), it's often necessary to extract the final answer in a clean format. The <code>output_postprocess_func</code> handles this and ensures the output is in the right form for evaluation.</p> </li> </ul> <p>In short, preprocessing prepares benchmark examples for the workflow, while postprocessing prepares workflow outputs for evaluation.</p> <p>In the following example, we define a <code>collate_func</code> to format the raw examples into a prompt for the workflow, and a <code>output_postprocess_func</code> to extract the final answer from the workflow output.</p> <p>Each example in the benchmark can be formatted using a <code>collate_func</code>, which transforms raw examples into a prompt or structured input for the agent.</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        example (dict): A dictionary containing the raw example data.\n\n    Returns: \n        The expected input for the (custom) workflow.\n    \"\"\"\n    problem = \"Question: {}\\n\\n\".format(example[\"question\"])\n    context_list = []\n    for item in example[\"context\"]:\n        context = \"Title: {}\\nText: {}\".format(item[0], \" \".join([t.strip() for t in item[1]]))\n        context_list.append(context)\n    context = \"\\n\\n\".join(context_list)\n    problem += \"Context: {}\\n\\n\".format(context)\n    problem += \"Answer:\" \n    return {\"problem\": problem}\n</code></pre> <p>After the agent generates an output, you can define how to extract the final answer using <code>output_postprocess_func</code>.  <pre><code>def output_postprocess_func(output: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        output (dict): The output from the workflow.\n\n    Returns: \n        The processed output that can be used to compute the metrics. The output will be directly passed to the benchmark's `evaluate` method. \n    \"\"\"\n    return output[\"answer\"]\n</code></pre></p>"},{"location":"tutorial/benchmark_and_evaluation.html#step-3-initialize-the-evaluator","title":"Step 3: Initialize the Evaluator","text":"<p>The Evaluator ties everything together \u2014 it runs the workflow over the benchmark and calculates performance metrics.</p> <p><pre><code>evaluator = Evaluator(\n    llm=llm, \n    collate_func=collate_func,\n    output_postprocess_func=output_postprocess_func,\n    verbose=True, \n    num_workers=3 \n)\n</code></pre> If <code>num_workers</code> is greater than 1, the evaluation will be parallelized across multiple threads.  </p>"},{"location":"tutorial/benchmark_and_evaluation.html#step-4-run-the-evaluation","title":"Step 4: Run the Evaluation","text":"<p>You can now run the evaluation by providing the workflow and benchmark to the evaluator:</p> <p><pre><code>with suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow, \n        benchmark=benchmark, \n        eval_mode=\"dev\", # Evaluation split: train / dev / test \n        sample_k=10 # If set, randomly sample k examples from the benchmark for evaluation  \n    )\n\nprint(\"Evaluation metrics: \", results)\n</code></pre> where <code>suppress_logger_info</code> is used to suppress the logger info.</p> <p>Please refer to the benchmark and evaluation example for a complete example.</p>"},{"location":"tutorial/benchmark_and_evaluation.html#custom-benchmark","title":"Custom Benchmark","text":"<p>To define a custom benchmark, you need to extend the <code>Benchmark</code> class and implement the following methods:</p> <ul> <li> <p><code>_load_data(self)</code>: </p> <p>Load the benchmark data, and set the <code>self._train_data</code>, <code>self._dev_data</code> and <code>self._test_data</code> attributes.</p> </li> <li> <p><code>_get_id(self, example: Any) -&gt; Any</code>: </p> <p>Return the unique identifier of an example.</p> </li> <li> <p><code>_get_label(self, example: Any) -&gt; Any</code>:</p> <p>Return the label or ground truth associated with a given example.</p> <p>This is used to compare predictions against the correct answer during evaluation. The output will be directly passed to the <code>evaluate</code> method. </p> </li> <li> <p><code>evaluate(self, prediction: Any, label: Any) -&gt; dict</code>: </p> <p>Compute the evaluation metrics for a single example, based on its prediction and ground-truth label (obtained from <code>_get_label</code>). This method should return a dictionary of metric name(s) and value(s).</p> </li> </ul> <p>For a complete example of a benchmark implementation, please refer to the HotPotQA class.</p>"},{"location":"tutorial/first_agent.html","title":"Build Your First Agent","text":"<p>In EvoAgentX, agents are intelligent components designed to complete specific tasks autonomously. This tutorial will walk you through the essential concepts of creating and using agents in EvoAgentX:</p> <ol> <li>Creating a Simple Agent with CustomizeAgent: Learn how to create a basic agent with custom prompts </li> <li>Working with Multiple Actions: Create more complex agents that can perform multiple tasks</li> <li>Saving and Loading Agents: Learn how to save and load your agents</li> </ol> <p>By the end of this tutorial, you'll be able to create both simple and complex agents, understand how they process inputs and outputs, and know how to save and reuse them in your projects.</p>"},{"location":"tutorial/first_agent.html#1-creating-a-simple-agent-with-customizeagent","title":"1. Creating a Simple Agent with CustomizeAgent","text":"<p>The easiest way to create an agent is using <code>CustomizeAgent</code>, which allows you to quickly define an agent with a specific prompt.  </p> <p>First, let's import the necessary components and setup the LLM:</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure LLM\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\", \n    openai_key=OPENAI_API_KEY, \n    stream=True\n)\n</code></pre> <p>Now, let's create a simple agent that prints hello world. There are two ways to create a CustomizeAgent:</p>"},{"location":"tutorial/first_agent.html#method-1-direct-initialization","title":"Method 1: Direct Initialization","text":"<p>You can directly initialize the agent with the <code>CustomizeAgent</code> class:  <pre><code>first_agent = CustomizeAgent(\n    name=\"FirstAgent\",\n    description=\"A simple agent that prints hello world\",\n    prompt=\"Print 'hello world'\", \n    llm_config=openai_config # specify the LLM configuration \n)\n</code></pre></p>"},{"location":"tutorial/first_agent.html#method-2-creating-from-dictionary","title":"Method 2: Creating from Dictionary","text":"<p>You can also create an agent by defining its configuration in a dictionary:</p> <pre><code>agent_data = {\n    \"name\": \"FirstAgent\",\n    \"description\": \"A simple agent that prints hello world\",\n    \"prompt\": \"Print 'hello world'\",\n    \"llm_config\": openai_config\n}\nfirst_agent = CustomizeAgent.from_dict(agent_data) # use .from_dict() to create an agent. \n</code></pre>"},{"location":"tutorial/first_agent.html#using-the-agent","title":"Using the Agent","text":"<p>Once created, you can use the agent to print hello world. </p> <pre><code># Execute the agent without input. The agent will return a Message object containing the results. \nmessage = first_agent()\n\nprint(f\"Response from {first_agent.name}:\")\nprint(message.content.content) # the content of a Message object is a LLMOutputParser object, where the `content` attribute is the raw LLM output. \n</code></pre> <p>For a complete example, please refer to the CustomizeAgent example. </p> <p>CustomizeAgent also offers other features including structured inputs/outputs and multiple parsing strategies. For detailed information, see the CustomizeAgent documentation.</p>"},{"location":"tutorial/first_agent.html#2-creating-an-agent-with-multiple-actions","title":"2. Creating an Agent with Multiple Actions","text":"<p>In EvoAgentX, you can create an agent with multiple predefined actions. This allows you to build more complex agents that can perform multiple tasks. Here's an example showing how to create an agent with <code>TestCodeGeneration</code> and <code>TestCodeReview</code> actions:</p>"},{"location":"tutorial/first_agent.html#defining-actions","title":"Defining Actions","text":"<p>First, we need to define the actions, which are subclasses of <code>Action</code>:  <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# Define the CodeGeneration action inputs\nclass TestCodeGenerationInput(ActionInput):\n    requirement: str = Field(description=\"The requirement for the code generation\")\n\n# Define the CodeGeneration action outputs\nclass TestCodeGenerationOutput(ActionOutput):\n    code: str = Field(description=\"The generated code\")\n\n# Define the CodeGeneration action\nclass TestCodeGeneration(Action): \n\n    def __init__(\n        self, \n        name: str=\"TestCodeGeneration\", \n        description: str=\"Generate code based on requirements\", \n        prompt: str=\"Generate code based on requirements: {requirement}\",\n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeGenerationInput\n        outputs_format = outputs_format or TestCodeGenerationOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeGenerationOutput:\n        action_input_attrs = self.inputs_format.get_attrs() # obtain the attributes of the action input \n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data) # format the prompt with the action input data \n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg, \n            parser=self.outputs_format, \n            parse_mode=\"str\" # specify how to parse the output \n        )\n        if return_prompt:\n            return output, prompt\n        return output\n\n\n# Define the CodeReview action inputs\nclass TestCodeReviewInput(ActionInput):\n    code: str = Field(description=\"The code to be reviewed\")\n    requirements: str = Field(description=\"The requirements for the code review\")\n\n# Define the CodeReview action outputs\nclass TestCodeReviewOutput(ActionOutput):\n    review: str = Field(description=\"The review of the code\")\n\n# Define the CodeReview action\nclass TestCodeReview(Action):\n    def __init__(\n        self, \n        name: str=\"TestCodeReview\", \n        description: str=\"Review the code based on requirements\", \n        prompt: str=\"Review the following code based on the requirements:\\n\\nRequirements: {requirements}\\n\\nCode:\\n{code}.\\n\\nYou should output a JSON object with the following format:\\n```json\\n{{\\n'review': '...'\\n}}\\n```\", \n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeReviewInput\n        outputs_format = outputs_format or TestCodeReviewOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeReviewOutput:\n        action_input_attrs = self.inputs_format.get_attrs()\n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data)\n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"json\" # specify how to parse the output \n        ) \n        if return_prompt:\n            return output, prompt\n        return output\n</code></pre></p> <p>From the above example, we can see that in order to define an action, we need to:</p> <ol> <li>Define the action inputs and outputs using <code>ActionInput</code> and <code>ActionOutput</code> classes</li> <li>Create an action class that inherits from <code>Action</code></li> <li>Implement the <code>execute</code> method which formulates the prompt with the action input data and uses the LLM to generate output, and specify how to parse the output using <code>parse_mode</code>.</li> </ol>"},{"location":"tutorial/first_agent.html#defining-an-agent","title":"Defining an Agent","text":"<p>Once we have defined the actions, we can create an agent by adding the actions to it:</p> <pre><code># Initialize the LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Define the agent \ndeveloper = Agent(\n    name=\"Developer\", \n    description=\"A developer who can write code and review code\",\n    actions=[TestCodeGeneration(), TestCodeReview()], \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"tutorial/first_agent.html#executing-different-actions","title":"Executing Different Actions","text":"<p>Once you've created an agent with multiple actions, you can execute specific actions:</p> <pre><code># List all available actions on the agent\nactions = developer.get_all_actions()\nprint(f\"Available actions of agent {developer.name}:\")\nfor action in actions:\n    print(f\"- {action.name}: {action.description}\")\n\n# Generate some code using the CodeGeneration action\ngeneration_result = developer.execute(\n    action_name=\"TestCodeGeneration\", # specify the action name\n    action_input_data={ \n        \"requirement\": \"Write a function that returns the sum of two numbers\"\n    }\n)\n\n# Access the generated code\ngenerated_code = generation_result.content.code\nprint(\"Generated code:\")\nprint(generated_code)\n\n# Review the generated code using the CodeReview action\nreview_result = developer.execute(\n    action_name=\"TestCodeReview\",\n    action_input_data={\n        \"requirements\": \"Write a function that returns the sum of two numbers\",\n        \"code\": generated_code\n    }\n)\n\n# Access the review results\nreview = review_result.content.review\nprint(\"\\nReview:\")\nprint(review)\n</code></pre> <p>This example demonstrates how to: 1. List all available actions on an agent 2. Generate code using the TestCodeGeneration action 3. Review the generated code using the TestCodeReview action 4. Access the results from each action execution</p> <p>For a complete working example, please refer to the Agent example. </p>"},{"location":"tutorial/first_agent.html#3-saving-and-loading-agents","title":"3. Saving and Loading Agents","text":"<p>You can save an agent to a file and load it later:</p> <pre><code># Save the agent to a file\ndeveloper.save_module(\"examples/output/developer.json\") # ignore the LLM config to avoid saving the API key \n\n# Load the agent from a file\ndeveloper = Agent.from_file(\"examples/output/developer.json\", llm_config=openai_config)\n</code></pre>"},{"location":"tutorial/first_workflow.html","title":"Build Your First Workflow","text":"<p>In EvoAgentX, workflows allow multiple agents to collaborate sequentially on complex tasks. This tutorial will guide you through creating and using workflows:</p> <ol> <li>Understanding Sequential Workflows: Learn how workflows connect multiple tasks together</li> <li>Building a Sequential Workflow: Create a workflow with planning and coding steps</li> <li>Executing and Managing Workflows: Run workflows with specific inputs</li> </ol> <p>By the end of this tutorial, you'll be able to create sequential workflows that coordinate multiple agents to solve complex problems.</p>"},{"location":"tutorial/first_workflow.html#1-understanding-sequential-workflows","title":"1. Understanding Sequential Workflows","text":"<p>A workflow in EvoAgentX represents a sequence of tasks that can be executed by different agents. The simplest workflow is a sequential workflow, where tasks are executed one after another with outputs from previous tasks feeding into subsequent ones.</p> <p>Let's start by importing the necessary components:</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.workflow import SequentialWorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"tutorial/first_workflow.html#2-building-a-sequential-workflow","title":"2. Building a Sequential Workflow","text":"<p>A sequential workflow consists of a series of tasks where each task has:</p> <ul> <li>A name and description</li> <li>Input and output definitions</li> <li>A prompt template</li> <li>Parsing mode and function (optional) </li> </ul> <p>Here's how to build a sequential workflow with planning and coding tasks:</p> <pre><code># Configure the LLM \nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY, stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# Define a custom parsing function (if needed)\nfrom evoagentx.core.registry import register_parse_function\nfrom evoagentx.core.module_utils import extract_code_blocks\n\n# [optional] Define a custom parsing function (if needed)\n# It is suggested to use the `@register_parse_function` decorator to register a custom parsing function, so the workflow can be saved and loaded correctly.  \n\n@register_parse_function\ndef custom_parse_func(content: str) -&gt; str:\n    return {\"code\": extract_code_blocks(content)[0]}\n\n# Define sequential tasks\ntasks = [\n    {\n        \"name\": \"Planning\",\n        \"description\": \"Create a detailed plan for code generation\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed plan with steps, components, and architecture\"}\n        ],\n        \"prompt\": \"You are a software architect. Your task is to create a detailed implementation plan for the given problem.\\n\\nProblem: {problem}\\n\\nPlease provide a comprehensive implementation plan including:\\n1. Problem breakdown\\n2. Algorithm or approach selection\\n3. Implementation steps\\n4. Potential edge cases and solutions\",\n        \"parse_mode\": \"str\",\n        # \"llm_config\": specific_llm_config # if you want to use a specific LLM for a task, you can add a key `llm_config` in the task dict.  \n    },\n    {\n        \"name\": \"Coding\",\n        \"description\": \"Implement the code based on the implementation plan\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed implementation plan from the Planning phase\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"code\", \"type\": \"str\", \"required\": True, \"description\": \"Implemented code with explanations\"}\n        ],\n        \"prompt\": \"You are a software developer. Your task is to implement the code based on the provided problem and implementation plan.\\n\\nProblem: {problem}\\nImplementation Plan: {plan}\\n\\nPlease provide the implementation code with appropriate comments.\",\n        \"parse_mode\": \"custom\",\n        \"parse_func\": custom_parse_func\n    }\n]\n\n# Create the sequential workflow graph\ngraph = SequentialWorkFlowGraph(\n    goal=\"Generate code to solve programming problems\",\n    tasks=tasks\n)\n</code></pre> <p>Note</p> <p>When you create a <code>SequentialWorkFlowGraph</code> with a list of tasks, the framework will create a <code>CustomizeAgent</code> for each task. Each task in the workflow becomes a specialized agent configured with the specific prompt, input/output formats, and parsing mode you defined. These agents are connected in sequence, with outputs from one agent becoming inputs to the next. </p> <p>The <code>parse_mode</code> controls how the output from an LLM is parsed into a structured format. Available options are: [<code>'str'</code> (default), <code>'json'</code>, <code>'title'</code>, <code>'xml'</code>, <code>'custom'</code>]. For detailed information about parsing modes and examples, please refer to the CustomizeAgent documentation.</p>"},{"location":"tutorial/first_workflow.html#3-executing-and-managing-workflows","title":"3. Executing and Managing Workflows","text":"<p>Once you've created a workflow graph, you can create an instance of the workflow and execute it:</p> <pre><code># Create agent manager and add agents from the workflow. It will create a `CustomizeAgent` for each task in the workflow. \nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(\n    graph, \n    llm_config=llm_config  # This config will be used for all tasks without `llm_config`. \n)\n\n# Create workflow instance\nworkflow = WorkFlow(graph=graph, agent_manager=agent_manager, llm=llm)\n\n# Execute the workflow with inputs\noutput = workflow.execute(\n    inputs = {\n        \"problem\": \"Write a function to find the longest palindromic substring in a given string.\"\n    }\n)\n\nprint(\"Workflow completed!\")\nprint(\"Workflow output:\\n\", output)\n</code></pre> <p>You should specify all the required inputs for the workflow in the <code>inputs</code> argument of the <code>execute</code> method. </p> <p>For a complete working example, please refer to the Sequential Workflow example. </p>"},{"location":"tutorial/first_workflow.html#4-saving-and-loading-workflows","title":"4. Saving and Loading Workflows","text":"<p>You can save a workflow graph for future use:</p> <pre><code># Save the workflow graph to a file\ngraph.save_module(\"examples/output/saved_sequential_workflow.json\")\n\n# Load the workflow graph from a file\nloaded_graph = SequentialWorkFlowGraph.from_file(\"examples/output/saved_sequential_workflow.json\")\n\n# Create a new workflow with the loaded graph\nnew_workflow = WorkFlow(graph=loaded_graph, agent_manager=agent_manager, llm=llm)\n</code></pre> <p>For more complex workflows or different types of workflow graphs, please refer to the Workflow Graphs documentation and the Action Graphs documentation. </p>"},{"location":"tutorial/rag.html","title":"Build Your First RAG System with RAGEngine","text":"<p>In EvoAgentX, the <code>RAGEngine</code> is a powerful tool for building Retrieval-Augmented Generation (RAG) systems. It allows you to load documents, create searchable indices, and retrieve relevant information to answer questions. This tutorial is designed for beginners and will guide you through the essential steps to create and use a RAG system with <code>RAGEngine</code>:</p> <ol> <li>Setting Up RAGEngine: Learn how to configure and initialize the engine.</li> <li>Indexing and Querying Documents: Load a simple dataset and query it to find answers.</li> <li>Saving and Loading Indices: Save your indexed data and reuse it later.</li> </ol> <p>By the end of this tutorial, you\u2019ll be able to set up a basic RAG system, index documents, query them, and persist your work for future use. We\u2019ll use a sample from the HotPotQA dataset to make it practical and fun!</p>"},{"location":"tutorial/rag.html#1-setting-up-ragengine","title":"1. Setting Up RAGEngine","text":"<p>The first step is to set up your environment and initialize <code>RAGEngine</code>. This involves installing dependencies, configuring the storage backend, and setting up the embedding model.</p>"},{"location":"tutorial/rag.html#install-dependencies","title":"Install Dependencies","text":"<p>Ensure you have EvoAgentX installed. You\u2019ll also need an OpenAI API key for embeddings. Run the following in your terminal:</p> <pre><code>pip install evoagentx llama_index pydantic python-dotenv\n</code></pre> <p>Create a <code>.env</code> file in your project directory and add your OpenAI API key:</p> <pre><code>OPENAI_API_KEY=your-openai-api-key\n</code></pre>"},{"location":"tutorial/rag.html#configure-the-environment","title":"Configure the Environment","text":"<p>Let\u2019s write a Python script to set up <code>RAGEngine</code>. We\u2019ll use SQLite for metadata storage and FAISS for vector embeddings, which are beginner-friendly options.</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure storage (SQLite for metadata, FAISS for vectors)\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# Configure RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=50),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=3, similarity_cutoff=0.3)\n)\n\n# Initialize RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\nprint(\"RAGEngine is ready to go!\")\n</code></pre>"},{"location":"tutorial/rag.html#whats-happening-here","title":"What\u2019s Happening Here?","text":"<ul> <li>Environment Setup: We load the OpenAI API key from the <code>.env</code> file.</li> <li>Storage Configuration: We set up SQLite to store metadata and FAISS to store vector embeddings. The <code>dimensions=1536</code> matches the OpenAI embedding model.</li> <li>RAG Configuration: We configure the pipeline:</li> <li><code>ReaderConfig</code>: Reads files (we\u2019ll use a JSON file later).</li> <li><code>ChunkerConfig</code>: Splits documents into 512-character chunks with 50-character overlap.</li> <li><code>EmbeddingConfig</code>: Uses OpenAI\u2019s <code>text-embedding-ada-002</code> for embeddings.</li> <li><code>IndexConfig</code>: Creates a vector index.</li> <li><code>RetrievalConfig</code>: Retrieves the top 3 most similar chunks with a similarity score above 0.3.</li> <li>Initialization: We create the <code>RAGEngine</code> instance, ready to process documents.</li> </ul> <p>For more details on configuration, see the RAGEngine documentation.</p>"},{"location":"tutorial/rag.html#2-indexing-and-querying-documents","title":"2. Indexing and Querying Documents","text":"<p>Now, let\u2019s index a document from the HotPotQA dataset and query it. We\u2019ll use the first HotPotQA example you provided, which contains information about Scott Derrickson and Ed Wood, and answer the question: \u201cWere Scott Derrickson and Ed Wood of the same nationality?\u201d</p>"},{"location":"tutorial/rag.html#prepare-the-dataset","title":"Prepare the Dataset","text":"<p>Create a directory called <code>data</code> and save the HotPotQA example as <code>hotpotqa_sample.json</code>:</p> <pre><code>{\n  \"_id\": \"5a8b57f25542995d1e6f1371\",\n  \"answer\": \"yes\",\n  \"question\": \"Were Scott Derrickson and Ed Wood of the same nationality?\",\n  \"supporting_facts\": [\n    [\n      \"Scott Derrickson\",\n      0\n    ],\n    [\n      \"Ed Wood\",\n      0\n    ]\n  ],\n  \"context\": [\n    [\n      \"Ed Wood (film)\",\n      [\n        \"Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.\",\n        \" The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.\",\n        \" Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\"\n      ]\n    ],\n    [\n      \"Scott Derrickson\",\n      [\n        \"Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\",\n        \" He lives in Los Angeles, California.\",\n        \" He is best known for directing horror films such as \\\"Sinister\\\", \\\"The Exorcism of Emily Rose\\\", and \\\"Deliver Us From Evil\\\", as well as the 2016 Marvel Cinematic Universe installment, \\\"Doctor Strange.\\\"\"\n      ]\n    ],\n    [\n      \"Woodson, Arkansas\",\n      [\n        \"Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.\",\n        \" Its population was 403 at the 2010 census.\",\n        \" It is part of the Little Rock\\u2013North Little Rock\\u2013Conway Metropolitan Statistical Area.\",\n        \" Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.\",\n        \" Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.\"\n      ]\n    ],\n    [\n      \"Tyler Bates\",\n      [\n        \"Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.\",\n        \" Much of his work is in the action and horror film genres, with films like \\\"Dawn of the Dead, 300, Sucker Punch,\\\" and \\\"John Wick.\\\"\",\n        \" He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.\",\n        \" With Gunn, he has scored every one of the director's films; including \\\"Guardians of the Galaxy\\\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.\",\n        \" In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \\\"The Pale Emperor\\\" and \\\"Heaven Upside Down\\\".\"\n      ]\n    ],\n    [\n      \"Ed Wood\",\n      [\n        \"Edward Davis Wood Jr. (October 10, 1924 \\u2013 December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\"\n      ]\n    ],\n    [\n      \"Deliver Us from Evil (2014 film)\",\n      [\n        \"Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.\",\n        \" The film is officially based on a 2001 non-fiction book entitled \\\"Beware the Night\\\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \\\"inspired by actual accounts\\\".\",\n        \" The film stars Eric Bana, \\u00c9dgar Ram\\u00edrez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.\"\n      ]\n    ],\n    [\n      \"Adam Collis\",\n      [\n        \"Adam Collis is an American filmmaker and actor.\",\n        \" He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.\",\n        \" He also studied cinema at the University of Southern California from 1991 to 1997.\",\n        \" Collis first work was the assistant director for the Scott Derrickson's short \\\"Love in the Ruins\\\" (1995).\",\n        \" In 1998, he played \\\"Crankshaft\\\" in Eric Koyanagi's \\\"Hundred Percent\\\".\"\n      ]\n    ],\n    [\n      \"Sinister (film)\",\n      [\n        \"Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.\",\n        \" It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.\"\n      ]\n    ],\n    [\n      \"Conrad Brooks\",\n      [\n        \"Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.\",\n        \" He moved to Hollywood, California in 1948 to pursue a career in acting.\",\n        \" He got his start in movies appearing in Ed Wood films such as \\\"Plan 9 from Outer Space\\\", \\\"Glen or Glenda\\\", and \\\"Jail Bait.\\\"\",\n        \" He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.\",\n        \" He also has since gone on to write, produce and direct several films.\"\n      ]\n    ],\n    [\n      \"Doctor Strange (2016 film)\",\n      [\n        \"Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.\",\n        \" It is the fourteenth film of the Marvel Cinematic Universe (MCU).\",\n        \" The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.\",\n        \" In \\\"Doctor Strange\\\", surgeon Strange learns the mystic arts after a career-ending car accident.\"\n      ]\n    ]\n  ],\n  \"type\": \"comparison\",\n  \"level\": \"hard\"\n}\n</code></pre>"},{"location":"tutorial/rag.html#index-the-document","title":"Index the Document","text":"<p>Let\u2019s write a script to load the JSON file, index its content, and query it. Create a file called <code>rag_tutorial.py</code>:</p> <pre><code>import os\nimport json\n\nfrom dotenv import load_dotenv\n\nfrom evoagentx.rag.schema import Query\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure storage (SQLite for metadata, FAISS for vectors)\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# Configure RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=50),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=3, similarity_cutoff=0.3)\n)\n\n# Initialize RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\nprint(\"RAGEngine is ready to go!\")\n\n# Step 1: Load and index the HotPotQA sample\nwith open(\"./data/hotpotqa_sample.json\", \"r\", encoding=\"utf-8\") as f:\n    hotpotqa_data = json.load(f)\n\n# Create a temporary text file with the context for indexing\ncontext = hotpotqa_data[\"context\"]\nwith open(\"./data/hotpotqa_context.txt\", \"w\", encoding=\"utf-8\") as f:\n    for title, sentences in context:\n        f.write(f\"# {title}\")\n        for sentence in sentences:\n            f.write(f\"{sentence}\\n\")\n        f.write(f\"\\n\\n\")\n\n# Index the text file\ncorpus = rag_engine.read(\n    file_paths=\"./data/hotpotqa_context.txt\",\n    filter_file_by_suffix=[\".txt\"],\n    merge_by_file=True,\n    show_progress=True,\n    corpus_id=\"hotpotqa_corpus\"\n)\nrag_engine.add(index_type=\"vector\", nodes=corpus, corpus_id=\"hotpotqa_corpus\")\n\nprint(\"Documents indexed successfully!\")\n\n# Step 2: Query the index\nquery = Query(query_str=\"Were Scott Derrickson and Ed Wood of the same nationality?\", top_k=3)\nresult = rag_engine.query(query, corpus_id=\"hotpotqa_corpus\")\n\n# Print the retrieved chunks\nprint(\"\\nRetrieved answers:\")\nfor i, chunk in enumerate(result.corpus.chunks, 1):\n    print(f\"{i}. {chunk.text}\")\n\n# Clean up\nrag_engine.clear(corpus_id=\"hotpotqa_corpus\")\n</code></pre>"},{"location":"tutorial/rag.html#run-the-script","title":"Run the Script","text":"<p>Run the script:</p> <pre><code>python rag_tutorial.py\n</code></pre>"},{"location":"tutorial/rag.html#whats-happening-here_1","title":"What\u2019s Happening Here?","text":"<ul> <li>Loading the Data: We read the HotPotQA JSON file and extract its <code>context</code> field, which contains text about Scott Derrickson and Ed Wood.</li> <li>Indexing:</li> <li>We create a temporary text file (<code>hotpotqa_context.txt</code>) with the context, formatted for readability.</li> <li>The <code>read</code> method loads the text file into a <code>Corpus</code>, splitting it into chunks (based on the 512-character limit set earlier).</li> <li>The <code>add</code> method indexes the chunks into a vector index using OpenAI embeddings.</li> <li>Querying:</li> <li>We create a <code>Query</code> object with the question from the dataset.</li> <li>The <code>query</code> method retrieves the top 3 chunks most similar to the question.</li> <li>The results include sentences indicating both individuals are American (e.g., \u201cScott Derrickson\u2026 is an American director\u201d and \u201cEdward Davis Wood Jr\u2026 was an American filmmaker\u201d).</li> <li>Cleanup: We clear the index to free up memory.</li> </ul>"},{"location":"tutorial/rag.html#expected-output","title":"Expected Output","text":"<p>You should see something like:</p> <pre><code>Documents indexed successfully!\n\nRetrieved answers:\n1. # Ed Wood (film)Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.\n The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau. \n Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\n\n\n# Scott DerricksonScott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\n He lives in Los Angeles, California.\n He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange.\"\n\n\n# Woodson, ArkansasWoodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.\n Its population was 403 at the 2010 census.\n It is part of the Little Rock\u2013North Little Rock\u2013Conway Metropolitan Statistical Area.\n Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.\n Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.\n\n\n# Tyler BatesTyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.\n Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"\n He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.\n With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.\n2. With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.\n In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".  \n\n\n# Ed WoodEdward Davis Wood Jr. (October 10, 1924 \u2013 December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n\n\n# Deliver Us from Evil (2014 film)Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.\n The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".\n The film stars Eric Bana, \u00c9dgar Ram\u00edrez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.\n\n\n# Adam CollisAdam Collis is an American filmmaker and actor.\n He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.\n He also studied cinema at the University of Southern California from 1991 to 1997.\n Collis first work was the assistant director for the Scott Derrickson's short \"Love in the Ruins\" (1995).\n In 1998, he played \"Crankshaft\" in Eric Koyanagi's \"Hundred Percent\".\n\n\n# Sinister (film)Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.\n It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.\n3. It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.       \n\n\n# Conrad BrooksConrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.\n He moved to Hollywood, California in 1948 to pursue a career in acting.\n He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"\n He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.\n He also has since gone on to write, produce and direct several films.\n\n\n# Doctor Strange (2016 film)Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.\n It is the fourteenth film of the Marvel Cinematic Universe (MCU).\n The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.\n In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.\n</code></pre> <p>The top1 in these chunks confirm that both Scott Derrickson and Ed Wood are American.</p> <p>For more details on indexing and querying, see the RAGEngine documentation.</p>"},{"location":"tutorial/rag.html#3-saving-and-loading-indices","title":"3. Saving and Loading Indices","text":"<p>Once you\u2019ve indexed your documents, you can save the index to reuse it later without reprocessing the data. This is useful for large datasets or production environments.</p>"},{"location":"tutorial/rag.html#save-the-index","title":"Save the Index","text":"<p>Modify the <code>rag_tutorial.py</code> script to save the index after indexing. Add the following line after <code>rag_engine.add</code>:</p> <pre><code># Save the index to disk\nrag_engine.save(output_path=\"./data/indexing\", corpus_id=\"hotpotqa_corpus\", index_type=\"vector\")\nprint(\"Index saved to ./data/indexing\")\n</code></pre>"},{"location":"tutorial/rag.html#load-the-index","title":"Load the Index","text":"<p>To load the saved index and query it, create a new script called <code>rag_load.py</code>:</p> <pre><code>import os\nimport json\n\nfrom dotenv import load_dotenv\n\nfrom evoagentx.rag.schema import Query\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# Load environment variables\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure storage (SQLite for metadata, FAISS for vectors)\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# Configure RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=50),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=3, similarity_cutoff=0.3)\n)\n\n# Initialize RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\nprint(\"RAGEngine is ready to go!\")\n\n# Load the saved index\nrag_engine.load(source=\"./data/indexing\", corpus_id=\"hotpotqa_corpus\", index_type=\"vector\")\nprint(\"Index loaded successfully!\")\n\n# Query the loaded index\nquery = Query(query_str=\"Were Scott Derrickson and Ed Wood of the same nationality?\", top_k=3)\nresult = rag_engine.query(query, corpus_id=\"hotpotqa_corpus\")\n\n# Print the retrieved chunks\nprint(\"\\nRetrieved answers:\")\nfor i, chunk in enumerate(result.corpus.chunks, 1):\n    print(f\"{i}. {chunk.text}\")\n\n# Clean up\nrag_engine.clear(corpus_id=\"hotpotqa_corpus\")\n</code></pre>"},{"location":"tutorial/rag.html#run-the-script_1","title":"Run the Script","text":"<p>Run the script:</p> <pre><code>python rag_load.py\n</code></pre>"},{"location":"tutorial/rag.html#whats-happening-here_2","title":"What\u2019s Happening Here?","text":"<ul> <li>Saving: The <code>save</code> method stores the vector index and metadata to the <code>./data/indexing</code> directory.</li> <li>Loading: The <code>load</code> method reconstructs the index from the saved files, making it ready for querying without re-indexing.</li> <li>Querying: We run the same query as before, and the results should be identical.</li> <li>Cleanup: We clear the index to keep things tidy.</li> </ul>"},{"location":"tutorial/rag.html#expected-output_1","title":"Expected Output","text":"<p>The output should be the same as in Section 2, confirming that the loaded index works correctly.</p> <p>For more details on saving and loading, see the RAGEngine documentation.</p>"},{"location":"tutorial/rag.html#next-steps","title":"Next Steps","text":"<p>Congratulations! You\u2019ve built your first RAG system with <code>RAGEngine</code>. Here are some ideas to explore next: - Try indexing a larger dataset or different file types (e.g., PDFs). - Experiment with different chunk sizes or embedding models (e.g., Hugging Face). - Integrate <code>RAGEngine</code> with an EvoAgentX agent to answer questions automatically.</p> <p>For a complete example, refer to the RAGEngine example.</p> <p>Happy building with EvoAgentX!</p>"},{"location":"tutorial/sew_optimizer.html","title":"SEW Optimizer Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running the SEW (Self-Evolving Workflow) optimizer in EvoAgentX. We'll use the HumanEval benchmark as an example to demonstrate how to optimize a multi-agent workflow.</p>"},{"location":"tutorial/sew_optimizer.html#1-overview","title":"1. Overview","text":"<p>The SEW optimizer is a powerful tool in EvoAgentX that enables you to:</p> <ul> <li>Automatically optimize multi-agent workflows (prompts and workflow structure)</li> <li>Evaluate optimization results on benchmark datasets</li> <li>Support different workflow representation scheme (Python, Yaml, BPMN, etc.)</li> </ul>"},{"location":"tutorial/sew_optimizer.html#2-setting-up-the-environment","title":"2. Setting Up the Environment","text":"<p>First, let's import the necessary modules for setting up the SEW optimizer:</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SEWWorkFlowGraph \nfrom evoagentx.agents import AgentManager\nfrom evoagentx.benchmark import HumanEval \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.optimizers import SEWOptimizer \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"tutorial/sew_optimizer.html#configure-the-llm-model","title":"Configure the LLM Model","text":"<p>Similar to other components in EvoAgentX, you'll need a valid OpenAI API key to initialize the LLM. </p> <pre><code>llm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre>"},{"location":"tutorial/sew_optimizer.html#3-setting-up-the-components","title":"3. Setting Up the Components","text":""},{"location":"tutorial/sew_optimizer.html#step-1-initialize-the-sew-workflow","title":"Step 1: Initialize the SEW Workflow","text":"<p>The SEW workflow is the core component that will be optimized. It represents a sequential workflow that aims to solve the code generation task. </p> <pre><code>sew_graph = SEWWorkFlowGraph(llm_config=llm_config)\nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(sew_graph)\n</code></pre>"},{"location":"tutorial/sew_optimizer.html#step-2-prepare-the-benchmark","title":"Step 2: Prepare the Benchmark","text":"<p>For this tutorial, we'll use a modified version of the HumanEval benchmark that splits the test data into development and test sets:</p> <pre><code>class HumanEvalSplits(HumanEval):\n    def _load_data(self):\n        # load the original test data \n        super()._load_data()\n        # split the data into dev and test\n        import numpy as np \n        np.random.seed(42)\n        num_dev_samples = int(len(self._test_data) * 0.2)\n        random_indices = np.random.permutation(len(self._test_data))\n        self._dev_data = [self._test_data[i] for i in random_indices[:num_dev_samples]]\n        self._test_data = [self._test_data[i] for i in random_indices[num_dev_samples:]]\n\n# Initialize the benchmark\nhumaneval = HumanEvalSplits()\n</code></pre> <p>The SEWOptimizer will evaluate the performance on the development set by default. Please make sure the benchmark has a development set properly set up. You can either:    - Use a benchmark that already provides a development set (like HotPotQA)    - Split your dataset into development and test sets (like in the HumanEvalSplits example above)    - Implement a custom benchmark with development set support</p>"},{"location":"tutorial/sew_optimizer.html#step-3-set-up-the-evaluator","title":"Step 3: Set Up the Evaluator","text":"<p>The evaluator is responsible for assessing the performance of the workflow during optimization. For more detailed information about how to set up and use the evaluator, please refer to the Benchmark and Evaluation Tutorial.</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    # convert raw example to the expected input for the SEW workflow\n    return {\"question\": example[\"prompt\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"tutorial/sew_optimizer.html#4-configuring-and-running-the-sew-optimizer","title":"4. Configuring and Running the SEW Optimizer","text":"<p>The SEW optimizer can be configured with various parameters to control the optimization process:</p> <pre><code>optimizer = SEWOptimizer(\n    graph=sew_graph,           # The workflow graph to optimize\n    evaluator=evaluator,       # The evaluator for performance assessment\n    llm=llm,                   # The language model\n    max_steps=10,             # Maximum optimization steps\n    eval_rounds=1,            # Number of evaluation rounds per step\n    repr_scheme=\"python\",     # Representation scheme for the workflow\n    optimize_mode=\"prompt\",   # What aspect to optimize (prompt/structure/all)\n    order=\"zero-order\"        # Optimization algorithm order (zero-order/first-order)\n)\n</code></pre>"},{"location":"tutorial/sew_optimizer.html#running-the-optimization","title":"Running the Optimization","text":"<p>To start the optimization process:</p> <pre><code># Optimize the SEW workflow\noptimizer.optimize(dataset=humaneval)\n\n# Evaluate the optimized workflow\nwith suppress_logger_info():\n    metrics = optimizer.evaluate(dataset=humaneval, eval_mode=\"test\")\nprint(\"Evaluation metrics: \", metrics)\n\n# Save the optimized SEW workflow\noptimizer.save(\"debug/optimized_sew_workflow.json\")\n</code></pre> <p>For a complete working example, please refer to sew_optimizer.py.</p>"},{"location":"tutorial/textgrad_optimizer.html","title":"TextGrad Optimizer Tutorial","text":"<p>This tutorial will guide you through the process of setting up and running the TextGrad optimizer in EvoAgentX. We'll use the MATH dataset as an example to demonstrate how to optimize the prompts and system prompts in a workflow.</p>"},{"location":"tutorial/textgrad_optimizer.html#1-textgrad","title":"1. TextGrad","text":"<p>TextGrad uses textual feedback from LLM to improve text variables. In EvoAgentX, we use TextGrad to optimize  agents' prompts and system prompts. For more information on TextGrad, see their paper and GitHub.</p>"},{"location":"tutorial/textgrad_optimizer.html#2-textgrad-optimizer","title":"2. TextGrad Optimizer","text":"<p>The TextGrad optimizer in EvoAgentX enables you to:</p> <ul> <li>Automatically optimize multi-agent workflows (prompts and/or system prompts)</li> <li>Evaluate optimization results on datasets</li> </ul>"},{"location":"tutorial/textgrad_optimizer.html#3-setting-up-the-environment","title":"3. Setting Up the Environment","text":"<p>First, let's import the necessary modules for setting up the TextGrad optimizer:</p> <pre><code>from evoagentx.benchmark import MATH\nfrom evoagentx.optimizers import TextGradOptimizer\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SequentialWorkFlowGraph\nfrom evoagentx.core.callbacks import suppress_logger_info\nfrom evoagentx.prompts import StringTemplate\n</code></pre>"},{"location":"tutorial/textgrad_optimizer.html#configure-the-llm-model","title":"Configure the LLM Model","text":"<p>You'll need a valid API key to initialize the LLM. See Quickstart for more details on how to set up your API key.</p> <p><code>TextGradOptimizer</code> allows the use of different LLMs for workflow execution and optimization. For example, we can use GPT 4o-mini for workflow execution and GPT 4o for optimization.</p> <pre><code>executor_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your_openai_api_key\")\nexecutor_llm = OpenAILLM(config=executor_config)\n\noptimizer_config = OpenAILLMConfig(model=\"gpt-4o\", openai_key=\"your_openai_api_key\")\noptimizer_llm = OpenAILLM(config=optimizer_config)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer.html#3-setting-up-the-components","title":"3. Setting Up the Components","text":""},{"location":"tutorial/textgrad_optimizer.html#step-1-initialize-the-workflow","title":"Step 1: Initialize the Workflow","text":"<p><code>TextGradOptimizer</code> only supports <code>SequentialWorkFlowGraph</code> and a specific variant of <code>WorkFlowGraph</code>. The workflow graph must have exactly one agent per node and each agent must only have one action. See Workflow Graph for more information on <code>SequentialWorkFlowGraph</code> and <code>WorkFlowGraph</code>. For this example, let us create the simplest workflow with only a single node.</p> <pre><code>math_graph_data = {\n    \"goal\": r\"Answer the math question. The answer should be in box format, e.g., \\boxed{123}\",\n    \"tasks\": [\n        {\n            \"name\": \"answer_generate\",\n            \"description\": \"Answer generation for Math.\",\n            \"inputs\": [\n                {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"The problem to solve.\"}\n            ],\n            \"outputs\": [\n                {\"name\": \"answer\", \"type\": \"str\", \"required\": True, \"description\": \"The generated answer.\"}\n            ],\n            \"prompt_template\": StringTemplate(instruction=\"Answer the math question. The answer should be in box format, e.g., \\\\boxed{{123}}\"),\n            \"parse_mode\": \"str\"\n        }\n    ] \n}\n\nworkflow_graph = SequentialWorkFlowGraph.from_dict(math_graph_data)\n</code></pre> <p><code>TextGradOptimizer</code> requires each agent be configured with a prompt template, rather than specifying the prompt using a string. This allows for a clear separation between the part of the prompt intended for optimization (i.e. instruction) and those that should remain unchanged (e.g. context, demonstrations). For more information on prompt templates, see Prompt Template.</p>"},{"location":"tutorial/textgrad_optimizer.html#step-2-prepare-the-dataset","title":"Step 2: Prepare the dataset","text":"<p>For this tutorial, we will use the MATH dataset which consists of challenging competition mathematics problems, spanning various difficulty levels and subject areas. The dataset is split into 7.5K training problems and 5K test problems. For demonstration purpose, let's take a smaller subset of the dataset to speed up the validation and evaluation process.</p> <pre><code>class MathSplits(MATH):\n    def _load_data(self):\n        super()._load_data()\n        import numpy as np \n        np.random.seed(42)\n        permutation = np.random.permutation(len(self._test_data))\n        full_test_data = self._test_data\n        # randomly select 10 samples for train, 40 for dev and 100 for test\n        self._train_data = [full_test_data[idx] for idx in permutation[:10]]\n        self._dev_data = [full_test_data[idx] for idx in permutation[10:50]]\n        self._test_data = [full_test_data[idx] for idx in permutation[50:150]]\n\nmath_splits = MathSplits()\n</code></pre> <p>During optimization, the <code>TextGradOptimizer</code> will evaluate the performance on the development set by default. Please make sure the dataset has a development set properly set up (i.e., <code>benchmark._dev_data</code> is not None). You can either:    - Use a dataset that already provides a development set    - Split your dataset to create a development set (like in the example above)    - Implement a custom dataset (inherits from <code>evoagentx.benchmark.Benchmark</code>) that properly sets up the development set. </p>"},{"location":"tutorial/textgrad_optimizer.html#step-3-set-up-the-evaluator","title":"Step 3: Set Up the Evaluator","text":"<p>The evaluator is responsible for assessing the performance of the workflow during optimization. For more detailed information about how to set up and use the evaluator, please refer to the Benchmark and Evaluation Tutorial.</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    return {\"problem\": example[\"problem\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer.html#4-configuring-and-running-the-textgrad-optimizer","title":"4. Configuring and Running the TextGrad Optimizer","text":"<p>The TextGradOptimizer can be configured with various parameters to control the optimization process:</p> <ul> <li><code>graph</code>: The workflow graph to optimize</li> <li><code>optimize_mode</code>: The mode of optimization:<ul> <li>\"all\": optimize both instruction prompts and system prompts</li> <li>\"instruction\": optimize only the instruction prompts</li> <li>\"system_prompt\": optimize only the system prompts</li> </ul> </li> <li><code>executor_llm</code>: The LLM used to execute the workflow</li> <li><code>optimizer_llm</code>: The LLM used to optimize the workflow</li> <li><code>batch_size</code>: The batch size for optimization</li> <li><code>max_steps</code>: The maximum number of optimization steps</li> <li><code>evaluator</code>: The evaluator to perform evaluation during optimization.</li> <li><code>eval_interval</code>: The number of steps between evaluations</li> <li><code>eval_rounds</code>: The number of evaluation rounds</li> <li><code>eval_config</code>: The evaluation configuration during optimization (passed to <code>TextGradOptimizer.evaluate()</code>). For example, if we don't want to evaluate on the entire development set, we can set  <code>eval_config = {\"sample_k\": 100}</code> to only evaluate on 100 random samples from the development set.</li> <li><code>save_interval</code>: The number of steps between saving the workflow graph</li> <li><code>save_path</code>: The path to save the workflow graph</li> <li><code>rollback</code>: Whether to rollback to the best workflow graph during optimization</li> <li><code>constraints</code>: An optional list of constraints for optimization. For example, \"The system prompt must not exceed 100 words\".</li> </ul> <pre><code>textgrad_optimizer = TextGradOptimizer(\n    graph=workflow_graph, \n    optimize_mode=\"all\",\n    executor_llm=executor_llm, \n    optimizer_llm=optimizer_llm,\n    batch_size=3,\n    max_steps=20,\n    evaluator=evaluator,\n    eval_every_n_steps=1,\n    eval_rounds=1,\n    save_interval=None,\n    save_path=\"./\",\n    rollback=True,\n    constraints=[]\n)\n</code></pre>"},{"location":"tutorial/textgrad_optimizer.html#running-the-optimization","title":"Running the Optimization","text":"<p>To start the optimization process: <pre><code>textgrad_optimizer.optimize(dataset=math_splits, seed=8)\n</code></pre> The <code>seed</code> is used for shuffling the training data. The training data is automatically re-shuffled every epoch. If <code>seed</code> is provided, the effective seed for shuffling the training data is <code>seed + epoch</code>.</p> <p>The final graph at the end of the optimization is not necessarily the best graph. If you wish to restore the graph that performed best on the development set, simply call <pre><code>textgrad_optimizer.restore_best_graph()\n</code></pre></p> <p>We can evaluate the workflow again to see the improvement after optimization. <pre><code>with suppress_logger_info():\n    result = textgrad_optimizer.evaluate(dataset=math_splits, eval_mode=\"test\")\nprint(f\"Evaluation result (after optimization):\\n{result}\")\n</code></pre></p> <p><code>TextGradOptimizer</code> always saves the final workflow graph and the best workflow graph to <code>save_path</code>. It also saves graphs during optimization if <code>save_interval</code> is not <code>None</code>. You can also save the workflow graph manually by calling <code>textgrad_optimizer.save()</code>.</p> <p>Note that <code>TextGradOptimizer</code> does not change the workflow structure but saving the workflow graph also saves the prompts and system prompts which will be different after optimization. Below is an example of a saved workflow graph after optimization using <code>TextGradOptimizer</code>.</p> <pre><code>{\n    \"class_name\": \"SequentialWorkFlowGraph\",\n    \"goal\": \"Answer the math question. The answer should be in box format, e.g., \\\\boxed{123}\",\n    \"tasks\": [\n        {\n            \"name\": \"answer_generate\",\n            \"description\": \"Answer generation for Math.\",\n            \"inputs\": [\n                {\n                    \"name\": \"problem\",\n                    \"type\": \"str\",\n                    \"description\": \"The problem to solve.\",\n                    \"required\": true\n                }\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"answer\",\n                    \"type\": \"str\",\n                    \"description\": \"The generated answer.\",\n                    \"required\": true\n                }\n            ],\n            \"prompt\": null,\n            \"prompt_template\": {\n                \"class_name\": \"StringTemplate\",\n                \"instruction\": \"To solve the math problem, follow these steps:\\n\\n1. **Contextual Overview**: Begin with a brief overview of the problem-solving strategy, using logical reasoning and mathematical principles to derive the solution. Include any relevant geometric or algebraic insights.\\n\\n2. **Key Steps Identification**: Break down the problem-solving process into distinct parts:\\n   - Identify the relevant mathematical operations and properties, such as symmetry, roots of unity, or trigonometric identities.\\n   - Perform the necessary calculations, ensuring each step logically follows from the previous one.\\n   - Present the final answer.\\n\\n3. **Conciseness and Clarity**: Provide a clear and concise explanation of your solution, avoiding unnecessary repetition. Use consistent formatting and notation throughout.\\n\\n4. **Mathematical Justification**: Explain the reasoning behind each step to ensure the solution is well-justified. Include explanations of reference angles, geometric interpretations, and any special conditions or edge cases.\\n\\n5. **Verification Step**: Include a quick verification step to confirm the accuracy of your calculations. Consider recalculating key values if initial assumptions were incorrect.\\n\\n6. **Visual Aids**: Where applicable, include diagrams or sketches to visually represent the problem and solution, enhancing understanding.\\n\\n7. **Final Answer Presentation**: Present the final answer clearly and ensure it is boxed, reflecting the correct solution. Verify that it aligns with the problem's requirements and any known correct solutions.\"\n            },\n            \"system_prompt\": \"You are a math-focused assistant dedicated to providing clear, concise, and educational solutions to mathematical problems. Your goal is to deliver structured and pedagogically sound explanations, ensuring mathematical accuracy and logical reasoning. Begin with a brief overview of the problem-solving approach, followed by detailed calculations, and conclude with a verification step. Use precise mathematical notation and consider potential edge cases. Present the final answer clearly, using the specified format, and incorporate visual aids or analogies where appropriate to enhance understanding and engagement. \\n\\nExplicitly include geometric explanations when applicable, describing the geometric context and relationships. Emphasize the importance of visual aids, such as diagrams or sketches, to enhance understanding. Ensure consistency in formatting and mathematical notation. Provide a brief explanation of the reference angle concept and its significance. Include contextual explanations of trigonometric identities and their applications. Critically evaluate initial assumptions and verify geometric properties before proceeding. Highlight the use of symmetry and conjugate pairs in complex numbers. Encourage re-evaluation and verification of steps, ensuring logical flow and clarity. Focus on deriving the correct answer and consider problem-specific strategies or known techniques.\",\n            \"parse_mode\": \"str\",\n            \"parse_func\": null,\n            \"parse_title\": null\n        }\n    ]\n}\n</code></pre> <p>For a complete working example, please refer to examples/textgrad/math_textgrad.py. Additional TextGrad optimization scripts for other datasets (e.g., <code>hotpotqa_textgrad.py</code> and <code>mbqq_textgrad.py</code>) are available in the examples/optimization/textgrad directory.</p>"},{"location":"tutorial/tools.html","title":"Working with Tools in EvoAgentX","text":"<p>This tutorial walks you through using EvoAgentX's powerful tool ecosystem. Tools allow agents to interact with the external world, perform computations, and access information. We'll cover:</p> <ol> <li>Understanding the Tool Architecture: Learn about the base Tool class and its functionality</li> <li>Code Interpreters: Execute Python code safely using Python and Docker interpreters</li> <li>Search Tools: Access information from the web using Wikipedia and Google search tools</li> <li>MCP Tools: Connect to external services using the Model Context Protocol</li> </ol> <p>By the end of this tutorial, you'll understand how to leverage these tools in your own agents and workflows.</p>"},{"location":"tutorial/tools.html#1-understanding-the-tool-architecture","title":"1. Understanding the Tool Architecture","text":"<p>At the core of EvoAgentX's tool ecosystem is the <code>Tool</code> base class, which provides a standardized interface for all tools. </p> <pre><code>from evoagentx.tools.tool import Tool\n</code></pre> <p>The <code>Tool</code> class implements three key methods:</p> <ul> <li><code>get_tool_schemas()</code>: Returns OpenAI-compatible function schemas for the tool</li> <li><code>get_tools()</code>: Returns a list of callable functions that the tool provides</li> <li><code>get_tool_descriptions()</code>: Returns descriptions of the tool's functionality</li> </ul> <p>All tools in EvoAgentX extend this base class, ensuring a consistent interface for agents to use them.</p>"},{"location":"tutorial/tools.html#key-concepts","title":"Key Concepts","text":"<ul> <li>Tool Integration: Tools seamlessly integrate with agents via function calling protocols</li> <li>Schemas: Each tool provides schemas that describe its functionality, parameters, and outputs</li> <li>Modularity: Tools can be easily added to any agent that supports function calling</li> </ul>"},{"location":"tutorial/tools.html#2-code-interpreters","title":"2. Code Interpreters","text":"<p>EvoAgentX provides two main code interpreter tools:</p> <ol> <li>PythonInterpreter: Executes Python code in a controlled environment</li> <li>DockerInterpreter: Executes code within isolated Docker containers</li> </ol>"},{"location":"tutorial/tools.html#21-pythoninterpreter","title":"2.1 PythonInterpreter","text":"<p>The PythonInterpreter provides a secure environment for executing Python code with fine-grained control over imports, directory access, and execution context. It uses a sandboxing approach to restrict potentially harmful operations.</p>"},{"location":"tutorial/tools.html#211-setup","title":"2.1.1 Setup","text":"<pre><code>from evoagentx.tools.interpreter_python import PythonInterpreter\n\n# Initialize with specific allowed imports and directory access\ninterpreter = PythonInterpreter(\n    project_path=\".\",  # Default is current directory\n    directory_names=[\"examples\", \"evoagentx\"],\n    allowed_imports={\"os\", \"sys\", \"math\", \"random\", \"datetime\"}\n)\n</code></pre>"},{"location":"tutorial/tools.html#212-available-methods","title":"2.1.2 Available Methods","text":"<p>The <code>PythonInterpreter</code> provides the following callable methods:</p>"},{"location":"tutorial/tools.html#method-1-executecode-language","title":"Method 1: execute(code, language)","text":"<p>Description: Executes Python code directly in a secure environment.</p> <p>Usage Example: <pre><code># Execute a simple code snippet\nresult = interpreter.execute(\"\"\"\nprint(\"Hello, World!\")\nimport math\nprint(f\"The value of pi is: {math.pi:.4f}\")\n\"\"\", \"python\")\n\nprint(result)\n</code></pre></p> <p>Return Type: <code>str</code></p> <p>Sample Return: <pre><code>Hello, World!\nThe value of pi is: 3.1416\n</code></pre></p>"},{"location":"tutorial/tools.html#method-2-execute_scriptfile_path-language","title":"Method 2: execute_script(file_path, language)","text":"<p>Description: Executes a Python script file in a secure environment.</p> <p>Usage Example: <pre><code># Execute a Python script file\nscript_path = \"examples/hello_world.py\"\nscript_result = interpreter.execute_script(script_path, \"python\")\nprint(script_result)\n</code></pre></p> <p>Return Type: <code>str</code></p> <p>Sample Return: <pre><code>Running hello_world.py...\nHello from the script file!\nScript execution completed.\n</code></pre></p>"},{"location":"tutorial/tools.html#213-setup-hints","title":"2.1.3 Setup Hints","text":"<ul> <li> <p>Project Path: The <code>project_path</code> parameter should point to the root directory of your project to ensure proper file access. Default is the current directory (\".\").</p> </li> <li> <p>Directory Names: The <code>directory_names</code> list specifies which directories within your project can be imported from. This is important for security to prevent unauthorized access. Default is an empty list <code>[]</code>.</p> </li> <li> <p>Allowed Imports: The <code>allowed_imports</code> set restricts which Python modules can be imported in executed code. Default is an empty list <code>[]</code>.</p> </li> <li>Important: If <code>allowed_imports</code> is set to an empty list, no import restrictions are applied.</li> <li>When specified, add only the modules you consider safe:</li> </ul> <pre><code># Example with restricted imports\ninterpreter = PythonInterpreter(\n    project_path=os.getcwd(),\n    directory_names=[\"examples\", \"evoagentx\", \"tests\"],\n    allowed_imports={\n        \"os\", \"sys\", \"time\", \"datetime\", \"math\", \"random\", \n        \"json\", \"csv\", \"re\", \"collections\", \"itertools\"\n    }\n)\n\n# Example with no import restrictions\ninterpreter = PythonInterpreter(\n    project_path=os.getcwd(),\n    directory_names=[\"examples\", \"evoagentx\"],\n    allowed_imports=[]  # Allows any module to be imported\n)\n</code></pre>"},{"location":"tutorial/tools.html#22-dockerinterpreter","title":"2.2 DockerInterpreter","text":"<p>The DockerInterpreter executes code in isolated Docker containers, providing maximum security and environment isolation. It allows safe execution of potentially risky code with custom environments, dependencies, and complete resource isolation. Docker must be installed and running on your machine to use this tool.</p>"},{"location":"tutorial/tools.html#221-setup","title":"2.2.1 Setup","text":"<pre><code>from evoagentx.tools.interpreter_docker import DockerInterpreter\n\n# Initialize with a specific Docker image\ninterpreter = DockerInterpreter(\n    image_tag=\"fundingsocietiesdocker/python3.9-slim\",\n    print_stdout=True,\n    print_stderr=True,\n    container_directory=\"/app\"\n)\n</code></pre>"},{"location":"tutorial/tools.html#222-available-methods","title":"2.2.2 Available Methods","text":"<p>The <code>DockerInterpreter</code> provides the following callable methods:</p>"},{"location":"tutorial/tools.html#method-1-executecode-language_1","title":"Method 1: execute(code, language)","text":"<p>Description: Executes code inside a Docker container.</p> <p>Usage Example: <pre><code># Execute Python code in a Docker container\nresult = interpreter.execute(\"\"\"\nimport platform\nprint(f\"Python version: {platform.python_version()}\")\nprint(f\"Platform: {platform.system()} {platform.release()}\")\n\"\"\", \"python\")\n\nprint(result)\n</code></pre></p> <p>Return Type: <code>str</code></p> <p>Sample Return: <pre><code>Python version: 3.9.16\nPlatform: Linux 5.15.0-1031-azure\n</code></pre></p>"},{"location":"tutorial/tools.html#method-2-execute_scriptfile_path-language_1","title":"Method 2: execute_script(file_path, language)","text":"<p>Description: Executes a script file inside a Docker container.</p> <p>Usage Example: <pre><code># Execute a Python script file in Docker\nscript_path = \"examples/docker_test.py\"\nscript_result = interpreter.execute_script(script_path, \"python\")\nprint(script_result)\n</code></pre></p> <p>Return Type: <code>str</code></p> <p>Sample Return: <pre><code>Running container with script: /app/script_12345.py\nHello from the Docker container!\nContainer execution completed.\n</code></pre></p>"},{"location":"tutorial/tools.html#223-setup-hints","title":"2.2.3 Setup Hints","text":"<ul> <li> <p>Docker Requirements: Ensure Docker is installed and running on your system before using this interpreter.</p> </li> <li> <p>Image Management: You need to provide either an <code>image_tag</code> or a <code>dockerfile_path</code>, not both:</p> </li> <li> <p>Option 1: Using an existing image <pre><code>interpreter = DockerInterpreter(\n    image_tag=\"python:3.9-slim\",  # Uses an existing Docker Hub image\n    container_directory=\"/app\"\n)\n</code></pre></p> </li> <li> <p>Option 2: Building from a Dockerfile <pre><code>interpreter = DockerInterpreter(\n    dockerfile_path=\"path/to/Dockerfile\",  # Builds a custom image\n    image_tag=\"my-custom-image-name\",      # Name for the built image\n    container_directory=\"/app\"\n)\n</code></pre></p> </li> <li> <p>File Access:</p> </li> <li>To make local files available in the container, use the <code>host_directory</code> parameter:   <pre><code>interpreter = DockerInterpreter(\n    image_tag=\"python:3.9-slim\",\n    host_directory=\"/path/to/local/files\",\n    container_directory=\"/app/data\"\n)\n</code></pre></li> <li> <p>This mounts the local directory to the specified container directory, making all files accessible.</p> </li> <li> <p>Container Lifecycle:</p> </li> <li>The Docker container is created when you initialize the interpreter and removed when the interpreter is destroyed.</li> <li> <p>For long-running sessions, you can set <code>print_stdout</code> and <code>print_stderr</code> to see real-time output.</p> </li> <li> <p>Troubleshooting:</p> </li> <li>If you encounter permission issues, ensure your user has Docker privileges.</li> <li>For network-related errors, check if your Docker daemon has proper network access.</li> </ul>"},{"location":"tutorial/tools.html#3-search-tools","title":"3. Search Tools","text":"<p>EvoAgentX provides several search tools to retrieve information from various sources:</p> <ol> <li>SearchWiki: Search Wikipedia for information</li> <li>SearchGoogle: Search Google using the official API</li> <li>SearchGoogleFree: Search Google without requiring an API key</li> </ol>"},{"location":"tutorial/tools.html#31-searchwiki","title":"3.1 SearchWiki","text":"<p>The SearchWiki tool retrieves information from Wikipedia articles, providing summaries, full content, and metadata. It offers a straightforward way to incorporate encyclopedic knowledge into your agents without complex API setups.</p>"},{"location":"tutorial/tools.html#311-setup","title":"3.1.1 Setup","text":"<pre><code>from evoagentx.tools.search_wiki import SearchWiki\n\n# Initialize with custom parameters\nwiki_search = SearchWiki(max_sentences=3)\n</code></pre>"},{"location":"tutorial/tools.html#312-available-methods","title":"3.1.2 Available Methods","text":"<p>The <code>SearchWiki</code> provides the following callable method:</p>"},{"location":"tutorial/tools.html#method-searchquery","title":"Method: search(query)","text":"<p>Description: Searches Wikipedia for articles matching the query.</p> <p>Usage Example: <pre><code># Search Wikipedia for information\nresults = wiki_search.search(\n    query=\"artificial intelligence agent architecture\"\n)\n\n# Process the results\nfor i, result in enumerate(results.get(\"results\", [])):\n    print(f\"Result {i+1}: {result['title']}\")\n    print(f\"Summary: {result['summary']}\")\n    print(f\"URL: {result['url']}\")\n</code></pre></p> <p>Return Type: <code>dict</code></p> <p>Sample Return: <pre><code>{\n    \"results\": [\n        {\n            \"title\": \"Artificial intelligence\",\n            \"summary\": \"Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. AI applications include advanced web search engines, recommendation systems, voice assistants...\",\n            \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n        },\n        {\n            \"title\": \"Intelligent agent\",\n            \"summary\": \"In artificial intelligence, an intelligent agent (IA) is anything which can perceive its environment, process those perceptions, and respond in pursuit of its own goals...\",\n            \"url\": \"https://en.wikipedia.org/wiki/Intelligent_agent\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"tutorial/tools.html#32-searchgoogle","title":"3.2 SearchGoogle","text":"<p>The SearchGoogle tool enables web searches through Google's official Custom Search API, providing high-quality search results with content extraction. It requires API credentials but offers more reliable and comprehensive search capabilities.</p>"},{"location":"tutorial/tools.html#321-setup","title":"3.2.1 Setup","text":"<pre><code>from evoagentx.tools.search_google import SearchGoogle\n\n# Initialize with custom parameters\ngoogle_search = SearchGoogle(\n    num_search_pages=3,\n    max_content_words=200\n)\n</code></pre>"},{"location":"tutorial/tools.html#322-available-methods","title":"3.2.2 Available Methods","text":"<p>The <code>SearchGoogle</code> provides the following callable method:</p>"},{"location":"tutorial/tools.html#method-searchquery_1","title":"Method: search(query)","text":"<p>Description: Searches Google for content matching the query.</p> <p>Usage Example: <pre><code># Search Google for information\nresults = google_search.search(\n    query=\"evolutionary algorithms for neural networks\"\n)\n\n# Process the results\nfor i, result in enumerate(results.get(\"results\", [])):\n    print(f\"Result {i+1}: {result['title']}\")\n    print(f\"URL: {result['url']}\")\n    print(f\"Content: {result['content'][:150]}...\")\n</code></pre></p> <p>Return Type: <code>dict</code></p> <p>Sample Return: <pre><code>{\n    \"results\": [\n        {\n            \"title\": \"Evolutionary Algorithms for Neural Networks - A Systematic Review\",\n            \"url\": \"https://example.com/paper1\",\n            \"content\": \"This paper provides a comprehensive review of evolutionary algorithms applied to neural network optimization. Key approaches include genetic algorithms, particle swarm optimization, and differential evolution...\"\n        },\n        {\n            \"title\": \"Applying Genetic Algorithms to Neural Network Training\",\n            \"url\": \"https://example.com/article2\",\n            \"content\": \"Genetic algorithms offer a powerful approach to optimizing neural network architectures and weights. This article explores how evolutionary computation can overcome limitations of gradient-based methods...\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"tutorial/tools.html#323-setup-hints","title":"3.2.3 Setup Hints","text":"<ul> <li> <p>API Requirements: This tool requires Google Custom Search API credentials. Set them in your environment:   <pre><code># In your .env file or environment variables\nGOOGLE_API_KEY=your_google_api_key_here\nGOOGLE_SEARCH_ENGINE_ID=your_search_engine_id_here\n</code></pre></p> </li> <li> <p>Obtaining Credentials:</p> </li> <li>Create a project in Google Cloud Console</li> <li>Enable the Custom Search API</li> <li>Create API credentials</li> <li>Set up a Custom Search Engine at https://cse.google.com/cse/</li> </ul>"},{"location":"tutorial/tools.html#33-searchgooglefree","title":"3.3 SearchGoogleFree","text":"<p>The SearchGoogleFree tool provides web search capability without requiring any API keys or authentication. It offers a simpler alternative to the official Google API with basic search results suitable for most general queries.</p>"},{"location":"tutorial/tools.html#331-setup","title":"3.3.1 Setup","text":"<pre><code>from evoagentx.tools.search_google_f import SearchGoogleFree\n\n# Initialize the free Google search\ngoogle_free = SearchGoogleFree(\n    num_search_pages=3,\n    max_content_words=500\n)\n</code></pre>"},{"location":"tutorial/tools.html#332-available-methods","title":"3.3.2 Available Methods","text":"<p>The <code>SearchGoogleFree</code> provides the following callable method:</p>"},{"location":"tutorial/tools.html#method-searchquery_2","title":"Method: search(query)","text":"<p>Description: Searches Google for content matching the query without requiring an API key.</p> <p>Usage Example: <pre><code># Search Google without an API key\nresults = google_free.search(\n    query=\"reinforcement learning algorithms\"\n)\n\n# Process the results\nfor i, result in enumerate(results.get(\"results\", [])):\n    print(f\"Result {i+1}: {result['title']}\")\n    print(f\"URL: {result['url']}\")\n</code></pre></p> <p>Return Type: <code>dict</code></p> <p>Sample Return: <pre><code>{\n    \"results\": [\n        {\n            \"title\": \"Introduction to Reinforcement Learning Algorithms\",\n            \"url\": \"https://example.com/intro-rl\",\n            \"snippet\": \"A comprehensive overview of reinforcement learning algorithms including Q-learning, SARSA, and policy gradient methods.\"\n        },\n        {\n            \"title\": \"Top 10 Reinforcement Learning Algorithms for Beginners\",\n            \"url\": \"https://example.com/top-rl\",\n            \"snippet\": \"Learn about the most commonly used reinforcement learning algorithms with practical examples and implementation tips.\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"tutorial/tools.html#4-mcp-tools","title":"4. MCP Tools","text":"<p>The Model Context Protocol (MCP) toolkit provides a standardized way to connect to external services through the MCP protocol. It enables agents to access specialized tools like job search services, data processing utilities, and other MCP-compatible APIs without requiring direct integration of each service.</p>"},{"location":"tutorial/tools.html#41-mcptoolkit","title":"4.1 MCPToolkit","text":""},{"location":"tutorial/tools.html#411-setup","title":"4.1.1 Setup","text":"<pre><code>from evoagentx.tools.mcp import MCPToolkit\n\n# Initialize with a configuration file\nmcp_toolkit = MCPToolkit(config_path=\"examples/sample_mcp.config\")\n\n# Or initialize with a configuration dictionary\nconfig = {\n    \"mcpServers\": {\n        \"hirebase\": {\n            \"command\": \"uvx\",\n            \"args\": [\"hirebase-mcp\"],\n            \"env\": {\"HIREBASE_API_KEY\": \"your_api_key_here\"}\n        }\n    }\n}\nmcp_toolkit = MCPToolkit(config=config)\n</code></pre>"},{"location":"tutorial/tools.html#412-available-methods","title":"4.1.2 Available Methods","text":"<p>The <code>MCPToolkit</code> provides the following callable methods:</p>"},{"location":"tutorial/tools.html#method-1-get_tools","title":"Method 1: get_tools()","text":"<p>Description: Returns a list of all available tools from connected MCP servers.</p> <p>Usage Example: <pre><code># Get all available MCP tools\ntools = mcp_toolkit.get_tools()\n\n# Display available tools\nfor i, tool in enumerate(tools):\n    print(f\"Tool {i+1}: {tool.name}\")\n    print(f\"Description: {tool.descriptions[0]}\")\n</code></pre></p> <p>Return Type: <code>List[Tool]</code></p> <p>Sample Return: <pre><code>[MCPTool(name=\"HirebaseSearch\", descriptions=[\"Search for job information by providing keywords\"]), \n MCPTool(name=\"HirebaseAnalyze\", descriptions=[\"Analyze job market trends for given skills\"])]\n</code></pre></p>"},{"location":"tutorial/tools.html#method-2-disconnect","title":"Method 2: disconnect()","text":"<p>Description: Disconnects from all MCP servers and cleans up resources.</p> <p>Usage Example: <pre><code># When done with the MCP toolkit\nmcp_toolkit.disconnect()\n</code></pre></p> <p>Return Type: <code>None</code></p>"},{"location":"tutorial/tools.html#413-using-mcp-tools","title":"4.1.3 Using MCP Tools","text":"<p>Once you have obtained the tools from the MCPToolkit, you can use them like any other EvoAgentX tool:</p> <pre><code># Get all tools from the toolkit\ntools = mcp_toolkit.get_tools()\n\n# Find a specific tool\nhirebase_tool = None\nfor tool in tools:\n    if \"hire\" in tool.name.lower() or \"search\" in tool.name.lower():\n        hirebase_tool = tool\n        break\n\nif hirebase_tool:\n    # Use the tool to search for information\n    search_query = \"data scientist\"\n    result = hirebase_tool.tools[0](**{\"query\": search_query})\n\n    print(f\"Search results for '{search_query}':\")\n    print(result)\n</code></pre>"},{"location":"tutorial/tools.html#414-setup-hints","title":"4.1.4 Setup Hints","text":"<ul> <li> <p>Configuration File: The configuration file should follow the MCP protocol's server configuration format:   <pre><code>{\n    \"mcpServers\": {\n        \"serverName\": {\n            \"command\": \"executable_command\",\n            \"args\": [\"command_arguments\"],\n            \"env\": {\"ENV_VAR_NAME\": \"value\"}\n        }\n    }\n}\n</code></pre></p> </li> <li> <p>Server Types:</p> </li> <li>Command-based servers: Use the <code>command</code> field to specify an executable</li> <li> <p>URL-based servers: Use the <code>url</code> field to specify a server endpoint</p> </li> <li> <p>Connection Management:</p> </li> <li>Always call <code>disconnect()</code> when you're done using the MCPToolkit to free resources</li> <li> <p>Use a try-finally block for automatic cleanup:     <pre><code>try:\n    toolkit = MCPToolkit(config_path=\"config.json\")\n    tools = toolkit.get_tools()\n    # Use tools here\nfinally:\n    toolkit.disconnect()\n</code></pre></p> </li> <li> <p>Error Handling:</p> </li> <li>The MCPToolkit will log warning messages if it can't connect to servers</li> <li> <p>It's good practice to implement error handling around tool calls:     <pre><code>try:\n    result = tool.tools[0](**{\"query\": \"example query\"})\nexcept Exception as e:\n    print(f\"Error calling MCP tool: {str(e)}\")\n</code></pre></p> </li> <li> <p>Environment Variables:</p> </li> <li>API keys and other sensitive information can be provided via environment variables in the config</li> <li>You can also set them in your environment before running your application</li> </ul>"},{"location":"tutorial/tools.html#summary","title":"Summary","text":"<p>In this tutorial, we've explored the tool ecosystem in EvoAgentX:</p> <ol> <li>Tool Architecture: Understood the base Tool class and its standardized interface</li> <li>Code Interpreters: Learned how to execute Python code securely using both Python and Docker interpreters</li> <li>Search Tools: Discovered how to access web information using Wikipedia and Google search tools</li> <li>MCP Tools: Learned how to connect to external services using the Model Context Protocol</li> </ol> <p>Tools in EvoAgentX extend your agents' capabilities by providing access to external resources and computation. By combining these tools with agents and workflows, you can build powerful AI systems that can retrieve information, perform calculations, and interact with the world.</p> <p>For more advanced usage and customization options, refer to the API documentation and explore the examples in the repository. </p>"},{"location":"zh/index.html","title":"EvoAgentX","text":"<p> \u4e00\u4e2a\u7528\u4e8e\u8bc4\u4f30\u548c\u6f14\u8fdb\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u81ea\u52a8\u5316\u6846\u67b6\u3002 </p> <p> </p>"},{"location":"zh/index.html#-\u7b80\u4ecb","title":"\ud83d\ude80 \u7b80\u4ecb","text":"<p>EvoAgentX \u662f\u4e00\u4e2a\u5f00\u6e90\u6846\u67b6\uff0c\u65e8\u5728\u81ea\u52a8\u5316\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u751f\u6210\u3001\u6267\u884c\u3001\u8bc4\u4f30\u548c\u4f18\u5316\u3002\u901a\u8fc7\u5229\u7528\u5927\u8bed\u8a00\u6a21\u578b\uff08LLMs\uff09\uff0cEvoAgentX \u4f7f\u5f00\u53d1\u8005\u548c\u7814\u7a76\u4eba\u5458\u80fd\u591f\u5feb\u901f\u6784\u5efa\u3001\u6d4b\u8bd5\u548c\u90e8\u7f72\u591a\u4ee3\u7406\u7cfb\u7edf\uff0c\u8fd9\u4e9b\u7cfb\u7edf\u53ef\u4ee5\u968f\u7740\u65f6\u95f4\u63a8\u79fb\u5728\u590d\u6742\u6027\u548c\u80fd\u529b\u4e0a\u4e0d\u65ad\u589e\u957f\u3002</p>"},{"location":"zh/index.html#-\u4e3b\u8981\u7279\u6027","title":"\u2728 \u4e3b\u8981\u7279\u6027","text":"<ul> <li>\u7b80\u5355\u7684\u4ee3\u7406\u548c\u5de5\u4f5c\u6d41\u5b9a\u5236\uff1a\u4f7f\u7528\u81ea\u7136\u8bed\u8a00\u63d0\u793a\u8f7b\u677e\u521b\u5efa\u81ea\u5b9a\u4e49\u4ee3\u7406\u548c\u5de5\u4f5c\u6d41\u3002EvoAgentX \u8ba9\u4f60\u80fd\u591f\u8f7b\u677e\u5730\u5c06\u9ad8\u5c42\u6b21\u60f3\u6cd5\u8f6c\u5316\u4e3a\u53ef\u5de5\u4f5c\u7684\u7cfb\u7edf\u3002</li> <li>\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u6267\u884c\uff1a\u4ece\u7b80\u5355\u7684\u76ee\u6807\u63cf\u8ff0\u81ea\u52a8\u751f\u6210\u548c\u6267\u884c\u4ee3\u7406\u5de5\u4f5c\u6d41\uff0c\u51cf\u5c11\u591a\u4ee3\u7406\u7cfb\u7edf\u8bbe\u8ba1\u4e2d\u7684\u624b\u52a8\u5de5\u4f5c\u91cf\u3002</li> <li>\u5de5\u4f5c\u6d41\u4f18\u5316\uff1a\u96c6\u6210\u73b0\u6709\u5de5\u4f5c\u6d41\u4f18\u5316\u6280\u672f\uff0c\u901a\u8fc7\u8fed\u4ee3\u4f18\u5316\u63d0\u5347\u5de5\u4f5c\u6d41\u6027\u80fd\u3002</li> <li>\u57fa\u51c6\u6d4b\u8bd5\u4e0e\u8bc4\u4f30\uff1a\u5305\u542b\u5185\u7f6e\u57fa\u51c6\u6d4b\u8bd5\u548c\u6807\u51c6\u5316\u8bc4\u4f30\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u4e0d\u540c\u4efb\u52a1\u548c\u4ee3\u7406\u914d\u7f6e\u4e0b\u7684\u5de5\u4f5c\u6d41\u6548\u679c\u3002</li> <li>\u5de5\u4f5c\u6d41\u6267\u884c\u5de5\u5177\u5305\uff1a\u63d0\u4f9b\u6267\u884c\u590d\u6742\u5de5\u4f5c\u6d41\u6240\u9700\u7684\u4e00\u7cfb\u5217\u5de5\u5177\uff0c\u5982\u641c\u7d22\u7ec4\u4ef6\u548c\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u3002</li> </ul>"},{"location":"zh/index.html#-\u5de5\u4f5c\u539f\u7406","title":"\ud83d\udd0d \u5de5\u4f5c\u539f\u7406","text":"<p>EvoAgentX \u4f7f\u7528\u6a21\u5757\u5316\u67b6\u6784\uff0c\u5305\u542b\u4ee5\u4e0b\u6838\u5fc3\u7ec4\u4ef6\uff1a</p> <ol> <li>\u5de5\u4f5c\u6d41\u751f\u6210\u5668\uff1a\u6839\u636e\u4f60\u7684\u76ee\u6807\u521b\u5efa\u4ee3\u7406\u5de5\u4f5c\u6d41</li> <li>\u4ee3\u7406\u7ba1\u7406\u5668\uff1a\u5904\u7406\u4ee3\u7406\u7684\u521b\u5efa\u3001\u914d\u7f6e\u548c\u90e8\u7f72</li> <li>\u5de5\u4f5c\u6d41\u6267\u884c\u5668\uff1a\u9ad8\u6548\u8fd0\u884c\u5de5\u4f5c\u6d41\uff0c\u786e\u4fdd\u4ee3\u7406\u95f4\u6b63\u786e\u901a\u4fe1</li> <li>\u8bc4\u4f30\u5668\uff1a\u63d0\u4f9b\u6027\u80fd\u6307\u6807\u548c\u6539\u8fdb\u5efa\u8bae</li> <li>\u4f18\u5316\u5668\uff1a\u901a\u8fc7\u4e0d\u65ad\u6f14\u8fdb\u63d0\u5347\u5de5\u4f5c\u6d41\u6027\u80fd</li> </ol>"},{"location":"zh/index.html#-\u793e\u533a","title":"\ud83d\udc65 \u793e\u533a","text":"<ul> <li>Discord\uff1a\u52a0\u5165\u6211\u4eec\u7684 Discord \u9891\u9053 \u8fdb\u884c\u8ba8\u8bba\u548c\u83b7\u53d6\u652f\u6301</li> <li>GitHub\uff1a\u5728 GitHub \u4e0a\u4e3a\u9879\u76ee\u505a\u51fa\u8d21\u732e</li> <li>Email\uff1a\u901a\u8fc7 evoagentx.ai@gmail.com \u8054\u7cfb\u6211\u4eec</li> <li>\u5fae\u4fe1\uff1a\u901a\u8fc7 \u5fae\u4fe1 \u83b7\u53d6\u66f4\u65b0\u548c\u652f\u6301\u3002</li> </ul>"},{"location":"zh/index.html#-\u8d21\u732e","title":"\ud83e\udd1d \u8d21\u732e","text":"<p>\u6211\u4eec\u6b22\u8fce\u793e\u533a\u8d21\u732e\uff01\u8bf7\u53c2\u9605\u6211\u4eec\u7684\u8d21\u732e\u6307\u5357\u4e86\u89e3\u66f4\u591a\u8be6\u60c5\u3002</p>"},{"location":"zh/installation.html","title":"EvoAgentX \u5b89\u88c5\u6307\u5357","text":"<p>\u672c\u6307\u5357\u5c06\u5f15\u5bfc\u60a8\u5b8c\u6210\u5728\u7cfb\u7edf\u4e0a\u5b89\u88c5 EvoAgentX\u3001\u8bbe\u7f6e\u6240\u9700\u4f9d\u8d56\u9879\u4ee5\u53ca\u4e3a\u60a8\u7684\u9879\u76ee\u914d\u7f6e\u6846\u67b6\u7684\u5168\u8fc7\u7a0b\u3002</p>"},{"location":"zh/installation.html#\u524d\u7f6e\u6761\u4ef6","title":"\u524d\u7f6e\u6761\u4ef6","text":"<p>\u5728\u5b89\u88c5 EvoAgentX \u4e4b\u524d\uff0c\u8bf7\u786e\u4fdd\u60a8\u5177\u5907\u4ee5\u4e0b\u524d\u7f6e\u6761\u4ef6\uff1a</p> <ul> <li>Python 3.10 \u6216\u66f4\u9ad8\u7248\u672c  </li> <li>pip\uff08Python \u5305\u5b89\u88c5\u5668\uff09  </li> <li>Git\uff08\u7528\u4e8e\u514b\u9686\u4ed3\u5e93\uff09  </li> <li>Conda\uff08\u63a8\u8350\u7528\u4e8e\u73af\u5883\u7ba1\u7406\uff0c\u4f46\u53ef\u9009\uff09  </li> </ul>"},{"location":"zh/installation.html#\u5b89\u88c5\u65b9\u6cd5","title":"\u5b89\u88c5\u65b9\u6cd5","text":"<p>\u5b89\u88c5 EvoAgentX \u6709\u591a\u79cd\u65b9\u5f0f\u3002\u8bf7\u9009\u62e9\u6700\u7b26\u5408\u60a8\u9700\u6c42\u7684\u65b9\u6cd5\u3002</p>"},{"location":"zh/installation.html#\u65b9\u6cd5-1\u4f7f\u7528-pip\u63a8\u8350","title":"\u65b9\u6cd5 1\uff1a\u4f7f\u7528 pip\uff08\u63a8\u8350\uff09","text":"<p>\u5b89\u88c5 EvoAgentX \u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u4f7f\u7528 pip\uff1a</p> <pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre>"},{"location":"zh/installation.html#\u65b9\u6cd5-2\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\u5f00\u53d1\u8005\u4e13\u7528","title":"\u65b9\u6cd5 2\uff1a\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\uff08\u5f00\u53d1\u8005\u4e13\u7528\uff09","text":"<p>\u5982\u679c\u60a8\u5e0c\u671b\u4e3a EvoAgentX \u505a\u8d21\u732e\uff0c\u6216\u9700\u8981\u83b7\u53d6\u6700\u65b0\u7684\u5f00\u53d1\u7248\u672c\uff0c\u53ef\u4ee5\u76f4\u63a5\u4ece\u6e90\u4ee3\u7801\u5b89\u88c5\uff1a</p> <pre><code># \u514b\u9686\u4ed3\u5e93\ngit clone https://github.com/EvoAgentX/EvoAgentX/\n\n# \u8fdb\u5165\u9879\u76ee\u76ee\u5f55\ncd EvoAgentX\n\n# \u4ee5\u5f00\u53d1\u6a21\u5f0f\u5b89\u88c5\u5305\npip install -e .\n</code></pre>"},{"location":"zh/installation.html#\u65b9\u6cd5-3\u4f7f\u7528-conda-\u73af\u5883\u63a8\u8350\u7528\u4e8e\u9694\u79bb","title":"\u65b9\u6cd5 3\uff1a\u4f7f\u7528 Conda \u73af\u5883\uff08\u63a8\u8350\u7528\u4e8e\u9694\u79bb\uff09","text":"<p>\u5982\u679c\u60a8\u504f\u597d\u4f7f\u7528 Conda \u6765\u7ba1\u7406 Python \u73af\u5883\uff0c\u8bf7\u6309\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c\uff1a</p> <pre><code># \u521b\u5efa\u65b0\u7684conda\u73af\u5883\nconda create -n evoagentx python=3.10\n\n# \u6fc0\u6d3b\u73af\u5883\nconda activate evoagentx\n\n# \u5b89\u88c5\u5305\npip install -r requirements.txt\n# \u6216\u8005\u4ee5\u5f00\u53d1\u6a21\u5f0f\u5b89\u88c5\npip install -e .\n</code></pre>"},{"location":"zh/installation.html#\u9a8c\u8bc1\u5b89\u88c5","title":"\u9a8c\u8bc1\u5b89\u88c5","text":"<p>\u8981\u9a8c\u8bc1 EvoAgentX \u662f\u5426\u5df2\u6b63\u786e\u5b89\u88c5\uff0c\u8bf7\u8fd0\u884c\u4ee5\u4e0b Python \u4ee3\u7801\uff1a</p> <pre><code>import evoagentx\n\n# \u6253\u5370\u7248\u672c\nprint(evoagentx.__version__)\n</code></pre> <p>\u60a8\u5e94\u8be5\u80fd\u5728\u63a7\u5236\u53f0\u770b\u5230 EvoAgentX \u5f53\u524d\u7684\u7248\u672c\u53f7\u3002</p>"},{"location":"zh/quickstart.html","title":"EvoAgentX \u5feb\u901f\u5f00\u59cb\u6307\u5357","text":"<p>\u672c\u5feb\u901f\u5f00\u59cb\u6307\u5357\u5c06\u5f15\u5bfc\u4f60\u5b8c\u6210\u4f7f\u7528 EvoAgentX \u7684\u57fa\u7840\u6b65\u9aa4\u3002\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u4f60\u5c06\u5b66\u4e60\u5982\u4f55\uff1a 1. \u914d\u7f6e\u7528\u4e8e\u8bbf\u95ee LLM \u7684 API \u5bc6\u94a5 2. \u81ea\u52a8\u521b\u5efa\u5e76\u6267\u884c\u5de5\u4f5c\u6d41  </p>"},{"location":"zh/quickstart.html#\u5b89\u88c5","title":"\u5b89\u88c5","text":"<p><pre><code>pip install git+https://github.com/EvoAgentX/EvoAgentX.git\n</code></pre> \u8bf7\u53c2\u9605 \u5b89\u88c5\u6307\u5357 \u83b7\u53d6\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"zh/quickstart.html#api-\u5bc6\u94a5-\u548c-llm-\u8bbe\u7f6e","title":"API \u5bc6\u94a5 \u548c LLM \u8bbe\u7f6e","text":"<p>\u8981\u5728 EvoAgentX \u4e2d\u6267\u884c\u5de5\u4f5c\u6d41\uff0c\u9996\u5148\u9700\u8981\u914d\u7f6e\u7528\u4e8e\u8bbf\u95ee\u5927\u6a21\u578b\uff08LLM\uff09\u7684 API \u5bc6\u94a5\u3002\u63a8\u8350\u4ee5\u4e0b\u4e24\u79cd\u65b9\u5f0f\uff1a</p>"},{"location":"zh/quickstart.html#\u65b9\u6cd5\u4e00\u5728\u7ec8\u7aef\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf","title":"\u65b9\u6cd5\u4e00\uff1a\u5728\u7ec8\u7aef\u8bbe\u7f6e\u73af\u5883\u53d8\u91cf","text":"<p>\u6b64\u65b9\u6cd5\u76f4\u63a5\u5728\u7cfb\u7edf\u73af\u5883\u4e2d\u8bbe\u7f6e API \u5bc6\u94a5\u3002</p> <p>\u5bf9\u4e8e Linux/macOS\uff1a <pre><code>export OPENAI_API_KEY=&lt;\u4f60\u7684-openai-api-key&gt;\n</code></pre></p> <p>\u5bf9\u4e8e Windows \u547d\u4ee4\u63d0\u793a\u7b26\uff1a <pre><code>set OPENAI_API_KEY=&lt;\u4f60\u7684-openai-api-key&gt;\n</code></pre></p> <p>\u5bf9\u4e8e Windows PowerShell\uff1a <pre><code>$env:OPENAI_API_KEY=\"&lt;\u4f60\u7684-openai-api-key&gt;\"  # \u5f15\u53f7\u662f\u5fc5\u9700\u7684\n</code></pre></p> <p>\u8bbe\u7f6e\u5b8c\u6210\u540e\uff0c\u53ef\u5728 Python \u4e2d\u8fd9\u6837\u83b7\u53d6\uff1a <pre><code>import os\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre></p>"},{"location":"zh/quickstart.html#\u65b9\u6cd5\u4e8c\u4f7f\u7528-env-\u6587\u4ef6","title":"\u65b9\u6cd5\u4e8c\uff1a\u4f7f\u7528 <code>.env</code> \u6587\u4ef6","text":"<p>\u4e5f\u53ef\u4ee5\u5728\u9879\u76ee\u6839\u76ee\u5f55\u4e0b\u521b\u5efa <code>.env</code> \u6587\u4ef6\u6765\u5b58\u50a8 API \u5bc6\u94a5\u3002</p> <p>\u6587\u4ef6\u5185\u5bb9\u793a\u4f8b\uff1a <pre><code>OPENAI_API_KEY=&lt;\u4f60\u7684-openai-api-key&gt;\n</code></pre></p> <p>\u7136\u540e\u5728 Python \u4e2d\u4f7f\u7528 <code>python-dotenv</code> \u52a0\u8f7d\uff1a <pre><code>from dotenv import load_dotenv \nimport os \n\nload_dotenv()  # \u4ece .env \u6587\u4ef6\u52a0\u8f7d\u73af\u5883\u53d8\u91cf\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre></p> <p>\ud83d\udd10 \u63d0\u793a\uff1a\u5207\u52ff\u5c06 <code>.env</code> \u6587\u4ef6\u63d0\u4ea4\u5230\u516c\u5171\u5e73\u53f0\uff08\u5982 GitHub\uff09\uff0c\u8bf7\u5c06\u5176\u6dfb\u52a0\u5230 <code>.gitignore</code>\u3002</p>"},{"location":"zh/quickstart.html#\u5728-evoagentx-\u4e2d\u914d\u7f6e\u5e76\u4f7f\u7528-llm","title":"\u5728 EvoAgentX \u4e2d\u914d\u7f6e\u5e76\u4f7f\u7528 LLM","text":"<p>\u914d\u7f6e\u597d API \u5bc6\u94a5\u540e\uff0c\u53ef\u6309\u5982\u4e0b\u65b9\u5f0f\u521d\u59cb\u5316\u5e76\u4f7f\u7528 LLM\uff1a <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# \u4ece\u73af\u5883\u52a0\u8f7d API \u5bc6\u94a5\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u5b9a\u4e49 LLM \u914d\u7f6e\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",       # \u6307\u5b9a\u6a21\u578b\u540d\u79f0\n    openai_key=OPENAI_API_KEY, # \u76f4\u63a5\u4f20\u5165\u5bc6\u94a5\n    stream=True,               # \u542f\u7528\u6d41\u5f0f\u54cd\u5e94\n    output_response=True       # \u6253\u5370\u54cd\u5e94\u5230\u6807\u51c6\u8f93\u51fa\n)\n\n# \u521d\u59cb\u5316\u8bed\u8a00\u6a21\u578b\nllm = OpenAILLM(config=openai_config)\n\n# \u4ece LLM \u751f\u6210\u54cd\u5e94\nresponse = llm.generate(prompt=\"What is Agentic Workflow?\")\n</code></pre></p> <p>\u4f60\u53ef\u4ee5\u5728 LLM \u6a21\u5757\u6307\u5357 \u4e2d\u627e\u5230\u66f4\u591a\u5173\u4e8e\u652f\u6301\u7684 LLM \u7c7b\u578b\u53ca\u5176\u53c2\u6570\u7684\u8be6\u7ec6\u4fe1\u606f\u3002</p>"},{"location":"zh/quickstart.html#\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u6267\u884c","title":"\u81ea\u52a8\u5de5\u4f5c\u6d41\u751f\u6210\u4e0e\u6267\u884c","text":"<p>\u914d\u7f6e\u5b8c\u6210\u540e\uff0c\u5373\u53ef\u5728 EvoAgentX \u4e2d\u81ea\u52a8\u751f\u6210\u5e76\u6267\u884c\u667a\u80fd\u5de5\u4f5c\u6d41\u3002\u672c\u8282\u5c55\u793a\u751f\u6210\u5de5\u4f5c\u6d41\u3001\u5b9e\u4f8b\u5316\u4ee3\u7406\u5e76\u8fd0\u884c\u7684\u6838\u5fc3\u6b65\u9aa4\u3002</p> <p>\u9996\u5148\uff0c\u5bfc\u5165\u5fc5\u8981\u7684\u6a21\u5757\uff1a</p> <pre><code>from evoagentx.workflow import WorkFlowGenerator, WorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\n</code></pre>"},{"location":"zh/quickstart.html#\u7b2c\u4e00\u6b65\u751f\u6210\u5de5\u4f5c\u6d41\u4e0e\u4efb\u52a1\u56fe","title":"\u7b2c\u4e00\u6b65\uff1a\u751f\u6210\u5de5\u4f5c\u6d41\u4e0e\u4efb\u52a1\u56fe","text":"<p>\u4f7f\u7528 <code>WorkFlowGenerator</code> \u57fa\u4e8e\u81ea\u7136\u8bed\u8a00\u76ee\u6807\u81ea\u52a8\u521b\u5efa\u5de5\u4f5c\u6d41\uff1a <pre><code>goal = \"Generate html code for the Tetris game that can be played in the browser.\"\nwf_generator = WorkFlowGenerator(llm=llm)\nworkflow_graph: WorkFlowGraph = wf_generator.generate_workflow(goal=goal)\n</code></pre> <code>WorkFlowGraph</code> \u662f\u4e00\u4e2a\u6570\u636e\u7ed3\u6784\uff0c\u7528\u4e8e\u5b58\u50a8\u6574\u4f53\u5de5\u4f5c\u6d41\u8ba1\u5212\uff0c\u5305\u62ec\u4efb\u52a1\u8282\u70b9\u53ca\u5176\u5173\u7cfb\uff0c\u4f46\u5c1a\u672a\u5305\u542b\u53ef\u6267\u884c\u7684\u4ee3\u7406\u3002</p> <p>\u53ef\u9009\uff1a\u53ef\u89c6\u5316\u6216\u4fdd\u5b58\u751f\u6210\u7684\u5de5\u4f5c\u6d41\uff1a <pre><code># \u53ef\u89c6\u5316\u5de5\u4f5c\u6d41\u7ed3\u6784\uff08\u53ef\u9009\uff09\nworkflow_graph.display()\n\n# \u5c06\u5de5\u4f5c\u6d41\u4fdd\u5b58\u4e3a JSON \u6587\u4ef6\uff08\u53ef\u9009\uff09\nworkflow_graph.save_module(\"/path/to/save/workflow_demo.json\")\n</code></pre> \u6211\u4eec\u63d0\u4f9b\u4e86\u4e00\u4e2a\u751f\u6210\u7684\u5de5\u4f5c\u6d41\u793a\u4f8b here\u3002\u4f60\u53ef\u4ee5\u91cd\u65b0\u52a0\u8f7d\u4fdd\u5b58\u7684\u5de5\u4f5c\u6d41\uff1a <pre><code>workflow_graph = WorkFlowGraph.from_file(\"/path/to/save/workflow_demo.json\")\n</code></pre></p>"},{"location":"zh/quickstart.html#\u7b2c\u4e8c\u6b65\u521b\u5efa\u5e76\u7ba1\u7406\u6267\u884c\u4ee3\u7406","title":"\u7b2c\u4e8c\u6b65\uff1a\u521b\u5efa\u5e76\u7ba1\u7406\u6267\u884c\u4ee3\u7406","text":"<p>\u4f7f\u7528 <code>AgentManager</code> \u57fa\u4e8e\u5de5\u4f5c\u6d41\u56fe\u5b9e\u4f8b\u5316\u5e76\u7ba1\u7406\u4ee3\u7406\uff1a <pre><code>agent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=openai_config)\n</code></pre></p>"},{"location":"zh/quickstart.html#\u7b2c\u4e09\u6b65\u6267\u884c\u5de5\u4f5c\u6d41","title":"\u7b2c\u4e09\u6b65\uff1a\u6267\u884c\u5de5\u4f5c\u6d41","text":"<p>\u4ee3\u7406\u51c6\u5907\u5c31\u7eea\u540e\uff0c\u53ef\u4ee5\u521b\u5efa <code>WorkFlow</code> \u5b9e\u4f8b\u5e76\u8fd0\u884c\uff1a <pre><code>workflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\noutput = workflow.execute()\nprint(output)\n</code></pre></p> <p>\u66f4\u591a\u793a\u4f8b\u8bf7\u53c2\u89c1 \u5b8c\u6574\u5de5\u4f5c\u6d41\u6f14\u793a\u3002</p>"},{"location":"zh/api/actions.html","title":"\ud83c\udfaf \u52a8\u4f5c\u63a5\u53e3","text":""},{"location":"zh/api/agents.html","title":"\ud83e\udd16 Agent \u63a5\u53e3","text":""},{"location":"zh/api/benchmark.html","title":"\ud83e\uddea \u57fa\u51c6\u6d4b\u8bd5\u63a5\u53e3","text":""},{"location":"zh/api/core.html","title":"\ud83e\udde0 \u6838\u5fc3\u6a21\u5757","text":""},{"location":"zh/api/evaluators.html","title":"\ud83e\uddd1\u200d\u2696\ufe0f \u8bc4\u4f30\u5668\u63a5\u53e3","text":""},{"location":"zh/api/memory.html","title":"\ud83d\udd87\ufe0f \u5de5\u5177\u96c6\u63a5\u53e3","text":""},{"location":"zh/api/models.html","title":"\ud83e\uddec \u6a21\u578b\u63a5\u53e3","text":""},{"location":"zh/api/optimizers.html","title":"\ud83e\uddee \u4f18\u5316\u5668\u63a5\u53e3","text":""},{"location":"zh/api/storages.html","title":"\ud83d\udcbe \u5b58\u50a8\u6a21\u5757","text":""},{"location":"zh/api/tools.html","title":"\ud83d\udee0\ufe0f \u5de5\u5177\u96c6\u63a5\u53e3","text":""},{"location":"zh/api/workflow.html","title":"\ud83d\udd01 \u5de5\u4f5c\u6d41\u63a5\u53e3","text":""},{"location":"zh/modules/action_graph.html","title":"\u52a8\u4f5c\u56fe","text":""},{"location":"zh/modules/action_graph.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>ActionGraph</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7528\u4e8e\u5728\u5355\u4e2a\u4efb\u52a1\u4e2d\u521b\u5efa\u548c\u6267\u884c\u64cd\u4f5c\uff08\u52a8\u4f5c\uff09\u5e8f\u5217\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u3001\u7ba1\u7406\u548c\u6267\u884c\u4e00\u7cfb\u5217\u9700\u8981\u6309\u7279\u5b9a\u987a\u5e8f\u6267\u884c\u7684\u64cd\u4f5c\uff0c\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002</p> <p>\u52a8\u4f5c\u56fe\u8868\u793a\u4e00\u7ec4\u6309\u9884\u5b9a\u4e49\u987a\u5e8f\u6267\u884c\u7684\u8fd0\u7b97\u7b26\uff08\u52a8\u4f5c\uff09\uff0c\u7528\u4e8e\u5904\u7406\u8f93\u5165\u5e76\u4ea7\u751f\u8f93\u51fa\u3002\u4e0e\u5728\u66f4\u9ad8\u5c42\u6b21\u7ba1\u7406\u591a\u4e2a\u4efb\u52a1\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\u7684 <code>WorkFlowGraph</code> \u4e0d\u540c\uff0c<code>ActionGraph</code> \u4e13\u6ce8\u4e8e\u5355\u4e2a\u4efb\u52a1\u5185\u7684\u8be6\u7ec6\u6267\u884c\u6b65\u9aa4\u3002</p>"},{"location":"zh/modules/action_graph.html#\u67b6\u6784","title":"\u67b6\u6784","text":""},{"location":"zh/modules/action_graph.html#\u52a8\u4f5c\u56fe\u67b6\u6784","title":"\u52a8\u4f5c\u56fe\u67b6\u6784","text":"<p><code>ActionGraph</code> \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>\u8fd0\u7b97\u7b26\uff1a</p> <p>\u6bcf\u4e2a\u8fd0\u7b97\u7b26\u4ee3\u8868\u53ef\u4ee5\u4f5c\u4e3a\u4efb\u52a1\u4e00\u90e8\u5206\u6267\u884c\u7684\u7279\u5b9a\u64cd\u4f5c\u6216\u52a8\u4f5c\uff0c\u5177\u6709\u4ee5\u4e0b\u5c5e\u6027\uff1a</p> <ul> <li><code>name</code>\uff1a\u8fd0\u7b97\u7b26\u7684\u552f\u4e00\u6807\u8bc6\u7b26</li> <li><code>description</code>\uff1a\u8fd0\u7b97\u7b26\u529f\u80fd\u7684\u8be6\u7ec6\u63cf\u8ff0</li> <li><code>llm</code>\uff1a\u7528\u4e8e\u6267\u884c\u8fd0\u7b97\u7b26\u7684 LLM</li> <li><code>outputs_format</code>\uff1a\u8fd0\u7b97\u7b26\u8f93\u51fa\u7684\u7ed3\u6784\u5316\u683c\u5f0f</li> <li><code>interface</code>\uff1a\u8c03\u7528\u8fd0\u7b97\u7b26\u7684\u63a5\u53e3</li> <li><code>prompt</code>\uff1a\u6267\u884c\u6b64\u8fd0\u7b97\u7b26\u65f6\u7528\u4e8e\u6307\u5bfc LLM \u7684\u6a21\u677f</li> </ul> </li> <li> <p>LLM\uff1a</p> <p>ActionGraph \u4f7f\u7528\u8bed\u8a00\u5b66\u4e60\u6a21\u578b\uff08LLM\uff09\u6765\u6267\u884c\u8fd0\u7b97\u7b26\u3002\u5b83\u63a5\u6536 <code>llm_config</code> \u4f5c\u4e3a\u8f93\u5165\u5e76\u521b\u5efa LLM \u5b9e\u4f8b\uff0c\u8be5\u5b9e\u4f8b\u5c06\u88ab\u4f20\u9012\u7ed9\u8fd0\u7b97\u7b26\u6267\u884c\u3002LLM \u63d0\u4f9b\u4e86\u6267\u884c\u6bcf\u4e2a\u52a8\u4f5c\u6240\u9700\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b\u3002</p> </li> <li> <p>\u6267\u884c\u6d41\u7a0b\uff1a</p> <p>ActionGraph \u5b9a\u4e49\u4e86\u7279\u5b9a\u7684\u6267\u884c\u987a\u5e8f\uff1a</p> <ul> <li>\u52a8\u4f5c\u6309\u9884\u5b9a\u987a\u5e8f\u6267\u884c\uff08\u5728 <code>execute</code> \u6216 <code>async_execute</code> \u65b9\u6cd5\u4e2d\u4f7f\u7528\u4ee3\u7801\u6307\u5b9a\uff09</li> <li>\u6bcf\u4e2a\u52a8\u4f5c\u53ef\u4ee5\u4f7f\u7528\u4e4b\u524d\u52a8\u4f5c\u7684\u7ed3\u679c</li> <li>\u5728\u6240\u6709\u52a8\u4f5c\u6267\u884c\u5b8c\u6210\u540e\u4ea7\u751f\u6700\u7ec8\u8f93\u51fa</li> </ul> </li> </ol>"},{"location":"zh/modules/action_graph.html#\u4e0e\u5de5\u4f5c\u6d41\u56fe\u7684\u6bd4\u8f83","title":"\u4e0e\u5de5\u4f5c\u6d41\u56fe\u7684\u6bd4\u8f83","text":"<p>\u867d\u7136 <code>ActionGraph</code> \u548c <code>WorkFlowGraph</code> \u90fd\u7ba1\u7406\u6267\u884c\u6d41\u7a0b\uff0c\u4f46\u5b83\u4eec\u5728\u62bd\u8c61\u5c42\u6b21\u4e0a\u6709\u6240\u4e0d\u540c\uff1a</p> \u7279\u6027 \u52a8\u4f5c\u56fe \u5de5\u4f5c\u6d41\u56fe \u8303\u56f4 \u5355\u4e2a\u4efb\u52a1\u6267\u884c \u591a\u4efb\u52a1\u5de5\u4f5c\u6d41\u7f16\u6392 \u7ec4\u4ef6 \u8fd0\u7b97\u7b26\uff08\u52a8\u4f5c\uff09 \u8282\u70b9\uff08\u4efb\u52a1\uff09\u548c\u8fb9\uff08\u4f9d\u8d56\u5173\u7cfb\uff09 \u91cd\u70b9 \u4efb\u52a1\u5185\u7684\u8be6\u7ec6\u6b65\u9aa4 \u4e0d\u540c\u4efb\u52a1\u4e4b\u95f4\u7684\u5173\u7cfb \u7075\u6d3b\u6027 \u56fa\u5b9a\u6267\u884c\u987a\u5e8f \u57fa\u4e8e\u4f9d\u8d56\u5173\u7cfb\u7684\u52a8\u6001\u6267\u884c \u4e3b\u8981\u7528\u9014 \u5b9a\u4e49\u53ef\u91cd\u7528\u7684\u4efb\u52a1\u6267\u884c\u6a21\u5f0f \u7f16\u6392\u590d\u6742\u7684\u591a\u6b65\u9aa4\u5de5\u4f5c\u6d41 \u7c92\u5ea6 \u7ec6\u7c92\u5ea6\u64cd\u4f5c \u7c97\u7c92\u5ea6\u4efb\u52a1"},{"location":"zh/modules/action_graph.html#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/action_graph.html#\u57fa\u672c\u52a8\u4f5c\u56fe\u521b\u5efa","title":"\u57fa\u672c\u52a8\u4f5c\u56fe\u521b\u5efa","text":"<pre><code>from evoagentx.workflow import ActionGraph\nfrom evoagentx.workflow.operators import Custom\nfrom evoagentx.models import OpenAILLMConfig \n\n# \u521b\u5efa LLM \u914d\u7f6e\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\n\n# \u521b\u5efa\u81ea\u5b9a\u4e49 ActionGraph\nclass MyActionGraph(ActionGraph):\n    def __init__(self, llm_config, **kwargs):\n\n        name = kwargs.pop(\"name\") if \"name\" in kwargs else \"Custom Action Graph\"\n        description = kwargs.pop(\"description\") if \"description\" in kwargs else \"A custom action graph for text processing\"\n        # \u57fa\u4e8e `llm_config` \u521b\u5efa LLM \u5b9e\u4f8b `self._llm` \u5e76\u5c06\u5176\u4f20\u9012\u7ed9\u8fd0\u7b97\u7b26\n        super().__init__(name=name, description=description, llm_config=llm_config, **kwargs)\n        # \u5b9a\u4e49\u8fd0\u7b97\u7b26\n        self.extract_entities = Custom(self._llm) # , prompt=\"Extract key entities from the following text: {input}\")\n        self.analyze_sentiment = Custom(self._llm) # , prompt=\"Analyze the sentiment of the following text: {input}\")\n        self.summarize = Custom(self._llm) # , prompt=\"Summarize the following text in one paragraph: {input}\")\n\n    def execute(self, text: str) -&gt; dict:\n        # \u6309\u987a\u5e8f\u6267\u884c\u8fd0\u7b97\u7b26\uff08\u6307\u5b9a\u8fd0\u7b97\u7b26\u7684\u6267\u884c\u987a\u5e8f\uff09\n        entities = self.extract_entities(input=text, instruction=\"Extract key entities from the provided text\")[\"response\"]\n        sentiment = self.analyze_sentiment(input=text, instruction=\"Analyze the sentiment of the provided text\")[\"response\"]\n        summary = self.summarize(input=text, instruction=\"Summarize the provided text in one paragraph\")[\"response\"]\n\n        # \u8fd4\u56de\u7ec4\u5408\u7ed3\u679c\n        return {\n            \"entities\": entities,\n            \"sentiment\": sentiment,\n            \"summary\": summary\n        }\n\n# \u521b\u5efa\u52a8\u4f5c\u56fe\naction_graph = MyActionGraph(llm_config=llm_config)\n\n# \u6267\u884c\u52a8\u4f5c\u56fe\nresult = action_graph.execute(text=\"This is a test text\")\nprint(result)\n</code></pre>"},{"location":"zh/modules/action_graph.html#\u5728\u5de5\u4f5c\u6d41\u56fe\u4e2d\u4f7f\u7528\u52a8\u4f5c\u56fe","title":"\u5728\u5de5\u4f5c\u6d41\u56fe\u4e2d\u4f7f\u7528\u52a8\u4f5c\u56fe","text":"<p>\u60a8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>ActionGraph</code> \u6216\u5728 <code>WorkFlowGraph</code> \u4e2d\u5c06\u5176\u4f5c\u4e3a\u8282\u70b9\u4f7f\u7528\u3002</p> <pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph\nfrom evoagentx.workflow.action_graph import QAActionGraph\nfrom evoagentx.core.base_config import Parameter\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import WorkFlow\n\n# \u521b\u5efa LLM \u914d\u7f6e\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# \u521b\u5efa\u52a8\u4f5c\u56fe\nqa_graph = QAActionGraph(llm_config=llm_config)\n\n# \u521b\u5efa\u4f7f\u7528\u52a8\u4f5c\u56fe\u7684\u5de5\u4f5c\u6d41\u8282\u70b9\nqa_node = WorkFlowNode(\n    name=\"QATask\",\n    description=\"Answer questions using a QA system\",\n    # \u8f93\u5165\u540d\u79f0\u5e94\u4e0e\u52a8\u4f5c\u56fe\u7684 `execute` \u65b9\u6cd5\u4e2d\u7684\u53c2\u6570\u5339\u914d\n    inputs=[Parameter(name=\"problem\", type=\"string\", description=\"The problem to answer\")],\n    outputs=[Parameter(name=\"answer\", type=\"string\", description=\"The answer to the problem\")],\n    action_graph=qa_graph  # \u4f7f\u7528 action_graph \u800c\u4e0d\u662f agents\n)\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph = WorkFlowGraph(goal=\"Answer a question\", nodes=[qa_node])\n\n# \u5b9a\u4e49\u5de5\u4f5c\u6d41\nworkflow = WorkFlow(graph=workflow_graph, llm=llm)\n\n# \u6267\u884c\u5de5\u4f5c\u6d41\nresult = workflow.execute(inputs={\"problem\": \"What is the capital of France?\"})\nprint(result)\n</code></pre> <p>Warning</p> <p>\u5728 <code>WorkFlowNode</code> \u4e2d\u4f7f\u7528 <code>ActionGraph</code> \u65f6\uff0c<code>WorkFlowNode</code> \u7684 <code>inputs</code> \u53c2\u6570\u5e94\u4e0e <code>ActionGraph</code> \u7684 <code>execute</code> \u65b9\u6cd5\u4e2d\u6240\u9700\u7684\u53c2\u6570\u5339\u914d\u3002<code>execute</code> \u65b9\u6cd5\u5e94\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\u6216 <code>LLMOutputParser</code> \u5b9e\u4f8b\uff0c\u5176\u952e\u4e0e <code>WorkFlowNode</code> \u4e2d <code>outputs</code> \u7684\u540d\u79f0\u5339\u914d\u3002</p>"},{"location":"zh/modules/action_graph.html#\u4fdd\u5b58\u548c\u52a0\u8f7d\u52a8\u4f5c\u56fe","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u52a8\u4f5c\u56fe","text":"<pre><code># \u4fdd\u5b58\u52a8\u4f5c\u56fe\naction_graph.save_module(\"examples/output/my_action_graph.json\")\n\n# \u52a0\u8f7d\u52a8\u4f5c\u56fe\nfrom evoagentx.workflow.action_graph import ActionGraph\nloaded_graph = ActionGraph.from_file(\"examples/output/my_action_graph.json\", llm_config=llm_config)\n</code></pre> <p><code>ActionGraph</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u5355\u4e2a\u4efb\u52a1\u5185\u7684\u590d\u6742\u64cd\u4f5c\u5e8f\u5217\uff0c\u8865\u5145\u4e86 EvoAgentX \u6846\u67b6\u4e2d <code>WorkFlowGraph</code> \u7684\u9ad8\u7ea7\u7f16\u6392\u529f\u80fd\u3002</p>"},{"location":"zh/modules/agent.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>Agent</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u521b\u5efa\u667a\u80fd AI \u4ee3\u7406\u7684\u57fa\u7840\u6784\u5efa\u5757\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u7ec4\u5408\u8bed\u8a00\u6a21\u578b\u3001\u52a8\u4f5c\u548c\u5185\u5b58\u7ba1\u7406\u3002</p>"},{"location":"zh/modules/agent.html#\u67b6\u6784","title":"\u67b6\u6784","text":"<p>Agent \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>\u5927\u8bed\u8a00\u6a21\u578b (LLM)\uff1a</p> <p>LLM \u901a\u8fc7 <code>llm</code> \u6216 <code>llm_config</code> \u53c2\u6570\u6307\u5b9a\uff0c\u4f5c\u4e3a\u4ee3\u7406\u7684\u57fa\u7840\u6784\u5efa\u5757\u3002\u5b83\u8d1f\u8d23\u89e3\u91ca\u4e0a\u4e0b\u6587\u3001\u751f\u6210\u54cd\u5e94\u548c\u505a\u51fa\u9ad8\u7ea7\u51b3\u7b56\u3002LLM \u5c06\u88ab\u4f20\u9012\u7ed9\u52a8\u4f5c\u4ee5\u6267\u884c\u7279\u5b9a\u4efb\u52a1\u3002</p> </li> <li> <p>\u52a8\u4f5c (Actions)\uff1a</p> <p>\u52a8\u4f5c\u662f\u4ee3\u7406\u7684\u57fa\u672c\u64cd\u4f5c\u5355\u5143\u3002\u6bcf\u4e2a\u52a8\u4f5c\u5c01\u88c5\u4e86\u4e00\u4e2a\u7279\u5b9a\u4efb\u52a1\uff0c\u662f\u5b9e\u9645\u8c03\u7528 LLM \u8fdb\u884c\u63a8\u7406\u3001\u751f\u6210\u6216\u51b3\u7b56\u7684\u5730\u65b9\u3002\u867d\u7136\u4ee3\u7406\u63d0\u4f9b\u6574\u4f53\u7f16\u6392\uff0c\u4f46 LLM \u901a\u8fc7\u52a8\u4f5c\u6267\u884c\u5176\u6838\u5fc3\u529f\u80fd\u3002\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u88ab\u8bbe\u8ba1\u4e3a\u53ea\u505a\u4e00\u4ef6\u4e8b\u2014\u2014\u6bd4\u5982\u68c0\u7d22\u77e5\u8bc6\u3001\u603b\u7ed3\u8f93\u5165\u6216\u8c03\u7528 API\u2014\u2014\u5e76\u4e14\u53ef\u4ee5\u5305\u542b\u4ee5\u4e0b\u7ec4\u4ef6\uff1a</p> <ul> <li>prompt\uff1a\u7528\u4e8e\u6307\u5bfc LLM \u6267\u884c\u6b64\u7279\u5b9a\u4efb\u52a1\u7684\u63d0\u793a\u6a21\u677f\u3002</li> <li>inputs_format\uff1a\u4f20\u5165\u52a8\u4f5c\u7684\u8f93\u5165\u7684\u9884\u671f\u7ed3\u6784\u548c\u952e\u3002</li> <li>outputs_format\uff1a\u7528\u4e8e\u89e3\u91ca\u548c\u89e3\u6790 LLM \u8f93\u51fa\u7684\u683c\u5f0f\u3002</li> <li>tools\uff1a\u53ef\u4ee5\u5728\u52a8\u4f5c\u4e2d\u96c6\u6210\u548c\u4f7f\u7528\u7684\u53ef\u9009\u5de5\u5177\u3002</li> </ul> </li> <li> <p>\u5185\u5b58\u7ec4\u4ef6\uff1a</p> <p>\u5185\u5b58\u5141\u8bb8\u4ee3\u7406\u5728\u4ea4\u4e92\u8fc7\u7a0b\u4e2d\u4fdd\u7559\u548c\u56de\u5fc6\u76f8\u5173\u4fe1\u606f\uff0c\u589e\u5f3a\u4e0a\u4e0b\u6587\u611f\u77e5\u80fd\u529b\u3002EvoAgentX \u6846\u67b6\u4e2d\u6709\u4e24\u79cd\u7c7b\u578b\u7684\u5185\u5b58\uff1a</p> <ul> <li>\u77ed\u671f\u5185\u5b58\uff1a\u7ef4\u62a4\u5f53\u524d\u4efb\u52a1\u7684\u4e2d\u95f4\u5bf9\u8bdd\u6216\u4e0a\u4e0b\u6587\u3002</li> <li>\u957f\u671f\u5185\u5b58\uff08\u53ef\u9009\uff09\uff1a\u5b58\u50a8\u53ef\u4ee5\u8de8\u8d8a\u4f1a\u8bdd\u6216\u4efb\u52a1\u7684\u6301\u4e45\u77e5\u8bc6\u3002\u8fd9\u4f7f\u4ee3\u7406\u80fd\u591f\u4ece\u8fc7\u53bb\u7684\u7ecf\u9a8c\u4e2d\u5b66\u4e60\u3001\u7ef4\u62a4\u7528\u6237\u504f\u597d\u6216\u968f\u65f6\u95f4\u6784\u5efa\u77e5\u8bc6\u5e93\u3002</li> </ul> </li> </ol>"},{"location":"zh/modules/agent.html#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/agent.html#\u57fa\u672c-agent-\u521b\u5efa","title":"\u57fa\u672c Agent \u521b\u5efa","text":"<p>\u8981\u521b\u5efa\u4ee3\u7406\uff0c\u60a8\u9700\u8981\u5b9a\u4e49\u4ee3\u7406\u5c06\u6267\u884c\u7684\u52a8\u4f5c\u3002\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u88ab\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u7ee7\u627f\u81ea <code>Action</code> \u7c7b\u7684\u7c7b\u3002\u52a8\u4f5c\u7c7b\u5e94\u8be5\u5b9a\u4e49\u4ee5\u4e0b\u7ec4\u4ef6\uff1a<code>name</code>\u3001<code>description</code>\u3001<code>prompt</code>\u3001<code>inputs_format</code> \u548c <code>outputs_format</code>\uff0c\u5e76\u5b9e\u73b0 <code>execute</code> \u65b9\u6cd5\uff08\u5982\u679c\u60a8\u60f3\u5f02\u6b65\u4f7f\u7528\u4ee3\u7406\uff0c\u8fd8\u9700\u8981\u5b9e\u73b0 <code>async_exectue</code>\uff09\u3002</p> <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# \u5b9a\u4e49\u4e00\u4e2a\u4f7f\u7528 LLM \u56de\u7b54\u95ee\u9898\u7684\u7b80\u5355\u52a8\u4f5c\n\nclass AnswerQuestionInput(ActionInput):\n    question: str\n\nclass AnswerQuestionOutput(ActionOutput):\n    answer: str\n\nclass AnswerQuestionAction(Action):\n\n    def __init__(\n        self, \n        name = \"answer_question\",\n        description = \"Answers a factual question using the LLM\",   \n        prompt = \"Answer the following question as accurately as possible:\\n\\n{question}\",\n        inputs_format = AnswerQuestionInput,\n        outputs_format = AnswerQuestionOutput,\n        **kwargs\n    ):\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )\n\n        if return_prompt:\n            return response, prompt\n        return response \n\n    async def async_execute(self, llm, inputs, sys_msg = None, return_prompt = False, **kwargs) -&gt; AnswerQuestionOutput:\n        question = inputs.get(\"question\")\n        prompt = self.prompt.format(question=question)\n        response = await llm.async_generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"str\"\n        )   \n        if return_prompt:\n            return response, prompt\n        return response \n\n# \u914d\u7f6e LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your-api-key\")\n\n# \u521b\u5efa\u4ee3\u7406\nagent = Agent(\n    name=\"AssistantAgent\",\n    description=\"Answers a factual question using the LLM\",\n    llm_config=llm_config,\n    system_prompt=\"You are a helpful assistant.\",\n    actions=[AnswerQuestionAction()]\n)\n</code></pre>"},{"location":"zh/modules/agent.html#\u6267\u884c\u52a8\u4f5c","title":"\u6267\u884c\u52a8\u4f5c","text":"<p>\u60a8\u53ef\u4ee5\u76f4\u63a5\u50cf\u51fd\u6570\u4e00\u6837\u8c03\u7528 <code>Agent</code> \u5b9e\u4f8b\u3002\u8fd9\u5c06\u5185\u90e8\u4f7f\u7528\u6307\u5b9a\u7684 <code>action_name</code> \u548c <code>action_input_data</code> \u8c03\u7528\u5339\u914d\u52a8\u4f5c\u7684 <code>execute()</code> \u65b9\u6cd5\u3002</p> <pre><code># \u4f7f\u7528\u8f93\u5165\u6570\u636e\u6267\u884c\u52a8\u4f5c\nmessage = agent(\n    action_name=\"answer_question\",\n    action_input_data={\"question\": \"What is the capital of France?\"}\n)\n\n# \u8bbf\u95ee\u8f93\u51fa\nresult = message.content.answer \n</code></pre>"},{"location":"zh/modules/agent.html#\u5f02\u6b65\u6267\u884c","title":"\u5f02\u6b65\u6267\u884c","text":"<p>\u60a8\u4e5f\u53ef\u4ee5\u5728\u5f02\u6b65\u4e0a\u4e0b\u6587\u4e2d\u8c03\u7528 <code>Agent</code> \u5b9e\u4f8b\u3002\u5982\u679c\u52a8\u4f5c\u5b9a\u4e49\u4e86 <code>async_execute</code> \u65b9\u6cd5\uff0c\u5f53\u60a8 <code>await</code> \u4ee3\u7406\u65f6\uff0c\u5b83\u5c06\u81ea\u52a8\u4f7f\u7528\u3002</p> <pre><code># \u5f02\u6b65\u6267\u884c\u52a8\u4f5c\nimport asyncio \n\nasync def main():\n    message = await agent(\n        action_name=\"answer_question\",\n        action_input_data={\"question\": \"What is the capital of France?\"}\n    )\n    return message.content.answer \n\nresult = asyncio.run(main())\nprint(result)\n</code></pre>"},{"location":"zh/modules/agent.html#\u5185\u5b58\u7ba1\u7406","title":"\u5185\u5b58\u7ba1\u7406","text":"<p>\u4ee3\u7406\u7ef4\u62a4\u77ed\u671f\u5185\u5b58\u4ee5\u8ddf\u8e2a\u5bf9\u8bdd\u4e0a\u4e0b\u6587\uff1a</p> <pre><code># \u8bbf\u95ee\u4ee3\u7406\u7684\u5185\u5b58\nmessages = agent.short_term_memory.get(n=5)  # \u83b7\u53d6\u6700\u540e 5 \u6761\u6d88\u606f\n\n# \u6e05\u9664\u5185\u5b58\nagent.clear_short_term_memory()\n</code></pre>"},{"location":"zh/modules/agent.html#\u4ee3\u7406\u914d\u7f6e\u6587\u4ef6","title":"\u4ee3\u7406\u914d\u7f6e\u6587\u4ef6","text":"<p>\u60a8\u53ef\u4ee5\u83b7\u53d6\u4ee3\u7406\u53ca\u5176\u529f\u80fd\u7684\u4eba\u7c7b\u53ef\u8bfb\u63cf\u8ff0\uff1a</p> <pre><code># \u83b7\u53d6\u6240\u6709\u52a8\u4f5c\u7684\u63cf\u8ff0\nprofile = agent.get_agent_profile()\nprint(profile)\n\n# \u83b7\u53d6\u7279\u5b9a\u52a8\u4f5c\u7684\u63cf\u8ff0\nprofile = agent.get_agent_profile(action_names=[\"answer_question\"])\nprint(profile)\n</code></pre>"},{"location":"zh/modules/agent.html#\u63d0\u793a\u7ba1\u7406","title":"\u63d0\u793a\u7ba1\u7406","text":"<p>\u8bbf\u95ee\u548c\u4fee\u6539\u4ee3\u7406\u4f7f\u7528\u7684\u63d0\u793a\uff1a</p> <pre><code># \u83b7\u53d6\u6240\u6709\u63d0\u793a\nprompts = agent.get_prompts()\n# prompts \u662f\u4e00\u4e2a\u5b57\u5178\uff0c\u7ed3\u6784\u5982\u4e0b\uff1a\n# {'answer_question': {'system_prompt': 'You are a helpful assistant.', 'prompt': 'Answer the following question as accurately as possible:\\n\\n{question}'}}\n\n# \u8bbe\u7f6e\u7279\u5b9a\u63d0\u793a\nagent.set_prompt(\n    action_name=\"answer_question\",\n    prompt=\"Please provide a clear and concise answer to the following query:\\n\\n{question}\",\n    system_prompt=\"You are a helpful assistant.\" # \u53ef\u9009\uff0c\u5982\u679c\u672a\u63d0\u4f9b\uff0c\u7cfb\u7edf\u63d0\u793a\u5c06\u4fdd\u6301\u4e0d\u53d8\n)\n\n# \u66f4\u65b0\u6240\u6709\u63d0\u793a\nprompts_dict = {\n    \"answer_question\": {\n        \"system_prompt\": \"You are an expert in providing concise, accurate information.\",\n        \"prompt\": \"Please answer this question with precision and clarity:\\n\\n{question}\"\n    }\n}\nagent.set_prompts(prompts_dict)\n</code></pre>"},{"location":"zh/modules/agent.html#\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","text":"<p>\u4ee3\u7406\u53ef\u4ee5\u88ab\u6301\u4e45\u5316\u548c\u91cd\u65b0\u52a0\u8f7d\uff1a</p> <pre><code># \u4fdd\u5b58\u4ee3\u7406\nagent.save_module(\"./agents/my_agent.json\")\n\n# \u52a0\u8f7d\u4ee3\u7406\uff08\u9700\u8981\u518d\u6b21\u63d0\u4f9b llm_config\uff09\nloaded_agent = Agent.from_file(\n    \"./agents/my_agent.json\", \n    llm_config=llm_config\n)\n</code></pre>"},{"location":"zh/modules/agent.html#\u4e0a\u4e0b\u6587\u63d0\u53d6","title":"\u4e0a\u4e0b\u6587\u63d0\u53d6","text":"<p>\u4ee3\u7406\u5305\u542b\u5185\u7f6e\u7684\u4e0a\u4e0b\u6587\u63d0\u53d6\u673a\u5236\uff0c\u53ef\u4ee5\u81ea\u52a8\u4ece\u5bf9\u8bdd\u5386\u53f2\u4e2d\u6d3e\u751f\u52a8\u52a8\u4f5c\u7684\u9002\u5f53\u8f93\u5165\uff1a</p> <pre><code># \u5728\u6ca1\u6709\u663e\u5f0f\u8f93\u5165\u6570\u636e\u7684\u60c5\u51b5\u4e0b\u6267\u884c\u65f6\uff0c\u4e0a\u4e0b\u6587\u4f1a\u81ea\u52a8\u63d0\u53d6\nresponse = agent.execute(\n    action_name=\"action_name\",\n    msgs=conversation_history\n)\n\n# \u624b\u52a8\u83b7\u53d6\u52a8\u4f5c\u8f93\u5165\naction = agent.get_action(\"action_name\")\ninputs = agent.get_action_inputs(action)\n</code></pre>"},{"location":"zh/modules/benchmark.html","title":"\u57fa\u51c6\u6d4b\u8bd5","text":""},{"location":"zh/modules/benchmark.html#\u57fa\u51c6\u6d4b\u8bd5\u6982\u8ff0","title":"\u57fa\u51c6\u6d4b\u8bd5\u6982\u8ff0","text":"<p>EvoAgentX \u63d0\u4f9b\u4e86\u4e00\u5957\u57fa\u51c6\u6d4b\u8bd5\u5de5\u5177\uff0c\u7528\u4e8e\u8bc4\u4f30\u4e0d\u540c\u7684\u57fa\u4e8e\u4ee3\u7406\u7684\u7cfb\u7edf\u3002\u4ee5\u4e0b\u662f\u5f53\u524d\u5305\u542b\u7684\u57fa\u51c6\u6d4b\u8bd5\u53ca\u5176\u57fa\u672c\u6570\u636e\u96c6\u7edf\u8ba1\u4fe1\u606f\uff1a</p> \u4efb\u52a1 \u6570\u636e\u96c6\u540d\u79f0 \u8bad\u7ec3\u96c6\u6570\u91cf \u9a8c\u8bc1\u96c6\u6570\u91cf \u6d4b\u8bd5\u96c6\u6570\u91cf \u95ee\u7b54 NQ 79,168 8,757 3,610 \u591a\u8df3\u95ee\u7b54 HotPotQA 90,447 7,405 / \u6570\u5b66 GSM8K 7,473 / 1,319 \u6570\u5b66 MATH 7,500 / 5,000 \u4ee3\u7801\u751f\u6210 HumanEval / / 164 \u4ee3\u7801\u751f\u6210 MBPP / / 427 \u4ee3\u7801\u751f\u6210 LiveCodeBench(v1~v5) / / 400~880 \u4ee3\u7801\u6267\u884c LiveCodeBench / / 479 \u6d4b\u8bd5\u8f93\u51fa\u9884\u6d4b LiveCodeBench / / 442 <p>\u6211\u4eec\u7684\u6846\u67b6\u63d0\u4f9b\u4e86\u81ea\u52a8\u6570\u636e\u96c6\u4e0b\u8f7d\u529f\u80fd\uff0c\u6240\u6709\u57fa\u51c6\u6d4b\u8bd5\u90fd\u5185\u7f6e\u4e86\u8bc4\u4f30\u65b9\u6cd5\u3002\u8be5\u6846\u67b6\u8bbe\u8ba1\u5141\u8bb8\u7528\u6237\u8f7b\u677e\u52a0\u8f7d\u3001\u4f7f\u7528\u548c\u8bc4\u4f30\u5404\u79cd\u4efb\u52a1\u7684\u6570\u636e\u96c6\uff0c\u800c\u65e0\u9700\u624b\u52a8\u5904\u7406\u6570\u636e\u4e0b\u8f7d\u548c\u8bc4\u4f30\u903b\u8f91\u3002 \u6240\u6709\u6570\u636e\u96c6\u5728\u9996\u6b21\u4f7f\u7528\u65f6\u90fd\u4f1a\u81ea\u52a8\u4e0b\u8f7d\u5230\u9ed8\u8ba4\u8def\u5f84\uff08~/.evoagentx/data/\uff09\uff0c\u6216\u8005\u7528\u6237\u53ef\u4ee5\u901a\u8fc7 <code>path</code> \u53c2\u6570\u6307\u5b9a\u81ea\u5b9a\u4e49\u8def\u5f84\u3002\u6bcf\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7c7b\u90fd\u5b9e\u73b0\u4e86\u6807\u51c6\u5316\u63a5\u53e3\uff0c\u5305\u62ec\u6570\u636e\u52a0\u8f7d\u3001\u6807\u7b7e\u68c0\u7d22\u548c\u9884\u6d4b\u8bc4\u4f30\u7684\u65b9\u6cd5\u3002</p> <p>\u4e0b\u9762\uff0c\u6211\u4eec\u4ecb\u7ecd\u6bcf\u4e2a\u57fa\u51c6\u6d4b\u8bd5\u7684\u9884\u5904\u7406\u6b65\u9aa4\u548c\u8bc4\u4f30\u6307\u6807\u3002</p> <ul> <li>\u95ee\u7b54<ul> <li>NQ</li> <li>HotPotQA</li> </ul> </li> <li>\u6570\u5b66<ul> <li>GSM8K</li> <li>MATH</li> </ul> </li> <li>\u4ee3\u7801\u751f\u6210<ul> <li>HumanEval</li> <li>MBPP</li> <li>LiveCodeBench</li> </ul> </li> </ul>"},{"location":"zh/modules/benchmark.html#\u9884\u5904\u7406\u548c\u8bc4\u4f30\u6307\u6807","title":"\u9884\u5904\u7406\u548c\u8bc4\u4f30\u6307\u6807","text":""},{"location":"zh/modules/benchmark.html#\u95ee\u7b54","title":"\u95ee\u7b54","text":"<p>\u5bf9\u4e8e\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u6211\u4eec\u9ed8\u8ba4\u4f7f\u7528\u7cbe\u786e\u5339\u914d\uff08EM\uff09\u3001F1 \u5206\u6570\u548c\u51c6\u786e\u7387\uff08ACC\uff09\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002EM \u8981\u6c42\u9884\u6d4b\u7b54\u6848\u4e0e\u771f\u5b9e\u7b54\u6848\u5b8c\u5168\u4e00\u81f4\u3002ACC \u8981\u6c42\u9884\u6d4b\u7b54\u6848\u5305\u542b\u771f\u5b9e\u7b54\u6848\uff0c\u8fd9\u5728 LLM \u7528\u4e8e\u751f\u6210\u7b54\u6848\u65f6\u7279\u522b\u6709\u7528\u3002</p>"},{"location":"zh/modules/benchmark.html#nq","title":"NQ","text":"<p>Natural Questions (NQ) \u5305\u542b\u6765\u81ea\u8c37\u6b4c\u641c\u7d22\u5f15\u64ce\u7684\u95ee\u9898\uff0c\u7b54\u6848\u7531\u4eba\u5de5\u6807\u6ce8\u8005\u6807\u6ce8\uff0c\u662f\u7ef4\u57fa\u767e\u79d1\u524d 5 \u4e2a\u641c\u7d22\u7ed3\u679c\u9875\u9762\u4e2d\u7684\u6bb5\u843d\u6216\u5b9e\u4f53\u3002\u6211\u4eec\u4f7f\u7528 DPR \u4ed3\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u5212\u5206\uff0c\u5305\u542b 79,168 \u4e2a\u8bad\u7ec3\u6837\u672c\u30018,757 \u4e2a\u5f00\u53d1\u6837\u672c\u548c 3,610 \u4e2a\u6d4b\u8bd5\u6837\u672c\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import NQ\nnq_dataset = NQ() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = nq_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"\u95ee\u9898\", \n    \"answers\": [\"\u53ef\u80fd\u7684\u7b54\u6848\"]\n}\n</code></pre></p>"},{"location":"zh/modules/benchmark.html#hotpotqa","title":"HotPotQA","text":"<p>HotPotQA \u662f\u4e00\u4e2a\u591a\u8df3\u95ee\u7b54\u6570\u636e\u96c6\uff0c\u9700\u8981\u591a\u6b65\u63a8\u7406\u6765\u56de\u7b54\u95ee\u9898\u3002\u6211\u4eec\u4f7f\u7528\u6570\u636e\u96c6\u7684\u5e72\u6270\u9879\u8bbe\u7f6e\u3002\u6bcf\u4e2a\u6837\u672c\u5305\u542b\u4e00\u4e2a\u95ee\u9898\u3001\u4e00\u4e2a\u7b54\u6848\u3001\u4e00\u4e9b\u5305\u542b\u652f\u6301\u4fe1\u606f\u548c\u5e72\u6270\u4fe1\u606f\u7684\u4e0a\u4e0b\u6587\uff0c\u4ee5\u53ca\u652f\u6301\u4e8b\u5b9e\u3002\u6211\u4eec\u53ea\u5305\u542b\u8bad\u7ec3\u96c6\u548c\u5f00\u53d1\u96c6\uff0c\u56e0\u4e3a\u6d4b\u8bd5\u96c6\u4e0d\u516c\u5f00\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import HotPotQA\nhotpotqa_dataset = HotPotQA() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = hotpotqa_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff0c\u5176\u4e2d supporting_fact \u7684\u7b2c\u4e8c\u4e2a\u5143\u7d20\uff08\u6574\u6570\uff09\u662f\u652f\u6301\u7b54\u6848\u7684\u4e0a\u4e0b\u6587\u53e5\u5b50\u7d22\u5f15\uff1a <pre><code>{\n        \"_id\": \"\u6837\u672cID\", \n        \"question\": \"\u95ee\u9898\", \n        \"answer\": \"\u7b54\u6848\", \n        \"context\": [[\"\u4e0a\u4e0b\u6587\u6807\u9898\", [\"\u4e0a\u4e0b\u6587\u53e5\u5b50\", \"\u53e6\u4e00\u4e2a\u53e5\u5b50\"]]],\n        \"supporting_facts\": [[\"\u652f\u6301\u6807\u9898\", 0]]\n    }\n</code></pre></p>"},{"location":"zh/modules/benchmark.html#\u6570\u5b66","title":"\u6570\u5b66","text":"<p>\u5bf9\u4e8e\u6570\u5b66\u6570\u636e\u96c6\uff0c\u6211\u4eec\u4f7f\u7528\u89e3\u51b3\u7387\u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\u3002\u89e3\u51b3\u7387\u662f\u6b63\u786e\u89e3\u51b3\u7684\u6837\u672c\u6570\u91cf\u4e0e\u603b\u6837\u672c\u6570\u91cf\u7684\u6bd4\u7387\u3002</p>"},{"location":"zh/modules/benchmark.html#gsm8k","title":"GSM8K","text":"<p>GSM8K \u7531\u4eba\u5de5\u95ee\u9898\u7f16\u5199\u8005\u521b\u5efa\u7684\u9ad8\u8d28\u91cf\u5c0f\u5b66\u6570\u5b66\u95ee\u9898\u7ec4\u6210\u3002\u8fd9\u4e9b\u95ee\u9898\u9700\u8981\u591a\u6b65\u6570\u5b66\u63a8\u7406\u6765\u89e3\u51b3\u3002\u6211\u4eec\u4f7f\u7528\u539f\u59cb\u4ed3\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u5212\u5206\uff0c\u5305\u542b 7.5K \u4e2a\u8bad\u7ec3\u95ee\u9898\u548c 1K \u4e2a\u6d4b\u8bd5\u95ee\u9898\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import GSM8K\ngsm8k_dataset = GSM8K() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = gsm8k_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>{\n    \"id\": \"test-1\", \n    \"question\": \"\u95ee\u9898\", \n    \"answer\": \"\u7b54\u6848\"\n}\n</code></pre></p>"},{"location":"zh/modules/benchmark.html#math","title":"MATH","text":"<p>Mathematics Aptitude Test of Heuristics (MATH) \u6570\u636e\u96c6\u5305\u542b\u6765\u81ea\u6570\u5b66\u7ade\u8d5b\u7684\u95ee\u9898\uff0c\u5305\u62ec AMC 10\u3001AMC 12\u3001AIME \u7b49\u3002MATH \u4e2d\u7684\u6bcf\u4e2a\u95ee\u9898\u90fd\u6709\u9010\u6b65\u89e3\u51b3\u65b9\u6848\u3002\u6211\u4eec\u4f7f\u7528\u539f\u59cb\u4ed3\u5e93\u63d0\u4f9b\u7684\u6570\u636e\u96c6\u5212\u5206\uff0c\u5305\u542b 7.5K \u4e2a\u8bad\u7ec3\u95ee\u9898\u548c 5K \u4e2a\u6d4b\u8bd5\u95ee\u9898\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import MATH\nmath_dataset = MATH() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = math_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\u3002\u5bf9\u4e8e <code>level</code> \u5b57\u6bb5\uff0c\u6709\u6548\u503c\u4e3a\uff1a\"Level 1\"\u3001\"Level 2\"\u3001\"Level 3\"\u3001\"Level 4\"\u3001\"Level 5\" \u548c \"Level ?\"\u3002<code>type</code> \u5b57\u6bb5\u53ef\u4ee5\u662f\u4ee5\u4e0b\u4e4b\u4e00\uff1a\"Geometry\"\u3001\"Algebra\"\u3001\"Intermediate Algebra\"\u3001\"Counting &amp; Probability\"\u3001\"Precalculus\"\u3001\"Number Theory\" \u6216 \"Prealgebra\"\u3002</p> <pre><code>{\n    \"id\": \"test-1\", \n    \"problem\": \"\u95ee\u9898\", \n    \"solution\": \"\u89e3\u51b3\u65b9\u6848\",\n    \"level\": \"Level 1\",\n    \"type\": \"Algebra\"\n}\n</code></pre>"},{"location":"zh/modules/benchmark.html#\u4ee3\u7801\u751f\u6210","title":"\u4ee3\u7801\u751f\u6210","text":"<p>\u5bf9\u4e8e\u4ee3\u7801\u751f\u6210\u57fa\u51c6\u6d4b\u8bd5\uff0c\u6211\u4eec\u4f7f\u7528 pass@k \u4f5c\u4e3a\u8bc4\u4f30\u6307\u6807\uff0c\u5176\u4e2d k \u662f\u6bcf\u4e2a\u95ee\u9898\u7684\u89e3\u51b3\u65b9\u6848\u6570\u91cf\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0ck \u8bbe\u7f6e\u4e3a 1\u3002</p>"},{"location":"zh/modules/benchmark.html#humaneval","title":"HumanEval","text":"<p>HumanEval \u662f\u4e00\u4e2a\u5305\u542b 164 \u4e2a\u7f16\u7a0b\u95ee\u9898\u7684\u6570\u636e\u96c6\uff0c\u6765\u81ea HumanEval \u57fa\u51c6\u6d4b\u8bd5\u3002\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u4e00\u4e2a\u51fd\u6570\u7b7e\u540d\u3001\u4e00\u4e2a\u89c4\u8303\u89e3\u51b3\u65b9\u6848\u548c\u4e00\u7ec4\u5355\u5143\u6d4b\u8bd5\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import HumanEval\nhumaneval_dataset = HumanEval() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = humaneval_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff1a <pre><code>{\n    \"task_id\": \"HumanEval/0\", \n    \"prompt\": \"\u95ee\u9898\u63d0\u793a\", \n    \"entry_point\": \"\u8981\u6d4b\u8bd5\u7684\u51fd\u6570\u540d\u79f0\",\n    \"canonical_solution\": \"\u95ee\u9898\u7684\u89c4\u8303\u89e3\u51b3\u65b9\u6848\", \n    \"test\": \"\u95ee\u9898\u7684\u5355\u5143\u6d4b\u8bd5\"\n}\n</code></pre></p>"},{"location":"zh/modules/benchmark.html#mbpp","title":"MBPP","text":"<p>Mostly Basic Python Problems (MBPP) \u5305\u542b\u6570\u767e\u4e2a\u5165\u95e8\u7ea7 Python \u7f16\u7a0b\u95ee\u9898\u3002\u6bcf\u4e2a\u95ee\u9898\u5305\u542b\u4efb\u52a1\u63cf\u8ff0\u3001\u4ee3\u7801\u89e3\u51b3\u65b9\u6848\u548c 3 \u4e2a\u81ea\u52a8\u5316\u6d4b\u8bd5\u7528\u4f8b\u3002\u6211\u4eec\u4f7f\u7528 MBPP \u6570\u636e\u96c6\u7684\u6e05\u7406\u5b50\u96c6\uff0c\u5305\u542b 427 \u4e2a\u7ecf\u8fc7\u4f5c\u8005\u624b\u52a8\u9a8c\u8bc1\u7684\u95ee\u9898\u3002\u4e3a\u4e86\u4fbf\u4e8e\u8bc4\u4f30\uff0c\u6211\u4eec\u5c06 MBPP \u6570\u636e\u96c6\u8f6c\u6362\u4e3a HumanEval \u683c\u5f0f\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff1a <pre><code>from evoagentx.benchmark import MBPP\nmbpp_dataset = MBPP() # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = mbpp_dataset.get_test_data()\n</code></pre> \u6570\u636e\u96c6\u4e2d\u7684\u6bcf\u4e2a\u6837\u672c\u683c\u5f0f\u5982\u4e0b\uff0c\u6211\u4eec\u4fdd\u7559\u539f\u59cb\u7684 MBPP <code>task_id</code>\uff1a <pre><code>{\n    \"task_id\": 2, \n    \"prompt\": \"\u95ee\u9898\u63d0\u793a\", \n    \"entry_point\": \"\u8981\u6d4b\u8bd5\u7684\u51fd\u6570\u540d\u79f0\",\n    \"canonical_solution\": \"\u95ee\u9898\u7684\u89c4\u8303\u89e3\u51b3\u65b9\u6848\", \n    \"test\": \"\u95ee\u9898\u7684\u5355\u5143\u6d4b\u8bd5\"\n}\n</code></pre> \u60a8\u8fd8\u53ef\u4ee5\u901a\u8fc7\u4f7f\u7528 <code>example[\"code\"]</code> \u8bbf\u95ee\u539f\u59cb MBPP \u5c5e\u6027\uff0c\u5982 \"code\"\u3001\"test_list\" \u7b49\u3002</p>"},{"location":"zh/modules/benchmark.html#livecodebench","title":"LiveCodeBench","text":"<p>LiveCodeBench \u662f\u4e00\u4e2a\u65e0\u6c61\u67d3\u7684 LLM \u4ee3\u7801\u8bc4\u4f30\u57fa\u51c6\u6d4b\u8bd5\uff0c\u5b83\u968f\u65f6\u95f4\u4e0d\u65ad\u6536\u96c6\u65b0\u95ee\u9898\u3002\u7279\u522b\u662f\uff0cLiveCodeBench \u8fd8\u5173\u6ce8\u66f4\u5e7f\u6cdb\u7684\u4ee3\u7801\u76f8\u5173\u80fd\u529b\uff0c\u5982\u4ee3\u7801\u6267\u884c\u548c\u6d4b\u8bd5\u8f93\u51fa\u9884\u6d4b\uff0c\u800c\u4e0d\u4ec5\u4ec5\u662f\u4ee3\u7801\u751f\u6210\u3002\u76ee\u524d\uff0cLiveCodeBench \u6258\u7ba1\u4e86 300 \u591a\u4e2a\u9ad8\u8d28\u91cf\u7f16\u7a0b\u95ee\u9898\uff0c\u53d1\u5e03\u65f6\u95f4\u5728 2023 \u5e74 5 \u6708\u81f3 2024 \u5e74 2 \u6708\u4e4b\u95f4\u3002</p> <p>\u60a8\u53ef\u4ee5\u4f7f\u7528\u4ee5\u4e0b\u4ee3\u7801\u52a0\u8f7d\u6570\u636e\u96c6\uff0c\u5176\u4e2d <code>scenario</code> \u53ef\u4ee5\u662f [<code>code_generation</code>\u3001<code>test_output_prediction</code>\u3001<code>code_execution</code>] \u4e4b\u4e00\uff0c\u8868\u793a\u4e0d\u540c\u7684\u4efb\u52a1\u3002<code>version</code> \u8868\u793a\u4ee3\u7801\u751f\u6210\u6570\u636e\u96c6\u7684\u4e0d\u540c\u7248\u672c\uff0c\u4ec5\u9002\u7528\u4e8e <code>code_generation</code> \u573a\u666f\uff0c\u53ef\u4ee5\u662f <code>[\"release_v1\"\u3001\"release_v2\"\u3001\"release_v3\"\u3001\"release_v4\"\u3001\"release_v5\"\u3001\"release_latest\"]</code> \u4e4b\u4e00\u3002\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 LiveCodeBench \u4ed3\u5e93\u3002</p> <pre><code>from evoagentx.benchmark import LiveCodeBench\nlivecodebench_dataset = LiveCodeBench(scenario=\"code_generation\", version=\"release_v1\") # \u53ef\u9009\uff1apath=\"/path/to/save_data\"\ntest_data = livecodebench_dataset.get_test_data()\n</code></pre>"},{"location":"zh/modules/customize_agent.html","title":"\u81ea\u5b9a\u4e49\u4ee3\u7406","text":""},{"location":"zh/modules/customize_agent.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>CustomizeAgent</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u7684\u6846\u67b6\uff0c\u7528\u4e8e\u521b\u5efa\u4e13\u95e8\u7684 LLM \u9a71\u52a8\u7684\u4ee3\u7406\u3002\u5b83\u5141\u8bb8\u5b9a\u4e49\u5177\u6709\u660e\u786e\u5b9a\u4e49\u7684\u8f93\u5165\u3001\u8f93\u51fa\u3001\u81ea\u5b9a\u4e49\u63d0\u793a\u6a21\u677f\u548c\u53ef\u914d\u7f6e\u89e3\u6790\u7b56\u7565\u7684\u4ee3\u7406\uff0c\u4f7f\u5176\u9002\u5408\u5feb\u901f\u539f\u578b\u8bbe\u8ba1\u548c\u90e8\u7f72\u7279\u5b9a\u9886\u57df\u7684\u4ee3\u7406\u3002</p>"},{"location":"zh/modules/customize_agent.html#\u4e3b\u8981\u7279\u6027","title":"\u4e3b\u8981\u7279\u6027","text":"<ul> <li>\u65e0\u9700\u81ea\u5b9a\u4e49\u4ee3\u7801\uff1a\u901a\u8fc7\u914d\u7f6e\u800c\u4e0d\u662f\u7f16\u5199\u81ea\u5b9a\u4e49\u4ee3\u7406\u7c7b\u6765\u521b\u5efa\u4e13\u95e8\u7684\u4ee3\u7406</li> <li>\u7075\u6d3b\u7684\u8f93\u5165/\u8f93\u51fa\u5b9a\u4e49\uff1a\u660e\u786e\u5b9a\u4e49\u4ee3\u7406\u63a5\u53d7\u7684\u8f93\u5165\u548c\u4ea7\u751f\u7684\u8f93\u51fa</li> <li>\u53ef\u81ea\u5b9a\u4e49\u7684\u89e3\u6790\u7b56\u7565\uff1a\u591a\u79cd\u89e3\u6790\u6a21\u5f0f\uff0c\u7528\u4e8e\u4ece LLM \u54cd\u5e94\u4e2d\u63d0\u53d6\u7ed3\u6784\u5316\u6570\u636e</li> <li>\u53ef\u91cd\u7528\u7ec4\u4ef6\uff1a\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406\u5b9a\u4e49\uff0c\u4ee5\u4fbf\u5728\u9879\u76ee\u4e4b\u95f4\u91cd\u7528</li> </ul>"},{"location":"zh/modules/customize_agent.html#\u57fa\u672c\u7528\u6cd5","title":"\u57fa\u672c\u7528\u6cd5","text":""},{"location":"zh/modules/customize_agent.html#\u7b80\u5355\u4ee3\u7406","title":"\u7b80\u5355\u4ee3\u7406","text":"<p>\u521b\u5efa <code>CustomizeAgent</code> \u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u53ea\u4f7f\u7528\u540d\u79f0\u3001\u63cf\u8ff0\u548c\u63d0\u793a\uff1a</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u914d\u7f6e LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\n\n# \u521b\u5efa\u4e00\u4e2a\u7b80\u5355\u4ee3\u7406\nsimple_agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"A basic agent that responds to queries\",\n    prompt=\"Answer the following question: {question}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"question\", \"type\": \"string\", \"description\": \"The question to answer\"}\n    ]\n)\n\n# \u6267\u884c\u4ee3\u7406\nresponse = simple_agent(inputs={\"question\": \"What is a language model?\"})\nprint(response.content.content)  # \u8bbf\u95ee\u539f\u59cb\u54cd\u5e94\u5185\u5bb9\n</code></pre> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff1a 1. \u7531\u4e8e\u63d0\u793a\u9700\u8981\u8f93\u5165\uff0c\u6211\u4eec\u5728 <code>inputs</code> \u53c2\u6570\u4e2d\u6307\u5b9a\u4e86\u8f93\u5165\u4fe1\u606f\uff08\u5305\u62ec\u5176\u540d\u79f0\u3001\u7c7b\u578b\u548c\u63cf\u8ff0\uff09\u3002 2. \u6b64\u5916\uff0c\u5f53\u4f7f\u7528 <code>simple_agent(...)</code> \u6267\u884c\u4ee3\u7406\u65f6\uff0c\u60a8\u5e94\u8be5\u5728 <code>inputs</code> \u53c2\u6570\u4e2d\u63d0\u4f9b\u6240\u6709\u8f93\u5165\u3002</p> <p>\u6267\u884c\u4ee3\u7406\u540e\u7684\u8f93\u51fa\u662f\u4e00\u4e2a <code>Message</code> \u5bf9\u8c61\uff0c\u5176\u4e2d\u5305\u542b <code>message.content.content</code> \u4e2d\u7684\u539f\u59cb LLM \u54cd\u5e94\u3002</p> <p>Note</p> <p>\u5728 <code>CustomizeAgent(inputs=[...])</code> \u4e2d\u6307\u5b9a\u7684\u6240\u6709\u8f93\u5165\u540d\u79f0\u90fd\u5e94\u8be5\u51fa\u73b0\u5728 <code>prompt</code> \u4e2d\u3002\u5426\u5219\uff0c\u5c06\u5f15\u53d1\u9519\u8bef\u3002</p>"},{"location":"zh/modules/customize_agent.html#\u7ed3\u6784\u5316\u8f93\u51fa","title":"\u7ed3\u6784\u5316\u8f93\u51fa","text":"<p><code>CustomizeAgent</code> \u6700\u5f3a\u5927\u7684\u529f\u80fd\u4e4b\u4e00\u662f\u80fd\u591f\u5b9a\u4e49\u7ed3\u6784\u5316\u8f93\u51fa\u3002\u8fd9\u5141\u8bb8\u60a8\u5c06\u975e\u7ed3\u6784\u5316\u7684 LLM \u54cd\u5e94\u8f6c\u6362\u4e3a\u66f4\u5bb9\u6613\u4ee5\u7f16\u7a0b\u65b9\u5f0f\u5904\u7406\u7684\u660e\u786e\u5b9a\u4e49\u7684\u6570\u636e\u7ed3\u6784\u3002</p>"},{"location":"zh/modules/customize_agent.html#\u57fa\u672c\u7ed3\u6784\u5316\u8f93\u51fa","title":"\u57fa\u672c\u7ed3\u6784\u5316\u8f93\u51fa","text":"<p>\u4ee5\u4e0b\u662f\u5b9a\u4e49\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u7b80\u5355\u793a\u4f8b\uff1a</p> <pre><code>from evoagentx.core.module_utils import extract_code_blocks\n\n# \u521b\u5efa\u4e00\u4e2a\u5177\u6709\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4ee3\u7801\u7f16\u5199\u4ee3\u7406\ncode_writer = CustomizeAgent(\n    name=\"CodeWriter\",\n    description=\"Writes Python code based on requirements\",\n    prompt=\"Write Python code that implements the following requirement: {requirement}\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"}\n    ],\n    parse_mode=\"custom\",  # \u4f7f\u7528\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\n    parse_func=lambda content: {\"code\": extract_code_blocks(content)[0]}  # \u63d0\u53d6\u7b2c\u4e00\u4e2a\u4ee3\u7801\u5757\n)\n\n# \u6267\u884c\u4ee3\u7406\nmessage = code_writer(\n    inputs={\"requirement\": \"Write a function that returns the sum of two numbers\"}\n)\nprint(message.content.code)  # \u76f4\u63a5\u8bbf\u95ee\u89e3\u6790\u540e\u7684\u4ee3\u7801\n</code></pre> <p>\u5728\u8fd9\u4e2a\u4f8b\u5b50\u4e2d\uff1a 1. \u6211\u4eec\u5728 <code>outputs</code> \u53c2\u6570\u4e2d\u5b9a\u4e49\u4e86\u4e00\u4e2a\u540d\u4e3a <code>code</code> \u7684\u8f93\u51fa\u5b57\u6bb5\u3002 2. \u6211\u4eec\u8bbe\u7f6e <code>parse_mode=\"custom\"</code> \u6765\u4f7f\u7528\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\u3002 3. <code>parse_func</code> \u4ece LLM \u54cd\u5e94\u4e2d\u63d0\u53d6\u7b2c\u4e00\u4e2a\u4ee3\u7801\u5757\u3002 4. \u6211\u4eec\u53ef\u4ee5\u901a\u8fc7 <code>message.content.code</code> \u76f4\u63a5\u8bbf\u95ee\u89e3\u6790\u540e\u7684\u4ee3\u7801\u3002</p> <p>\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7 <code>message.content.content</code> \u8bbf\u95ee\u539f\u59cb LLM \u54cd\u5e94\u3002</p> <p>Note</p> <ol> <li> <p>\u5982\u679c\u5728 <code>CustomizeAgent</code> \u4e2d\u8bbe\u7f6e\u4e86 <code>outputs</code> \u53c2\u6570\uff0c\u4ee3\u7406\u5c06\u5c1d\u8bd5\u6839\u636e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u89e3\u6790 LLM \u54cd\u5e94\u3002\u5982\u679c\u60a8\u4e0d\u60f3\u89e3\u6790 LLM \u54cd\u5e94\uff0c\u5219\u4e0d\u5e94\u8bbe\u7f6e <code>outputs</code> \u53c2\u6570\u3002\u53ef\u4ee5\u901a\u8fc7 <code>message.content.content</code> \u8bbf\u95ee\u539f\u59cb LLM \u54cd\u5e94\u3002</p> </li> <li> <p>CustomizeAgent \u652f\u6301\u4e0d\u540c\u7684\u89e3\u6790\u6a21\u5f0f\uff0c\u5982 <code>['str', 'json', 'xml', 'title', 'custom']</code>\u3002\u6709\u5173\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u89e3\u6790\u6a21\u5f0f\u90e8\u5206\u3002</p> </li> </ol>"},{"location":"zh/modules/customize_agent.html#\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa","title":"\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa","text":"<p>\u60a8\u53ef\u4ee5\u5b9a\u4e49\u591a\u4e2a\u8f93\u51fa\u5b57\u6bb5\u6765\u521b\u5efa\u66f4\u590d\u6742\u7684\u7ed3\u6784\u5316\u6570\u636e\uff1a</p> <pre><code># \u751f\u6210\u4ee3\u7801\u548c\u89e3\u91ca\u7684\u4ee3\u7406\nanalyzer = CustomizeAgent(\n    name=\"CodeAnalyzer\",\n    description=\"Generates and explains Python code\",\n    prompt=\"\"\"\n    Write Python code for: {requirement}\n\n    Provide your response in the following format:\n\n    ## code\n    [Your code implementation here]\n\n    ## explanation\n    [A brief explanation of how the code works]\n\n    ## complexity\n    [Time and space complexity analysis]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"explanation\", \"type\": \"string\", \"description\": \"Explanation of the code\"},\n        {\"name\": \"complexity\", \"type\": \"string\", \"description\": \"Complexity analysis\"}\n    ],\n    parse_mode=\"title\"  # \u4f7f\u7528\u9ed8\u8ba4\u7684\u6807\u9898\u89e3\u6790\u6a21\u5f0f\n)\n\n# \u6267\u884c\u4ee3\u7406\nresult = analyzer(inputs={\"requirement\": \"Write a binary search algorithm\"})\n\n# \u5206\u522b\u8bbf\u95ee\u6bcf\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nEXPLANATION:\")\nprint(result.content.explanation)\nprint(\"\\nCOMPLEXITY:\")\nprint(result.content.complexity)\n</code></pre>"},{"location":"zh/modules/customize_agent.html#\u63d0\u793a\u6a21\u677f\u7528\u6cd5","title":"\u63d0\u793a\u6a21\u677f\u7528\u6cd5","text":"<p><code>CustomizeAgent</code> \u8fd8\u652f\u6301\u4f7f\u7528 <code>PromptTemplate</code> \u8fdb\u884c\u66f4\u7075\u6d3b\u7684\u63d0\u793a\u6a21\u677f\u8bbe\u8ba1\u3002\u6709\u5173\u63d0\u793a\u6a21\u677f\u53ca\u5176\u9ad8\u7ea7\u529f\u80fd\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u63d0\u793a\u6a21\u677f\u6559\u7a0b\u3002</p>"},{"location":"zh/modules/customize_agent.html#\u7b80\u5355\u63d0\u793a\u6a21\u677f","title":"\u7b80\u5355\u63d0\u793a\u6a21\u677f","text":"<p>\u4ee5\u4e0b\u662f\u4f7f\u7528\u63d0\u793a\u6a21\u677f\u7684\u57fa\u672c\u793a\u4f8b\uff1a</p> <pre><code>from evoagentx.prompts import StringTemplate\n\nagent = CustomizeAgent(\n    name=\"FirstAgent\",\n    description=\"A simple agent that prints hello world\",\n    prompt_template=StringTemplate(\n        instruction=\"Print 'hello world'\",\n    ),\n    llm_config=openai_config\n)\n\nmessage = agent()\nprint(message.content.content)\n</code></pre>"},{"location":"zh/modules/customize_agent.html#\u5e26\u6709\u8f93\u5165\u548c\u8f93\u51fa\u7684\u63d0\u793a\u6a21\u677f","title":"\u5e26\u6709\u8f93\u5165\u548c\u8f93\u51fa\u7684\u63d0\u793a\u6a21\u677f","text":"<p>\u60a8\u53ef\u4ee5\u5c06\u63d0\u793a\u6a21\u677f\u4e0e\u7ed3\u6784\u5316\u8f93\u5165\u548c\u8f93\u51fa\u7ed3\u5408\u4f7f\u7528\uff1a</p> <pre><code>code_writer = CustomizeAgent(\n    name=\"CodeWriter\",\n    description=\"Writes Python code based on requirements\",\n    prompt_template=StringTemplate(\n        instruction=\"Write Python code that implements the provided `requirement`\",\n        # \u60a8\u53ef\u4ee5\u9009\u62e9\u6dfb\u52a0\u793a\u4f8b\uff1a\n        # demonstrations=[\n        #     {\n        #         \"requirement\": \"Print 'hello world'\",\n        #         \"code\": \"print('hello world')\"\n        #     }, \n        #     {\n        #         \"requirement\": \"Print 'Test Demonstration'\",\n        #         \"code\": \"print('Test Demonstration')\"\n        #     }\n        # ]\n    ), # \u4e0d\u9700\u8981\u5728\u63d0\u793a\u6a21\u677f\u7684\u6307\u4ee4\u4e2d\u660e\u786e\u6307\u5b9a\u8f93\u5165\u5360\u4f4d\u7b26\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"Coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"Generated Python code\"},\n    ],\n    parse_mode=\"custom\", \n    parse_func=lambda content: {\"code\": extract_code_blocks(content)[0]}\n)\n\nmessage = code_writer(\n    inputs={\"requirement\": \"Write a function that returns the sum of two numbers\"}\n)\nprint(message.content.code)\n</code></pre> <p><code>PromptTemplate</code> \u63d0\u4f9b\u4e86\u4e00\u79cd\u66f4\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u63d0\u793a\uff0c\u53ef\u4ee5\u5305\u62ec\uff1a - \u4e3b\u8981\u6307\u4ee4 - \u53ef\u9009\u7684\u4e0a\u4e0b\u6587\uff0c\u7528\u4e8e\u63d0\u4f9b\u989d\u5916\u4fe1\u606f - \u53ef\u9009\u7684\u7ea6\u675f\uff0cLLM \u5e94\u8be5\u9075\u5faa - \u53ef\u9009\u7684\u793a\u4f8b\uff0c\u7528\u4e8e\u5c11\u6837\u672c\u5b66\u4e60 - \u53ef\u9009\u7684\u5de5\u5177\u4fe1\u606f\uff0cLLM \u53ef\u4ee5\u4f7f\u7528 \u7b49\u3002</p> <p>Note</p> <ol> <li> <p>\u4f7f\u7528 <code>prompt_template</code> \u65f6\uff0c\u60a8\u4e0d\u9700\u8981\u5728\u6307\u4ee4\u5b57\u7b26\u4e32\u4e2d\u660e\u786e\u5305\u542b\u8f93\u5165\u5360\u4f4d\u7b26\uff0c\u5982 <code>{input_name}</code>\u3002\u6a21\u677f\u5c06\u81ea\u52a8\u5904\u7406\u8f93\u5165\u7684\u6620\u5c04\u3002</p> </li> <li> <p>\u6b64\u5916\uff0c\u60a8\u4e0d\u9700\u8981\u5728 <code>PromptTemplate</code> \u7684 <code>instruction</code> \u5b57\u6bb5\u4e2d\u660e\u786e\u6307\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002\u6a21\u677f\u5c06\u6839\u636e <code>outputs</code> \u53c2\u6570\u548c <code>parse_mode</code> \u53c2\u6570\u81ea\u52a8\u5236\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002\u4f46\u662f\uff0c<code>PromptTemplate</code> \u4e5f\u652f\u6301\u901a\u8fc7\u6307\u5b9a <code>PromptTemplate.format(custom_output_format=\"...\")</code> \u6765\u660e\u786e\u6307\u5b9a\u8f93\u51fa\u683c\u5f0f\u3002</p> </li> </ol>"},{"location":"zh/modules/customize_agent.html#\u89e3\u6790\u6a21\u5f0f","title":"\u89e3\u6790\u6a21\u5f0f","text":"<p>CustomizeAgent \u652f\u6301\u4e0d\u540c\u7684\u65b9\u5f0f\u6765\u89e3\u6790 LLM \u8f93\u51fa\uff1a</p>"},{"location":"zh/modules/customize_agent.html#1-\u5b57\u7b26\u4e32\u6a21\u5f0f-parse_modestr","title":"1. \u5b57\u7b26\u4e32\u6a21\u5f0f (<code>parse_mode=\"str\"</code>)","text":"<p>\u4f7f\u7528\u539f\u59cb LLM \u8f93\u51fa\u4f5c\u4e3a\u6bcf\u4e2a\u8f93\u51fa\u5b57\u6bb5\u7684\u503c\u3002\u9002\u7528\u4e8e\u4e0d\u9700\u8981\u7ed3\u6784\u5316\u89e3\u6790\u7684\u7b80\u5355\u4ee3\u7406\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"SimpleAgent\",\n    description=\"Returns raw output\",\n    prompt=\"Generate a greeting for {name}\",\n    inputs=[{\"name\": \"name\", \"type\": \"string\", \"description\": \"The name to greet\"}],\n    outputs=[{\"name\": \"greeting\", \"type\": \"string\", \"description\": \"The generated greeting\"}],\n    parse_mode=\"str\",\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u6267\u884c\u4ee3\u7406\u540e\uff0c\u60a8\u53ef\u4ee5\u901a\u8fc7 <code>message.content.content</code> \u6216 <code>message.content.greeting</code> \u8bbf\u95ee\u539f\u59cb LLM \u54cd\u5e94\u3002</p>"},{"location":"zh/modules/customize_agent.html#2-\u6807\u9898\u6a21\u5f0f-parse_modetitle\u9ed8\u8ba4","title":"2. \u6807\u9898\u6a21\u5f0f (<code>parse_mode=\"title\"</code>\uff0c\u9ed8\u8ba4)","text":"<p>\u63d0\u53d6\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684\u6807\u9898\u4e4b\u95f4\u7684\u5185\u5bb9\u3002\u8fd9\u662f\u9ed8\u8ba4\u7684\u89e3\u6790\u6a21\u5f0f\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"ReportGenerator\",\n    description=\"Generates a structured report\",\n    prompt=\"Create a report about {topic}\",\n    outputs=[\n        {\"name\": \"summary\", \"type\": \"string\", \"description\": \"Brief summary\"},\n        {\"name\": \"analysis\", \"type\": \"string\", \"description\": \"Detailed analysis\"}\n    ],\n    # \u9ed8\u8ba4\u6807\u9898\u6a21\u5f0f\u662f \"## {title}\"\n    title_format=\"### {title}\",  # \u53ef\u9009\uff1a\u81ea\u5b9a\u4e49\u6807\u9898\u683c\u5f0f\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u4f7f\u7528\u6b64\u914d\u7f6e\uff0c\u5e94\u8be5\u6307\u793a LLM \u5c06\u5176\u54cd\u5e94\u683c\u5f0f\u5316\u4e3a\uff1a</p> <pre><code>### summary\nBrief summary of the topic here.\n\n### analysis\nDetailed analysis of the topic here.\n</code></pre> <p>Note</p> <p>LLM \u8f93\u51fa\u7684\u7ae0\u8282\u6807\u9898\u5e94\u8be5\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u76f8\u540c\u3002\u5426\u5219\uff0c\u89e3\u6790\u5c06\u5931\u8d25\u3002\u4f8b\u5982\uff0c\u5728\u4e0a\u9762\u7684\u4f8b\u5b50\u4e2d\uff0c\u5982\u679c LLM \u8f93\u51fa <code>### Analysis</code>\uff0c\u8fd9\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0 <code>analysis</code> \u4e0d\u540c\uff0c\u89e3\u6790\u5c06\u5931\u8d25\u3002</p>"},{"location":"zh/modules/customize_agent.html#3-json-\u6a21\u5f0f-parse_modejson","title":"3. JSON \u6a21\u5f0f (<code>parse_mode=\"json\"</code>)","text":"<p>\u89e3\u6790 LLM \u8f93\u51fa\u7684 JSON \u5b57\u7b26\u4e32\u3002JSON \u5b57\u7b26\u4e32\u7684\u952e\u5e94\u8be5\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u76f8\u540c\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n        {\"name\": \"places\", \"type\": \"string\", \"description\": \"Locations mentioned\"},\n        {\"name\": \"dates\", \"type\": \"string\", \"description\": \"Dates mentioned\"}\n    ],\n    parse_mode=\"json\",\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u4f7f\u7528\u6b64\u6a21\u5f0f\u65f6\uff0cLLM \u5e94\u8be5\u8f93\u51fa\u4e00\u4e2a\u6709\u6548\u7684 JSON \u5b57\u7b26\u4e32\uff0c\u5176\u952e\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u3002\u4f8b\u5982\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\uff1a</p> <pre><code>{\n    \"people\": \"extracted people\",\n    \"places\": \"extracted places\",\n    \"dates\": \"extracted dates\"\n}\n</code></pre> <p>\u5982\u679c LLM \u54cd\u5e94\u4e2d\u6709\u591a\u4e2a JSON \u5b57\u7b26\u4e32\uff0c\u5c06\u53ea\u4f7f\u7528\u7b2c\u4e00\u4e2a\u3002</p>"},{"location":"zh/modules/customize_agent.html#4-xml-\u6a21\u5f0f-parse_modexml","title":"4. XML \u6a21\u5f0f (<code>parse_mode=\"xml\"</code>)","text":"<p>\u89e3\u6790 LLM \u8f93\u51fa\u7684 XML \u5b57\u7b26\u4e32\u3002XML \u5b57\u7b26\u4e32\u7684\u952e\u5e94\u8be5\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u76f8\u540c\u3002</p> <pre><code>agent = CustomizeAgent(\n    name=\"DataExtractor\",\n    description=\"Extracts structured data\",\n    prompt=\"Extract key information from this text: {text}\",\n    inputs=[\n        {\"name\": \"text\", \"type\": \"string\", \"description\": \"The text to extract information from\"}\n    ],\n    outputs=[\n        {\"name\": \"people\", \"type\": \"string\", \"description\": \"Names of people mentioned\"},\n    ],\n    parse_mode=\"xml\",\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>\u4f7f\u7528\u6b64\u6a21\u5f0f\u65f6\uff0cLLM \u5e94\u8be5\u751f\u6210\u5305\u542b\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684 XML \u6807\u7b7e\u7684\u6587\u672c\u3002\u4f8b\u5982\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\uff1a</p> <pre><code>The people mentioned in the text are: &lt;people&gt;John Doe and Jane Smith&lt;/people&gt;.\n</code></pre> <p>\u5982\u679c LLM \u8f93\u51fa\u5305\u542b\u591a\u4e2a\u5177\u6709\u76f8\u540c\u540d\u79f0\u7684 XML \u6807\u7b7e\uff0c\u5c06\u53ea\u4f7f\u7528\u7b2c\u4e00\u4e2a\u3002</p>"},{"location":"zh/modules/customize_agent.html#5-\u81ea\u5b9a\u4e49\u89e3\u6790-parse_modecustom","title":"5. \u81ea\u5b9a\u4e49\u89e3\u6790 (<code>parse_mode=\"custom\"</code>)","text":"<p>\u4e3a\u4e86\u83b7\u5f97\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u60a8\u53ef\u4ee5\u5b9a\u4e49\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\uff1a</p> <pre><code>from evoagentx.core.registry import register_parse_function\n\n@register_parse_function  # \u6ce8\u518c\u51fd\u6570\u4ee5\u4fbf\u5e8f\u5217\u5316\ndef extract_python_code(content: str) -&gt; dict:\n    \"\"\"\u4ece LLM \u54cd\u5e94\u4e2d\u63d0\u53d6 Python \u4ee3\u7801\"\"\"\n    code_blocks = extract_code_blocks(content)\n    return {\"code\": code_blocks[0] if code_blocks else \"\"}\n\nagent = CustomizeAgent(\n    name=\"CodeExplainer\",\n    description=\"Generates and explains code\",\n    prompt=\"Write a Python function that {requirement}\",\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The requirement to generate code for\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated code\"},\n    ],\n    parse_mode=\"custom\",\n    parse_func=extract_python_code,\n    # \u5176\u4ed6\u53c2\u6570...\n)\n</code></pre> <p>Note</p> <ol> <li> <p>\u89e3\u6790\u51fd\u6570\u5e94\u8be5\u6709\u4e00\u4e2a\u8f93\u5165\u53c2\u6570 <code>content</code>\uff0c\u5b83\u63a5\u6536\u539f\u59cb LLM \u54cd\u5e94\u4f5c\u4e3a\u8f93\u5165\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u952e\u4e0e\u8f93\u51fa\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u3002</p> </li> <li> <p>\u5efa\u8bae\u4f7f\u7528 <code>@register_parse_function</code> \u88c5\u9970\u5668\u6765\u6ce8\u518c\u89e3\u6790\u51fd\u6570\u4ee5\u4fbf\u5e8f\u5217\u5316\uff0c\u8fd9\u6837\u60a8\u5c31\u53ef\u4ee5\u4fdd\u5b58\u4ee3\u7406\u5e76\u5728\u4ee5\u540e\u52a0\u8f7d\u5b83\u3002</p> </li> </ol>"},{"location":"zh/modules/customize_agent.html#\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","text":"<p>\u60a8\u53ef\u4ee5\u4fdd\u5b58\u4ee3\u7406\u5b9a\u4e49\u4ee5\u4fbf\u4ee5\u540e\u91cd\u7528\uff1a</p> <pre><code># \u4fdd\u5b58\u4ee3\u7406\u914d\u7f6e\u3002\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c`llm_config` \u4e0d\u4f1a\u88ab\u4fdd\u5b58\u3002\ncode_writer.save_module(\"./agents/code_writer.json\")\n\n# \u4ece\u6587\u4ef6\u52a0\u8f7d\u4ee3\u7406\uff08\u9700\u8981\u518d\u6b21\u63d0\u4f9b llm_config\uff09\nloaded_agent = CustomizeAgent.from_file(\n    \"./agents/code_writer.json\", \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"zh/modules/customize_agent.html#\u9ad8\u7ea7\u793a\u4f8b\u591a\u6b65\u9aa4\u4ee3\u7801\u751f\u6210\u5668","title":"\u9ad8\u7ea7\u793a\u4f8b\uff1a\u591a\u6b65\u9aa4\u4ee3\u7801\u751f\u6210\u5668","text":"<p>\u4ee5\u4e0b\u662f\u4e00\u4e2a\u66f4\u9ad8\u7ea7\u7684\u793a\u4f8b\uff0c\u5c55\u793a\u4e86\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa\u7684\u4e13\u95e8\u4ee3\u7801\u751f\u6210\u4ee3\u7406\uff1a</p> <pre><code>from pydantic import Field\nfrom evoagentx.actions import ActionOutput\nfrom evoagentx.core.registry import register_parse_function\n\nclass CodeGeneratorOutput(ActionOutput):\n    code: str = Field(description=\"The generated Python code\")\n    documentation: str = Field(description=\"Documentation for the code\")\n    tests: str = Field(description=\"Unit tests for the code\")\n\n@register_parse_function\ndef parse_code_documentation_tests(content: str) -&gt; dict:\n    \"\"\"\u5c06 LLM \u8f93\u51fa\u89e3\u6790\u4e3a\u4ee3\u7801\u3001\u6587\u6863\u548c\u6d4b\u8bd5\u90e8\u5206\"\"\"\n    sections = content.split(\"## \")\n    result = {\"code\": \"\", \"documentation\": \"\", \"tests\": \"\"}\n\n    for section in sections:\n        if not section.strip():\n            continue\n\n        lines = section.strip().split(\"\\n\")\n        section_name = lines[0].lower()\n        section_content = \"\\n\".join(lines[1:]).strip()\n\n        if \"code\" in section_name:\n            # \u4ece\u4ee3\u7801\u5757\u4e2d\u63d0\u53d6\u4ee3\u7801\n            code_blocks = extract_code_blocks(section_content)\n            result[\"code\"] = code_blocks[0] if code_blocks else section_content\n        elif \"documentation\" in section_name:\n            result[\"documentation\"] = section_content\n        elif \"test\" in section_name:\n            # \u5982\u679c\u5b58\u5728\uff0c\u4ece\u4ee3\u7801\u5757\u4e2d\u63d0\u53d6\u4ee3\u7801\n            code_blocks = extract_code_blocks(section_content)\n            result[\"tests\"] = code_blocks[0] if code_blocks else section_content\n\n    return result\n\n# \u521b\u5efa\u9ad8\u7ea7\u4ee3\u7801\u751f\u6210\u5668\u4ee3\u7406\nadvanced_generator = CustomizeAgent(\n    name=\"AdvancedCodeGenerator\",\n    description=\"Generates complete code packages with documentation and tests\",\n    prompt=\"\"\"\n    Create a complete implementation based on this requirement:\n    {requirement}\n\n    Provide your response in the following format:\n\n    ## Code\n    [Include the Python code implementation here]\n\n    ## Documentation\n    [Include clear documentation explaining the code]\n\n    ## Tests\n    [Include unit tests that verify the code works correctly]\n    \"\"\",\n    llm_config=openai_config,\n    inputs=[\n        {\"name\": \"requirement\", \"type\": \"string\", \"description\": \"The coding requirement\"}\n    ],\n    outputs=[\n        {\"name\": \"code\", \"type\": \"string\", \"description\": \"The generated Python code\"},\n        {\"name\": \"documentation\", \"type\": \"string\", \"description\": \"Documentation for the code\"},\n        {\"name\": \"tests\", \"type\": \"string\", \"description\": \"Unit tests for the code\"}\n    ],\n    output_parser=CodeGeneratorOutput,\n    parse_mode=\"custom\",\n    parse_func=parse_code_documentation_tests,\n    system_prompt=\"You are an expert Python developer specialized in writing clean, efficient code with comprehensive documentation and tests.\"\n)\n\n# \u6267\u884c\u4ee3\u7406\nresult = advanced_generator(\n    inputs={\n        \"requirement\": \"Create a function to validate if a string is a valid email address\"\n    }\n)\n\n# \u8bbf\u95ee\u7ed3\u6784\u5316\u8f93\u51fa\nprint(\"CODE:\")\nprint(result.content.code)\nprint(\"\\nDOCUMENTATION:\")\nprint(result.content.documentation)\nprint(\"\\nTESTS:\")\nprint(result.content.tests)\n</code></pre> <p>\u8fd9\u4e2a\u9ad8\u7ea7\u793a\u4f8b\u5c55\u793a\u4e86\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u5b83\u53ef\u4ee5\u4ece\u5355\u4e2a LLM \u8c03\u7528\u4e2d\u4ea7\u751f\u591a\u4e2a\u7ed3\u6784\u5316\u8f93\u51fa\uff0c\u63d0\u4f9b\u5305\u542b\u5b9e\u73b0\u3001\u6587\u6863\u548c\u6d4b\u8bd5\u7684\u5b8c\u6574\u4ee3\u7801\u5305\u3002</p>"},{"location":"zh/modules/evaluator.html","title":"\u8bc4\u4f30\u5668","text":""},{"location":"zh/modules/evaluator.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>Evaluator</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7528\u4e8e\u8bc4\u4f30\u5de5\u4f5c\u6d41\u548c\u52a8\u4f5c\u56fe\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u7684\u6027\u80fd\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u8861\u91cf AI \u4ee3\u7406\u5728\u7279\u5b9a\u4efb\u52a1\u4e0a\u7684\u8868\u73b0\uff0c\u901a\u8fc7\u8fd0\u884c\u6d4b\u8bd5\u6570\u636e\u5e76\u8ba1\u7b97\u6307\u6807\u3002</p>"},{"location":"zh/modules/evaluator.html#\u67b6\u6784","title":"\u67b6\u6784","text":""},{"location":"zh/modules/evaluator.html#\u8bc4\u4f30\u5668\u67b6\u6784","title":"\u8bc4\u4f30\u5668\u67b6\u6784","text":"<p><code>Evaluator</code> \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>LLM \u5b9e\u4f8b\uff1a</p> <p>\u7528\u4e8e\u5728\u8bc4\u4f30\u671f\u95f4\u6267\u884c\u5de5\u4f5c\u6d41\u7684\u8bed\u8a00\u6a21\u578b\uff1a</p> <ul> <li>\u63d0\u4f9b\u5de5\u4f5c\u6d41\u6267\u884c\u6240\u9700\u7684\u63a8\u7406\u548c\u751f\u6210\u80fd\u529b</li> <li>\u53ef\u4ee5\u662f\u4efb\u4f55\u9075\u5faa <code>BaseLLM</code> \u63a5\u53e3\u7684\u5b9e\u73b0</li> </ul> </li> <li> <p>\u4ee3\u7406\u7ba1\u7406\u5668\uff1a</p> <p>\u7ba1\u7406\u8bc4\u4f30\u671f\u95f4\u5de5\u4f5c\u6d41\u56fe\u4f7f\u7528\u7684\u4ee3\u7406\uff1a</p> <ul> <li>\u63d0\u4f9b\u5de5\u4f5c\u6d41\u6267\u884c\u6240\u9700\u7684\u4ee3\u7406\u8bbf\u95ee</li> <li>\u4ec5\u5728\u8bc4\u4f30 <code>WorkFlowGraph</code> \u5b9e\u4f8b\u65f6\u9700\u8981\uff0c\u8bc4\u4f30 <code>ActionGraph</code> \u5b9e\u4f8b\u65f6\u53ef\u4ee5\u5ffd\u7565</li> </ul> </li> <li> <p>\u6570\u636e\u5904\u7406\u51fd\u6570\uff1a</p> <p>\u5728\u8bc4\u4f30\u671f\u95f4\u51c6\u5907\u548c\u5904\u7406\u6570\u636e\u7684\u51fd\u6570\uff1a</p> <ul> <li><code>collate_func</code>\uff1a\u4e3a\u5de5\u4f5c\u6d41\u6267\u884c\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b</li> <li><code>output_postprocess_func</code>\uff1a\u5728\u8bc4\u4f30\u524d\u5904\u7406\u5de5\u4f5c\u6d41\u8f93\u51fa</li> </ul> </li> </ol>"},{"location":"zh/modules/evaluator.html#\u8bc4\u4f30\u6d41\u7a0b","title":"\u8bc4\u4f30\u6d41\u7a0b","text":"<p>\u8bc4\u4f30\u6d41\u7a0b\u9075\u5faa\u4ee5\u4e0b\u6b65\u9aa4\uff1a</p> <ol> <li>\u6570\u636e\u5904\u7406\uff1a\u4ece\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u83b7\u53d6\u793a\u4f8b\uff0c\u5e76\u5c06\u5176\u5904\u7406\u6210\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u671f\u671b\u7684\u683c\u5f0f</li> <li>\u5de5\u4f5c\u6d41\u6267\u884c\uff1a\u901a\u8fc7\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u8fd0\u884c\u6bcf\u4e2a\u793a\u4f8b</li> <li>\u8f93\u51fa\u5904\u7406\uff1a\u5c06\u8f93\u51fa\u5904\u7406\u6210\u57fa\u51c6\u6d4b\u8bd5\u671f\u671b\u7684\u683c\u5f0f</li> <li>\u6307\u6807\u8ba1\u7b97\uff1a\u901a\u8fc7\u6bd4\u8f83\u8f93\u51fa\u4e0e\u771f\u5b9e\u503c\u6765\u8ba1\u7b97\u6027\u80fd\u6307\u6807</li> <li>\u7ed3\u679c\u805a\u5408\uff1a\u5c06\u5355\u4e2a\u6307\u6807\u805a\u5408\u6210\u6574\u4f53\u6027\u80fd\u5206\u6570</li> </ol>"},{"location":"zh/modules/evaluator.html#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/evaluator.html#\u57fa\u672c\u8bc4\u4f30\u5668\u521b\u5efa\u4e0e\u6267\u884c","title":"\u57fa\u672c\u8bc4\u4f30\u5668\u521b\u5efa\u4e0e\u6267\u884c","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316 LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# \u521d\u59cb\u5316\u4ee3\u7406\u7ba1\u7406\u5668\nagent_manager = AgentManager()\n\n# \u52a0\u8f7d\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\n\n# \u5c06\u4ee3\u7406\u6dfb\u52a0\u5230\u4ee3\u7406\u7ba1\u7406\u5668\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# \u521b\u5efa\u57fa\u51c6\u6d4b\u8bd5\nbenchmark = SomeBenchmark()\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4,  # \u4f7f\u7528 4 \u4e2a\u5e76\u884c\u5de5\u4f5c\u5668\n    verbose=True    # \u663e\u793a\u8fdb\u5ea6\u6761\n)\n\n# \u8fd0\u884c\u8bc4\u4f30\u5e76\u6291\u5236\u65e5\u5fd7\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow_graph,\n        benchmark=benchmark,\n        eval_mode=\"test\",    # \u5728\u6d4b\u8bd5\u96c6\u4e0a\u8bc4\u4f30\uff08\u9ed8\u8ba4\uff09\n        sample_k=100         # \u4f7f\u7528 100 \u4e2a\u968f\u673a\u793a\u4f8b\n    )\n\nprint(f\"\u8bc4\u4f30\u7ed3\u679c: {results}\")\n</code></pre>"},{"location":"zh/modules/evaluator.html#\u81ea\u5b9a\u4e49\u6570\u636e\u5904\u7406","title":"\u81ea\u5b9a\u4e49\u6570\u636e\u5904\u7406","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u81ea\u5b9a\u4e49\u6574\u7406\u51fd\u6570\u6765\u51c6\u5907\u8f93\u5165\u3002\u952e\u5e94\u8be5\u5339\u914d\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u7684\u8f93\u5165\u53c2\u6570\u3002\u8fd4\u56de\u503c\u5c06\u76f4\u63a5\u4f20\u9012\u7ed9\u5de5\u4f5c\u6d41\u56fe\u6216\u52a8\u4f5c\u56fe\u7684 `execute` \u65b9\u6cd5\u3002\ndef custom_collate(example):\n    return {\n        \"input_text\": example[\"question\"],\n        \"context\": example.get(\"context\", \"\")\n    }\n\n# \u81ea\u5b9a\u4e49\u8f93\u51fa\u5904\u7406\uff0c`output` \u662f\u5de5\u4f5c\u6d41\u7684\u8f93\u51fa\uff0c\u8fd4\u56de\u503c\u5c06\u4f20\u9012\u7ed9\u57fa\u51c6\u6d4b\u8bd5\u7684 `evaluate` \u65b9\u6cd5\u3002\ndef custom_postprocess(output):\n    if isinstance(output, dict):\n        return output.get(\"answer\", \"\")\n    return output\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u51fd\u6570\u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    collate_func=custom_collate,\n    output_postprocess_func=custom_postprocess,\n    num_workers=4,  # \u4f7f\u7528 4 \u4e2a\u5e76\u884c\u5de5\u4f5c\u5668\n    verbose=True    # \u663e\u793a\u8fdb\u5ea6\u6761\n)\n</code></pre>"},{"location":"zh/modules/evaluator.html#\u8bc4\u4f30\u52a8\u4f5c\u56fe","title":"\u8bc4\u4f30\u52a8\u4f5c\u56fe","text":"<pre><code>from evoagentx.workflow.action_graph import ActionGraph\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316 LLM\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\n\n# \u52a0\u8f7d\u52a8\u4f5c\u56fe\naction_graph = ActionGraph.from_file(\"path/to/action_graph.json\", llm_config=llm_config)\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\uff08\u52a8\u4f5c\u56fe\u4e0d\u9700\u8981 agent_manager\uff09\nevaluator = Evaluator(llm=llm, num_workers=4, verbose=True)\n\n# \u8fd0\u884c\u8bc4\u4f30\u5e76\u6291\u5236\u65e5\u5fd7\nwith suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=action_graph,\n        benchmark=benchmark\n    )\n</code></pre>"},{"location":"zh/modules/evaluator.html#\u5f02\u6b65\u8bc4\u4f30","title":"\u5f02\u6b65\u8bc4\u4f30","text":"<pre><code>import asyncio\nfrom evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.workflow.workflow_graph import WorkFlowGraph\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316 LLM \u548c\u7ec4\u4ef6\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nagent_manager = AgentManager()\nworkflow_graph = WorkFlowGraph.from_file(\"path/to/workflow.json\")\nbenchmark = SomeBenchmark()\n\n# \u521b\u5efa\u8bc4\u4f30\u5668\nevaluator = Evaluator(\n    llm=llm,\n    agent_manager=agent_manager,\n    num_workers=4\n)\n\n# \u8fd0\u884c\u5f02\u6b65\u8bc4\u4f30\nasync def run_async_eval():\n    with suppress_logger_info():\n        results = await evaluator.async_evaluate(\n            graph=workflow_graph,\n            benchmark=benchmark\n        )\n    return results\n\n# \u6267\u884c\u5f02\u6b65\u8bc4\u4f30\nresults = asyncio.run(run_async_eval())\n</code></pre>"},{"location":"zh/modules/evaluator.html#\u8bbf\u95ee\u8bc4\u4f30\u8bb0\u5f55","title":"\u8bbf\u95ee\u8bc4\u4f30\u8bb0\u5f55","text":"<pre><code>from evoagentx.evaluators import Evaluator\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.benchmark import SomeBenchmark\nfrom evoagentx.core.callbacks import suppress_logger_info\n\n# \u521d\u59cb\u5316\u7ec4\u4ef6\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\")\nllm = OpenAILLM(llm_config)\nbenchmark = SomeBenchmark()\nevaluator = Evaluator(llm=llm)\n\n# \u8fd0\u884c\u8bc4\u4f30\u5e76\u6291\u5236\u65e5\u5fd7\nwith suppress_logger_info():\n    evaluator.evaluate(graph=graph, benchmark=benchmark)\n\n# \u83b7\u53d6\u6240\u6709\u8bc4\u4f30\u8bb0\u5f55\nall_records = evaluator.get_all_evaluation_records()\n\n# \u83b7\u53d6\u7279\u5b9a\u793a\u4f8b\u7684\u8bb0\u5f55\nexample = benchmark.get_test_data()[0]\nrecord = evaluator.get_example_evaluation_record(benchmark, example)\n\n# \u901a\u8fc7\u793a\u4f8b ID \u83b7\u53d6\u8bb0\u5f55\nrecord_by_id = evaluator.get_evaluation_record_by_id(\n    benchmark=benchmark,\n    example_id=\"example-123\",\n    eval_mode=\"test\"\n)\n\n# \u8bbf\u95ee\u5de5\u4f5c\u6d41\u56fe\u8bc4\u4f30\u7684\u8f68\u8ff9\nif \"trajectory\" in record:\n    for message in record[\"trajectory\"]:\n        print(f\"{message.role}: {message.content}\")\n</code></pre> <p><code>Evaluator</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u5f3a\u5927\u7684\u65b9\u5f0f\u6765\u8bc4\u4f30\u5de5\u4f5c\u6d41\u548c\u52a8\u4f5c\u56fe\u7684\u6027\u80fd\uff0c\u4f7f EvoAgentX \u6846\u67b6\u4e2d\u7684\u5b9a\u91cf\u6bd4\u8f83\u548c\u6539\u8fdb\u8ddf\u8e2a\u6210\u4e3a\u53ef\u80fd\u3002</p>"},{"location":"zh/modules/llm.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p>LLM\uff08\u5927\u8bed\u8a00\u6a21\u578b\uff09\u6a21\u5757\u4e3a EvoAgentX \u6846\u67b6\u63d0\u4f9b\u4e86\u4e0e\u5404\u79cd\u8bed\u8a00\u6a21\u578b\u63d0\u4f9b\u5546\u4ea4\u4e92\u7684\u7edf\u4e00\u63a5\u53e3\u3002\u5b83\u62bd\u8c61\u4e86\u7279\u5b9a\u63d0\u4f9b\u5546\u7684\u5b9e\u73b0\u7ec6\u8282\uff0c\u4e3a\u751f\u6210\u6587\u672c\u3001\u7ba1\u7406\u6210\u672c\u548c\u5904\u7406\u54cd\u5e94\u63d0\u4f9b\u4e86\u4e00\u81f4\u7684 API\u3002</p>"},{"location":"zh/modules/llm.html#\u652f\u6301\u7684-llm-\u63d0\u4f9b\u5546","title":"\u652f\u6301\u7684 LLM \u63d0\u4f9b\u5546","text":"<p>EvoAgentX \u76ee\u524d\u652f\u6301\u4ee5\u4e0b LLM \u63d0\u4f9b\u5546\uff1a</p>"},{"location":"zh/modules/llm.html#openaillm","title":"OpenAILLM","text":"<p>\u8fd9\u662f\u8bbf\u95ee OpenAI \u8bed\u8a00\u6a21\u578b\u7684\u4e3b\u8981\u5b9e\u73b0\u3002\u5b83\u5904\u7406 GPT-4\u3001GPT-3.5-Turbo \u548c\u5176\u4ed6 OpenAI \u6a21\u578b\u7684\u8ba4\u8bc1\u3001\u8bf7\u6c42\u683c\u5f0f\u5316\u548c\u54cd\u5e94\u89e3\u6790\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <pre><code>from evoagentx.models import OpenAILLMConfig, OpenAILLM\n\n# Configure the model\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",  \n    openai_key=\"your-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = OpenAILLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Explain quantum computing in simple terms.\",\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)\n</code></pre>"},{"location":"zh/modules/llm.html#litellm","title":"LiteLLM","text":"<p>LiteLLM \u662f LiteLLM \u9879\u76ee \u7684\u9002\u914d\u5668\uff0c\u8be5\u9879\u76ee\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684 Python SDK \u548c\u4ee3\u7406\u670d\u52a1\u5668\uff0c\u7528\u4e8e\u4f7f\u7528 OpenAI API \u683c\u5f0f\u8c03\u7528\u8d85\u8fc7 100 \u4e2a LLM API\u3002\u5b83\u652f\u6301 Bedrock\u3001Azure\u3001OpenAI\u3001VertexAI\u3001Cohere\u3001Anthropic\u3001Sagemaker\u3001HuggingFace\u3001Replicate \u548c Groq \u7b49\u63d0\u4f9b\u5546\u3002\u591a\u4e8f\u4e86\u8fd9\u4e2a\u9879\u76ee\uff0cEvoAgentX \u4e2d\u7684 <code>LiteLLM</code> \u6a21\u578b\u7c7b\u53ef\u4ee5\u901a\u8fc7\u5355\u4e00\u63a5\u53e3\u65e0\u7f1d\u8bbf\u95ee\u5404\u79cd LLM \u63d0\u4f9b\u5546\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <p>\u4e3a\u4e86\u4e0e LiteLLM \u65e0\u7f1d\u96c6\u6210\uff0c\u60a8\u5e94\u8be5\u4f7f\u7528 LiteLLM \u5e73\u53f0\u5b9a\u4e49\u7684\u547d\u540d\u7ea6\u5b9a\u6765\u6307\u5b9a\u6a21\u578b\u540d\u79f0\u3002\u4f8b\u5982\uff0c\u5bf9\u4e8e Claude 3.0 Opus\uff0c\u60a8\u9700\u8981\u6307\u5b9a <code>anthropic/claude-3-opus-20240229</code>\u3002\u60a8\u53ef\u4ee5\u5728\u5176\u5b98\u65b9\u6587\u6863\u4e2d\u627e\u5230\u652f\u6301\u7684\u63d0\u4f9b\u5546\u548c\u6a21\u578b\u540d\u79f0\u7684\u5b8c\u6574\u5217\u8868\uff1ahttps://docs.litellm.ai/docs/providers\u3002</p> <pre><code>from evoagentx.models import LiteLLMConfig, LiteLLM\n\n# Configure the model\nconfig = LiteLLMConfig(\n    model=\"anthropic/claude-3-opus-20240229\", \n    anthropic_key=\"your-anthropic-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = LiteLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Design a system for autonomous vehicles.\",\n    system_message=\"You are an expert in autonomous systems design.\"\n)\n</code></pre>"},{"location":"zh/modules/llm.html#siliconflowllm","title":"SiliconFlowLLM","text":"<p>SiliconFlowLLM \u662f SiliconFlow \u5e73\u53f0 \u4e0a\u6258\u7ba1\u6a21\u578b\u7684\u9002\u914d\u5668\uff0c\u8be5\u5e73\u53f0\u901a\u8fc7 OpenAI \u517c\u5bb9\u7684 API \u63d0\u4f9b\u5bf9\u5f00\u6e90\u548c\u4e13\u6709\u6a21\u578b\u7684\u8bbf\u95ee\u3002\u5b83\u4f7f\u60a8\u80fd\u591f\u901a\u8fc7\u4f7f\u7528 SiliconFlow \u5e73\u53f0\u7684\u547d\u540d\u7ea6\u5b9a\u6307\u5b9a\u6a21\u578b\u540d\u79f0\u6765\u96c6\u6210 Qwen\u3001DeepSeek \u6216 Mixtral \u7b49\u6a21\u578b\u3002</p> <p>\u5f97\u76ca\u4e8e SiliconFlow \u7684\u7edf\u4e00\u63a5\u53e3\uff0cEvoAgentX \u4e2d\u7684 <code>SiliconFlowLLM</code> \u6a21\u578b\u7c7b\u5141\u8bb8\u4f7f\u7528\u76f8\u540c\u7684 API \u683c\u5f0f\u5728 SiliconFlow \u4e0a\u6258\u7ba1\u7684\u5404\u79cd\u5f3a\u5927 LLM \u4e4b\u95f4\u65e0\u7f1d\u5207\u6362\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <pre><code>from evoagentx.models import SiliconFlowConfig, SiliconFlowLLM\n\n# Configure the model\nconfig = SiliconFlowConfig(\n    model=\"deepseek-ai/DeepSeek-V3\",\n    siliconflow_key=\"your-siliconflow-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = SiliconFlowLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Write a poem about artificial intelligence.\",\n    system_message=\"You are a creative poet.\"\n)\n</code></pre>"},{"location":"zh/modules/llm.html#openrouterllm","title":"OpenRouterLLM","text":"<p>OpenRouterLLM \u662f OpenRouter \u5e73\u53f0 \u7684\u9002\u914d\u5668\uff0c\u8be5\u5e73\u53f0\u901a\u8fc7\u7edf\u4e00\u7684 API \u63d0\u4f9b\u5bf9\u5404\u79cd\u63d0\u4f9b\u5546\u7684\u8bed\u8a00\u6a21\u578b\u7684\u8bbf\u95ee\u3002\u5b83\u652f\u6301\u6765\u81ea Anthropic\u3001Google\u3001Meta\u3001Mistral AI \u7b49\u63d0\u4f9b\u5546\u7684\u6a21\u578b\uff0c\u6240\u6709\u8fd9\u4e9b\u90fd\u53ef\u4ee5\u901a\u8fc7\u5355\u4e00\u63a5\u53e3\u8bbf\u95ee\u3002</p> <p>EvoAgentX \u4e2d\u7684 <code>OpenRouterLLM</code> \u6a21\u578b\u7c7b\u4f7f\u60a8\u80fd\u591f\u8f7b\u677e\u5730\u5728 OpenRouter \u4e0a\u6258\u7ba1\u7684\u4e0d\u540c\u6a21\u578b\u4e4b\u95f4\u5207\u6362\uff0c\u540c\u65f6\u4fdd\u6301\u4e00\u81f4\u7684 API \u683c\u5f0f\u3002\u8fd9\u4f7f\u5f97\u60a8\u53ef\u4ee5\u8f7b\u677e\u5c1d\u8bd5\u4e0d\u540c\u7684\u6a21\u578b\uff0c\u4e3a\u60a8\u7684\u7279\u5b9a\u7528\u4f8b\u627e\u5230\u6700\u4f73\u9009\u62e9\u3002</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <pre><code>from evoagentx.models import OpenRouterConfig, OpenRouterLLM\n\n# Configure the model\nconfig = OpenRouterConfig(\n    model=\"openai/gpt-4o-mini\",  # \u6216 OpenRouter \u652f\u6301\u7684\u4efb\u4f55\u5176\u4ed6\u6a21\u578b\n    openrouter_key=\"your-openrouter-api-key\",\n    temperature=0.7,\n    max_tokens=1000\n)\n\n# Initialize the model\nllm = OpenRouterLLM(config=config)\n\n# Generate text\nresponse = llm.generate(\n    prompt=\"Analyze the impact of artificial intelligence on healthcare.\",\n    system_message=\"You are an AI ethics expert specializing in healthcare applications.\"\n)\n</code></pre>"},{"location":"zh/modules/llm.html#aliyun-llm","title":"Aliyun LLM","text":"<p>AliyunLLM \u662f EvoAgentX \u6846\u67b6\u7684\u5b9e\u73b0\uff0c\u7528\u4e8e\u8bbf\u95ee Aliyun\u901a\u4e49\u5343\u6587\u7cfb\u5217\u6a21\u578b\u3002\u5b83\u63d0\u4f9b\u4e86\u4e0e Aliyun DashScope API \u7684\u65e0\u7f1d\u96c6\u6210\uff0c\u5e76\u652f\u6301 Tongyi Qianqian \u7684\u591a\u79cd\u6a21\u578b\uff0c\u5305\u62ec qwen-turbo\u3001qwen-plus\u3001qwen-max \u7b49\u3002\u6211\u4eec\u5df2\u63d0\u4f9b\u53c2\u8003\u6210\u672c\u4f9b\u60a8\u53c2\u8003\uff1b\u7136\u800c\uff0c\u8bf7\u4ee5\u5b9e\u9645\u8d39\u7528\u4e3a\u51c6\u3002</p> <p>\u8981\u4f7f\u7528 DashScope API \u4e0e AliyunLLM \u8fdb\u884c\u96c6\u6210\uff0c\u9700\u8981\u4e00\u4e2a\u6765\u81ea\u767e\u70bc\u5e73\u53f0\u7684 API \u5bc6\u94a5\u3002\u4ee5\u4e0b\u6b65\u9aa4\u6982\u8ff0\u4e86\u8be5\u8fc7\u7a0b\uff1a</p> <p>\u57fa\u672c\u7528\u6cd5</p> <p>\u5728\u60a8\u7684 bash \u7ec8\u7aef\u4e2d\u6267\u884c\u4ee5\u4e0b\u547d\u4ee4\u4ee5\u8bbe\u7f6e API \u5bc6\u94a5\uff1a</p> <pre><code>export DASHSCOPE_API_KEY=\"\u60a8\u7684-api-\u5bc6\u94a5\"\n</code></pre> <p>\u4f7f\u7528python\u8c03\u7528\u6a21\u578b\u7684\u6837\u4f8b\u5982\u4e0b\uff1a</p> <pre><code>from evoagentx.models import AliyunLLM, AliyunLLMConfig\n\n# \u914d\u7f6e\u6a21\u578b\nconfig = AliyunLLMConfig(\n    model=\"qwen-turbo\",  # \u60a8\u53ef\u4ee5\u4f7f\u7528 qwen-turbo\u3001qwen-plus\u3001qwen-max \u7b49\u3002\n    aliyun_api_key=\"\u60a8\u7684 DASHSCOPE_API_KEY\",\n    temperature=0.7,\n    max_tokens=2000,\n    stream=False,\n    output_response=True\n)\n\n# \u521d\u59cb\u5316\u6a21\u578b\nllm = AliyunLLM(config)\n\n# \u751f\u6210\u6587\u672c\nresponse = llm.generate(\n    prompt=\"Explain quantum computing in simple terms.\",\n    system_message=\"You are a helpful assistant that explains complex topics simply.\"\n)\n</code></pre>"},{"location":"zh/modules/llm.html#\u672c\u5730-llm","title":"\u672c\u5730 LLM","text":"<p>\u6211\u4eec\u73b0\u5df2\u652f\u6301\u5728\u4efb\u52a1\u91cd\u4e2d\u8c03\u7528\u672c\u5730\u6a21\u578b\uff0c\u8fd9\u79cd\u65b9\u6cd5\u57fa\u4e8e LiteLLM \u6846\u67b6\u6253\u9020\uff0c\u63d0\u4f9b\u719f\u6089\u7684\u7528\u6237\u4f53\u9a8c\u3002\u4ee5 Ollama \u4e3a\u4f8b\uff0c\u8bf7\u60a8\u53ef\u4ee5\u4ee5\u4e0b\u6b65\u9aa4\u64cd\u4f5c\uff1a</p> <ol> <li>\u4e0b\u8f7d\u60a8\u9700\u8981\u7684\u6a21\u578b\uff0c\u4f8b\u5982 <code>ollama3</code>\u3002</li> <li>\u5728\u672c\u5730\u8fd0\u884c\u8be5\u6a21\u578b\u3002</li> <li>\u914d\u7f6e\u8bbe\u7f6e\uff0c\u6307\u5b9a <code>api_base</code>\uff08\u901a\u5e38\u4e3a\u7aef\u53e3 <code>11434</code>\uff09\u5e76\u5c06 <code>is_local</code> \u8bbe\u7f6e\u4e3a <code>True</code>\u3002</li> </ol> <p>\u73b0\u5728\uff0c\u60a8\u53ef\u4ee5\u65e0\u7f1d\u4f7f\u7528\u672c\u5730\u6a21\u578b\u4e86\uff01</p> <p>\u57fa\u672c\u7528\u6cd5\uff1a</p> <pre><code>from evoagentx.models.model_configs import LiteLLMConfig\nfrom evoagentx.models import LiteLLM\n\n# use local model\nconfig = LiteLLMConfig(\n    model=\"ollama/llama3\",\n    api_base=\"http://localhost:11434\",\n    is_local=True,\n    temperature=0.7,\n    max_tokens=1000,\n    output_response=True\n)\n\n# Generate \nllm = LiteLLM(config)\nresponse = llm.generate(prompt=\"What is Agentic Workflow?\")\n</code></pre>"},{"location":"zh/modules/llm.html#\u6838\u5fc3\u529f\u80fd","title":"\u6838\u5fc3\u529f\u80fd","text":"<p>EvoAgentX \u4e2d\u7684\u6240\u6709 LLM \u5b9e\u73b0\u90fd\u63d0\u4f9b\u4e86\u4e00\u7ec4\u4e00\u81f4\u7684\u6838\u5fc3\u529f\u80fd\uff0c\u7528\u4e8e\u751f\u6210\u6587\u672c\u548c\u7ba1\u7406\u751f\u6210\u8fc7\u7a0b\u3002</p>"},{"location":"zh/modules/llm.html#generate-\u51fd\u6570","title":"Generate \u51fd\u6570","text":"<p><code>generate</code> \u51fd\u6570\u662f\u4f7f\u7528\u8bed\u8a00\u6a21\u578b\u751f\u6210\u6587\u672c\u7684\u4e3b\u8981\u65b9\u6cd5\uff1a</p> <pre><code>def generate(\n    self,\n    prompt: Optional[Union[str, List[str]]] = None,\n    system_message: Optional[Union[str, List[str]]] = None,\n    messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n    parser: Optional[Type[LLMOutputParser]] = None,\n    parse_mode: Optional[str] = \"json\", \n    parse_func: Optional[Callable] = None,\n    **kwargs\n) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    Generate text based on the prompt and optional system message.\n\n    Args:\n        prompt: Input prompt(s) to the LLM.\n        system_message: System message(s) for the LLM.\n        messages: Chat message(s) for the LLM, already in the required format (either `prompt` or `messages` must be provided).\n        parser: Parser class to use for processing the output into a structured format.\n        parse_mode: The mode to use for parsing, must be the `parse_mode` supported by the `parser`. \n        parse_func: A function to apply to the parsed output.\n        **kwargs: Additional generation configuration parameters.\n\n    Returns:\n        For single generation: An LLMOutputParser instance.\n        For batch generation: A list of LLMOutputParser instances.\n    \"\"\"\n</code></pre>"},{"location":"zh/modules/llm.html#\u8f93\u5165\u65b9\u5f0f","title":"\u8f93\u5165\u65b9\u5f0f","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u6709\u51e0\u79cd\u4f7f\u7528 <code>generate</code> \u51fd\u6570\u5411 LLM \u63d0\u4f9b\u8f93\u5165\u7684\u65b9\u5f0f\uff1a</p> <p>\u65b9\u6cd5 1\uff1a\u63d0\u793a\u548c\u7cfb\u7edf\u6d88\u606f</p> <ol> <li> <p>\u63d0\u793a\uff08Prompt\uff09\uff1a\u60a8\u60f3\u8981\u83b7\u5f97\u54cd\u5e94\u7684\u5177\u4f53\u67e5\u8be2\u6216\u6307\u4ee4\u3002</p> </li> <li> <p>\u7cfb\u7edf\u6d88\u606f\uff08System Message\uff09\uff08\u53ef\u9009\uff09\uff1a\u6307\u5bfc\u6a21\u578b\u6574\u4f53\u884c\u4e3a\u548c\u89d2\u8272\u7684\u6307\u4ee4\u3002\u8fd9\u4e3a\u6a21\u578b\u5e94\u8be5\u5982\u4f55\u54cd\u5e94\u8bbe\u7f6e\u4e86\u4e0a\u4e0b\u6587\u3002</p> </li> </ol> <p>\u8fd9\u4e9b\u7ec4\u4ef6\u88ab\u8f6c\u6362\u4e3a\u8bed\u8a00\u6a21\u578b\u53ef\u4ee5\u7406\u89e3\u7684\u6807\u51c6\u6d88\u606f\u683c\u5f0f\uff1a</p> <pre><code># Simple example using prompt and system message\nresponse = llm.generate(\n    prompt=\"What are three ways to improve productivity?\",\n    system_message=\"You are a productivity expert providing concise, actionable advice.\"\n)\n</code></pre> <p>\u5728\u540e\u53f0\uff0c\u8fd9\u88ab\u8f6c\u6362\u4e3a\u5177\u6709\u9002\u5f53\u89d2\u8272\u7684\u6d88\u606f\uff1a</p> <pre><code>messages = [\n    {\"role\": \"system\", \"content\": \"You are a productivity expert providing concise, actionable advice.\"},\n    {\"role\": \"user\", \"content\": \"What are three ways to improve productivity?\"}\n]\n</code></pre> <p>\u65b9\u6cd5 2\uff1a\u76f4\u63a5\u4f7f\u7528\u6d88\u606f</p> <p>\u5bf9\u4e8e\u66f4\u590d\u6742\u7684\u5bf9\u8bdd\u6216\u5f53\u60a8\u9700\u8981\u7cbe\u786e\u63a7\u5236\u6d88\u606f\u683c\u5f0f\u65f6\uff0c\u60a8\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>messages</code> \u53c2\u6570\uff1a</p> <pre><code># Direct use of messages for multi-turn conversation\nresponse = llm.generate(\n    messages=[\n        {\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n        {\"role\": \"user\", \"content\": \"Hello, who are you?\"},\n        {\"role\": \"assistant\", \"content\": \"I'm an AI assistant designed to help with various tasks.\"},\n        {\"role\": \"user\", \"content\": \"Can you help me with programming?\"}\n    ]\n)\n</code></pre>"},{"location":"zh/modules/llm.html#\u6279\u91cf\u751f\u6210","title":"\u6279\u91cf\u751f\u6210","text":"<p>\u5bf9\u4e8e\u6279\u91cf\u5904\u7406\uff0c\u60a8\u53ef\u4ee5\u63d0\u4f9b\u63d0\u793a/\u7cfb\u7edf\u6d88\u606f\u5217\u8868\u6216\u6d88\u606f\u5217\u8868\u3002\u4f8b\u5982\uff1a</p> <pre><code># Batch processing example\nresponses = llm.generate(\n    prompt=[\"What is machine learning?\", \"Explain neural networks.\"],\n    system_message=[\"You are a data scientist.\", \"You are an AI researcher.\"]\n)\n</code></pre>"},{"location":"zh/modules/llm.html#\u8f93\u51fa\u89e3\u6790","title":"\u8f93\u51fa\u89e3\u6790","text":"<p><code>generate</code> \u51fd\u6570\u63d0\u4f9b\u4e86\u7075\u6d3b\u7684\u9009\u9879\u6765\u89e3\u6790\u548c\u7ed3\u6784\u5316\u6765\u81ea\u8bed\u8a00\u6a21\u578b\u7684\u539f\u59cb\u6587\u672c\u8f93\u51fa\uff1a</p> <ul> <li>parser\uff1a\u63a5\u53d7\u4e00\u4e2a\u7c7b\uff08\u901a\u5e38\u7ee7\u627f\u81ea <code>LLMOutputParser/ActionOutput</code>\uff09\uff0c\u8be5\u7c7b\u5b9a\u4e49\u4e86\u89e3\u6790\u8f93\u51fa\u7684\u7ed3\u6784\u3002\u5982\u679c\u672a\u63d0\u4f9b\uff0cLLM \u8f93\u51fa\u5c06\u4e0d\u4f1a\u88ab\u89e3\u6790\u3002\u5728\u8fd9\u4e24\u79cd\u60c5\u51b5\u4e0b\uff0c\u90fd\u53ef\u4ee5\u901a\u8fc7\u8fd4\u56de\u5bf9\u8c61\u7684 <code>.content</code> \u5c5e\u6027\u8bbf\u95ee\u539f\u59cb LLM \u8f93\u51fa\u3002</li> <li>parse_mode\uff1a\u786e\u5b9a\u5982\u4f55\u5c06\u539f\u59cb LLM \u8f93\u51fa\u89e3\u6790\u4e3a\u89e3\u6790\u5668\u5b9a\u4e49\u7684\u7ed3\u6784\uff0c\u6709\u6548\u9009\u9879\u4e3a\uff1a<code>'str'</code>\u3001<code>'json'</code>\uff08\u9ed8\u8ba4\uff09\u3001<code>'xml'</code>\u3001<code>'title'</code>\u3001<code>'custom'</code>\u3002</li> <li>parse_func\uff1a\u7528\u4e8e\u5728\u66f4\u590d\u6742\u7684\u573a\u666f\u4e2d\u5904\u7406\u89e3\u6790\u7684\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u4ec5\u5728 <code>parse_mode</code> \u4e3a <code>'custom'</code> \u65f6\u4f7f\u7528\u3002</li> </ul> <p>\u7ed3\u6784\u5316\u8f93\u51fa\u793a\u4f8b\uff1a <pre><code>from evoagentx.models import LLMOutputParser \nfrom pydantic import Field\n\nclass CodeWriterOutput(LLMOutputParser):\n    thought: str = Field(description=\"Thought process for writing the code\") \n    code: str = Field(description=\"The generated code\")\n\nprompt = \"\"\"\nWrite a Python function to calculate Fibonacci numbers. \n\nYour output should always be in the following format:\n\n## thought \n[Your thought process for writing the code]\n\n## code\n[The generated code]\n\"\"\"\nresponse = llm.generate(\n    prompt=prompt,\n    parser=CodeWriterOutput,\n    parse_mode=\"title\"\n)\n\nprint(\"Thought:\\n\", response.thought)\nprint(\"Code:\\n\", response.code)\n</code></pre></p>"},{"location":"zh/modules/llm.html#\u89e3\u6790\u6a21\u5f0f","title":"\u89e3\u6790\u6a21\u5f0f","text":"<p>EvoAgentX \u652f\u6301\u51e0\u79cd\u89e3\u6790\u7b56\u7565\uff1a</p> <ol> <li>\"str\"\uff1a\u76f4\u63a5\u4f7f\u7528\u539f\u59cb\u8f93\u51fa\u4f5c\u4e3a\u89e3\u6790\u5668\u4e2d\u5b9a\u4e49\u7684\u6bcf\u4e2a\u5b57\u6bb5\u3002</li> <li>\"json\"\uff08\u9ed8\u8ba4\uff09\uff1a\u4ece\u8f93\u51fa\u4e2d\u7684 JSON \u5b57\u7b26\u4e32\u63d0\u53d6\u5b57\u6bb5\u3002</li> <li>\"xml\"\uff1a\u4ece\u4e0e\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684 XML \u6807\u7b7e\u4e2d\u63d0\u53d6\u5185\u5bb9\u3002</li> <li>\"title\"\uff1a\u4ece markdown \u7ae0\u8282\u4e2d\u63d0\u53d6\u5185\u5bb9\uff08\u9ed8\u8ba4\u683c\u5f0f\uff1a\"## {title}\"\uff09\u3002</li> <li>\"custom\"\uff1a\u4f7f\u7528\u7531 <code>parse_func</code> \u6307\u5b9a\u7684\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\u3002</li> </ol> <p>Note</p> <p>\u5bf9\u4e8e <code>'json'</code>\u3001<code>'xml'</code> \u548c <code>'title'</code>\uff0c\u60a8\u5e94\u8be5\u901a\u8fc7 <code>prompt</code> \u6307\u793a LLM \u4ee5\u53ef\u4ee5\u88ab\u89e3\u6790\u5668\u89e3\u6790\u7684\u6307\u5b9a\u683c\u5f0f\u8f93\u51fa\u5185\u5bb9\u3002\u5426\u5219\uff0c\u89e3\u6790\u5c06\u5931\u8d25\u3002</p> <ol> <li> <p>\u5bf9\u4e8e <code>'json'</code>\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\u4e00\u4e2a\u5305\u542b\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684\u952e\u7684\u6709\u6548 JSON \u5b57\u7b26\u4e32\u3002\u5982\u679c\u539f\u59cb LLM \u8f93\u51fa\u4e2d\u6709\u591a\u4e2a JSON \u5b57\u7b26\u4e32\uff0c\u53ea\u4f1a\u89e3\u6790\u7b2c\u4e00\u4e2a\u3002</p> </li> <li> <p>\u5bf9\u4e8e <code>xml</code>\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\u5305\u542b\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u7684 XML \u6807\u7b7e\u7684\u5185\u5bb9\uff0c\u4f8b\u5982 <code>&lt;{field_name}&gt;...&lt;/{field_name}&gt;</code>\u3002\u5982\u679c\u6709\u591a\u4e2a\u5177\u6709\u76f8\u540c\u5b57\u6bb5\u540d\u79f0\u7684 XML \u6807\u7b7e\uff0c\u53ea\u4f1a\u4f7f\u7528\u7b2c\u4e00\u4e2a\u3002</p> </li> <li> <p>\u5bf9\u4e8e <code>title</code>\uff0c\u60a8\u5e94\u8be5\u6307\u793a LLM \u8f93\u51fa\u5305\u542b\u6807\u9898\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5b8c\u5168\u5339\u914d\u7684 markdown \u7ae0\u8282\u7684\u5185\u5bb9\u3002\u9ed8\u8ba4\u6807\u9898\u683c\u5f0f\u662f \"## {title}\"\u3002\u60a8\u53ef\u4ee5\u901a\u8fc7\u5728 <code>generate</code> \u51fd\u6570\u4e2d\u8bbe\u7f6e <code>title_format</code> \u53c2\u6570\u6765\u66f4\u6539\u5b83\uff0c\u4f8b\u5982 <code>generate(..., title_format=\"### {title}\")</code>\u3002<code>title_format</code> \u5fc5\u987b\u5305\u542b <code>{title}</code> \u4f5c\u4e3a\u5b57\u6bb5\u540d\u79f0\u7684\u5360\u4f4d\u7b26\u3002</p> </li> </ol>"},{"location":"zh/modules/llm.html#\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570","title":"\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570","text":"<p>\u4e3a\u4e86\u83b7\u5f97\u6700\u5927\u7684\u7075\u6d3b\u6027\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>parse_func</code> \u5b9a\u4e49\u81ea\u5b9a\u4e49\u89e3\u6790\u51fd\u6570\uff1a</p> <pre><code>from evoagentx.models import LLMOutputParser\nfrom evoagentx.core.module_utils import extract_code_block\n\nclass CodeOutput(LLMOutputParser):\n    code: str = Field(description=\"The generated code\")\n\n# Use custom parsing\nresponse = llm.generate(\n    prompt=\"Write a Python function to calculate Fibonacci numbers.\",\n    parser=CodeOutput,\n    parse_mode=\"custom\",\n    parse_func=lambda content: {\"code\": extract_code_block(content)[0]}\n)\n</code></pre> <p>Note</p> <p>\u89e3\u6790\u51fd\u6570\u5e94\u8be5\u6709\u4e00\u4e2a\u63a5\u6536\u539f\u59cb LLM \u8f93\u51fa\u7684\u8f93\u5165\u53c2\u6570 <code>content</code>\uff0c\u5e76\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\uff0c\u5176\u952e\u4e0e\u89e3\u6790\u5668\u7c7b\u4e2d\u7684\u5b57\u6bb5\u540d\u79f0\u5339\u914d\u3002</p>"},{"location":"zh/modules/llm.html#\u5f02\u6b65\u751f\u6210\u51fd\u6570","title":"\u5f02\u6b65\u751f\u6210\u51fd\u6570","text":"<p>\u5bf9\u4e8e\u9700\u8981\u5f02\u6b65\u64cd\u4f5c\u7684\u5e94\u7528\u7a0b\u5e8f\uff0c<code>async_generate</code> \u51fd\u6570\u63d0\u4f9b\u4e86\u4e0e <code>generate</code> \u51fd\u6570\u76f8\u540c\u7684\u529f\u80fd\uff0c\u4f46\u4ee5\u975e\u963b\u585e\u65b9\u5f0f\u8fd0\u884c\uff1a</p> <pre><code>async def async_generate(\n        self,\n        prompt: Optional[Union[str, List[str]]] = None,\n        system_message: Optional[Union[str, List[str]]] = None,\n        messages: Optional[Union[List[dict],List[List[dict]]]] = None,\n        parser: Optional[Type[LLMOutputParser]] = None,\n        parse_mode: Optional[str] = \"json\", \n        parse_func: Optional[Callable] = None,\n        **kwargs\n    ) -&gt; Union[LLMOutputParser, List[LLMOutputParser]]:\n    \"\"\"\n    \u57fa\u4e8e\u63d0\u793a\u548c\u53ef\u9009\u7684\u7cfb\u7edf\u6d88\u606f\u5f02\u6b65\u751f\u6210\u6587\u672c\u3002\n\n    \u53c2\u6570\uff1a\n        prompt: \u8f93\u5165\u5230 LLM \u7684\u63d0\u793a\u3002\n        system_message: LLM \u7684\u7cfb\u7edf\u6d88\u606f\u3002\n        messages: LLM \u7684\u804a\u5929\u6d88\u606f\uff0c\u5df2\u7ecf\u662f\u6240\u9700\u683c\u5f0f\uff08\u5fc5\u987b\u63d0\u4f9b `prompt` \u6216 `messages` \u4e4b\u4e00\uff09\u3002\n        parser: \u7528\u4e8e\u5c06\u8f93\u51fa\u5904\u7406\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\u7684\u89e3\u6790\u5668\u7c7b\u3002\n        parse_mode: \u7528\u4e8e\u89e3\u6790\u7684\u6a21\u5f0f\uff0c\u5fc5\u987b\u662f `parser` \u652f\u6301\u7684 `parse_mode`\u3002\n        parse_func: \u5e94\u7528\u4e8e\u89e3\u6790\u8f93\u51fa\u7684\u51fd\u6570\u3002\n        **kwargs: \u989d\u5916\u7684\u751f\u6210\u914d\u7f6e\u53c2\u6570\u3002\n\n    \u8fd4\u56de\uff1a\n        \u5355\u6b21\u751f\u6210\uff1a\u4e00\u4e2a LLMOutputParser \u5b9e\u4f8b\u3002\n        \u6279\u91cf\u751f\u6210\uff1aLLMOutputParser \u5b9e\u4f8b\u5217\u8868\u3002\n    \"\"\"\n</code></pre>"},{"location":"zh/modules/llm.html#\u6d41\u5f0f\u54cd\u5e94","title":"\u6d41\u5f0f\u54cd\u5e94","text":"<p>EvoAgentX \u652f\u6301\u6765\u81ea LLM \u7684\u6d41\u5f0f\u54cd\u5e94\uff0c\u8fd9\u4f7f\u60a8\u53ef\u4ee5\u5728\u751f\u6210\u8fc7\u7a0b\u4e2d\u9010\u4ee4\u724c\u67e5\u770b\u6a21\u578b\u7684\u8f93\u51fa\uff0c\u800c\u4e0d\u662f\u7b49\u5f85\u5b8c\u6574\u54cd\u5e94\u3002\u8fd9\u5bf9\u4e8e\u751f\u6210\u957f\u7bc7\u5185\u5bb9\u6216\u63d0\u4f9b\u66f4\u4ea4\u4e92\u5f0f\u7684\u4f53\u9a8c\u7279\u522b\u6709\u7528\u3002</p> <p>\u6709\u4e24\u79cd\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\u7684\u65b9\u5f0f\uff1a</p>"},{"location":"zh/modules/llm.html#\u5728-llm-\u914d\u7f6e\u4e2d\u914d\u7f6e\u6d41\u5f0f\u4f20\u8f93","title":"\u5728 LLM \u914d\u7f6e\u4e2d\u914d\u7f6e\u6d41\u5f0f\u4f20\u8f93","text":"<p>\u60a8\u53ef\u4ee5\u5728\u521d\u59cb\u5316 LLM \u65f6\u901a\u8fc7\u8bbe\u7f6e\u914d\u7f6e\u4e2d\u7684\u9002\u5f53\u53c2\u6570\u6765\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\uff1a</p> <pre><code># \u5728\u521d\u59cb\u5316\u65f6\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\",\n    stream=True,  # \u542f\u7528\u6d41\u5f0f\u4f20\u8f93\n    output_response=True  # \u5b9e\u65f6\u5c06\u4ee4\u724c\u6253\u5370\u5230\u63a7\u5236\u53f0\n)\n\nllm = OpenAILLM(config=config)\n\n# \u73b0\u5728\u6240\u6709\u5bf9 generate() \u7684\u8c03\u7528\u90fd\u5c06\u9ed8\u8ba4\u4f7f\u7528\u6d41\u5f0f\u4f20\u8f93\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\"\n)\n</code></pre>"},{"location":"zh/modules/llm.html#\u5728\u751f\u6210\u65b9\u6cd5\u4e2d\u542f\u7528\u6d41\u5f0f\u4f20\u8f93","title":"\u5728\u751f\u6210\u65b9\u6cd5\u4e2d\u542f\u7528\u6d41\u5f0f\u4f20\u8f93","text":"<p>\u6216\u8005\uff0c\u60a8\u53ef\u4ee5\u4e3a\u7279\u5b9a\u7684\u751f\u6210\u8c03\u7528\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\uff1a</p> <pre><code># \u4f7f\u7528\u9ed8\u8ba4\u975e\u6d41\u5f0f\u884c\u4e3a\u521d\u59cb\u5316\u7684 LLM\nconfig = OpenAILLMConfig(\n    model=\"gpt-4o-mini\",\n    openai_key=\"your-api-key\"\n)\n\nllm = OpenAILLM(config=config)\n\n# \u4ec5\u4e3a\u6b64\u7279\u5b9a\u8c03\u7528\u8986\u76d6\u8bbe\u7f6e\nresponse = llm.generate(\n    prompt=\"Write a story about space exploration.\",\n    stream=True,  # \u4ec5\u4e3a\u6b64\u8c03\u7528\u542f\u7528\u6d41\u5f0f\u4f20\u8f93\n    output_response=True  # \u5b9e\u65f6\u5c06\u4ee4\u724c\u6253\u5370\u5230\u63a7\u5236\u53f0\n)\n</code></pre>"},{"location":"zh/modules/prompt_template.html","title":"\u63d0\u793a\u6a21\u677f","text":""},{"location":"zh/modules/prompt_template.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>PromptTemplate</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u4e14\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u8bed\u8a00\u6a21\u578b\u7684\u63d0\u793a\u3002\u5b83\u652f\u6301\u5404\u79cd\u7ec4\u4ef6\uff0c\u5982\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u7ea6\u675f\u3001\u5de5\u5177\u548c\u793a\u4f8b\uff0c\u4f7f\u521b\u5efa\u4e00\u81f4\u4e14\u683c\u5f0f\u826f\u597d\u7684\u63d0\u793a\u53d8\u5f97\u66f4\u52a0\u5bb9\u6613\u3002</p>"},{"location":"zh/modules/prompt_template.html#\u4e3b\u8981\u7279\u6027","title":"\u4e3b\u8981\u7279\u6027","text":"<ul> <li>\u7ed3\u6784\u5316\u63d0\u793a\u7ec4\u4ef6\uff1a\u4f7f\u7528\u6e05\u6670\u7684\u90e8\u5206\u5b9a\u4e49\u63d0\u793a\uff0c\u5305\u62ec\u6307\u4ee4\u3001\u4e0a\u4e0b\u6587\u3001\u7ea6\u675f\u7b49</li> <li>\u7075\u6d3b\u7684\u8f93\u51fa\u683c\u5f0f\uff1a\u652f\u6301\u591a\u79cd\u8f93\u51fa\u89e3\u6790\u6a21\u5f0f\uff08JSON\u3001XML\u3001\u6807\u9898\u683c\u5f0f\uff09</li> <li>\u5c11\u6837\u672c\u5b66\u4e60\u652f\u6301\uff1a\u6613\u4e8e\u96c6\u6210\u793a\u4f8b\u8fdb\u884c\u5c11\u6837\u672c\u5b66\u4e60</li> <li>\u8f93\u5165/\u8f93\u51fa\u9a8c\u8bc1\uff1a\u81ea\u52a8\u9a8c\u8bc1\u6240\u9700\u7684\u8f93\u5165\u548c\u8f93\u51fa</li> <li>\u804a\u5929\u683c\u5f0f\u652f\u6301\uff1a\u901a\u8fc7 <code>ChatTemplate</code> \u7279\u522b\u652f\u6301\u57fa\u4e8e\u804a\u5929\u7684\u4ea4\u4e92</li> </ul>"},{"location":"zh/modules/prompt_template.html#\u57fa\u672c\u7528\u6cd5","title":"\u57fa\u672c\u7528\u6cd5","text":""},{"location":"zh/modules/prompt_template.html#\u7b80\u5355\u6a21\u677f","title":"\u7b80\u5355\u6a21\u677f","text":"<p>\u521b\u5efa <code>PromptTemplate</code> \u7684\u6700\u7b80\u5355\u65b9\u6cd5\u662f\u53ea\u4f7f\u7528\u6307\u4ee4\uff1a</p> <pre><code>from evoagentx.prompts import StringTemplate\n\ntemplate = StringTemplate(\n    instruction=\"Write a function that calculates the factorial of a number\"\n)\n\n# \u5c06\u6a21\u677f\u683c\u5f0f\u5316\u4e3a\u63d0\u793a\u5b57\u7b26\u4e32\nprompt = template.format()\n</code></pre>"},{"location":"zh/modules/prompt_template.html#\u5e26\u6709\u4e0a\u4e0b\u6587\u548c\u7ea6\u675f\u7684\u6a21\u677f","title":"\u5e26\u6709\u4e0a\u4e0b\u6587\u548c\u7ea6\u675f\u7684\u6a21\u677f","text":"<p>\u60a8\u53ef\u4ee5\u6dfb\u52a0\u4e0a\u4e0b\u6587\u548c\u7ea6\u675f\u6765\u63d0\u4f9b\u66f4\u591a\u6307\u5bfc\uff1a</p> <pre><code>template = StringTemplate(\n    instruction=\"Write a function that calculates the factorial of a number\",\n    context=\"The factorial of a number n is the product of all positive integers less than or equal to n\",\n    constraints=[\n        \"Use recursion to implement\",\n        \"Include input validation\",\n        \"Add a docstring with examples\"\n    ]\n)\n\nprompt = template.format()\n</code></pre>"},{"location":"zh/modules/prompt_template.html#\u5e26\u6709\u793a\u4f8b\u7684\u6a21\u677f","title":"\u5e26\u6709\u793a\u4f8b\u7684\u6a21\u677f","text":"<p>\u60a8\u53ef\u4ee5\u6dfb\u52a0\u793a\u4f8b\u8fdb\u884c\u5c11\u6837\u672c\u5b66\u4e60\uff1a</p> <pre><code>from evoagentx.models import OpenAILLMConfig\nfrom evoagentx.actions import ActionInput, ActionOutput\n\ntemplate = StringTemplate(\n    instruction=\"Write a function that implements the provided requirement\",\n    demonstrations=[\n        {\n            \"requirement\": \"Write a function that returns the sum of two numbers\",\n            \"code\": \"def add_numbers(a: int, b: int) -&gt; int:\\n    return a + b\"\n        },\n        {\n            \"requirement\": \"Write a function that checks if a number is even\",\n            \"code\": \"def is_even(n: int) -&gt; bool:\\n    return n % 2 == 0\"\n        }\n    ]\n)\n\nclass InputFormat(ActionInput):\n    requirement: str\n\nclass OutputFormat(ActionOutput):\n    code: str\n\nprompt = template.format(\n    values={\"requirement\": \"Write a function that returns the factorial of a number\"}, \n    inputs_format=InputFormat,\n    outputs_format=OutputFormat,\n)\n</code></pre> <p>Note</p> <p>\u4f7f\u7528 <code>demonstrations</code> \u65f6\u9700\u8981 <code>inputs_format</code> \u548c <code>outputs_format</code> \u6765\u6b63\u786e\u6620\u5c04\u793a\u4f8b\u4e2d\u7684\u8f93\u5165\u548c\u8f93\u51fa\u3002</p>"},{"location":"zh/modules/prompt_template.html#\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f","title":"\u7ed3\u6784\u5316\u8f93\u51fa\u683c\u5f0f","text":"<p>\u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u6a21\u677f\u4f1a\u6839\u636e <code>outputs_format</code> \u548c <code>parse_mode</code> \u81ea\u52a8\u751f\u6210\u8f93\u51fa\u683c\u5f0f\u3002</p>"},{"location":"zh/modules/prompt_template.html#\u6807\u9898\u683c\u5f0f\u9ed8\u8ba4","title":"\u6807\u9898\u683c\u5f0f\uff08\u9ed8\u8ba4\uff09","text":"<p><pre><code>template = StringTemplate(\n    instruction=\"Analyze the given text\",\n    inputs_format=TextAnalysisInput, # A Pydantic model with text field\n    outputs_format=TextAnalysisOutput,  # A Pydantic model with summary and sentiment fields\n    parse_mode=\"title\"\n)\n</code></pre> \u4e0a\u8ff0 <code>template</code> \u5c06\u751f\u6210\u5982\u4e0b\u683c\u5f0f\u7684\u8f93\u51fa\uff1a <pre><code>## summary\n[\u6458\u8981\u5185\u5bb9]\n\n## sentiment\n[\u60c5\u611f\u5206\u6790]\n</code></pre></p>"},{"location":"zh/modules/prompt_template.html#json-\u683c\u5f0f","title":"JSON \u683c\u5f0f","text":"<p><pre><code>template = StringTemplate(\n    instruction=\"Analyze the given text\",\n    inputs_format=TextAnalysisInput,\n    outputs_format=TextAnalysisOutput, \n    parse_mode=\"json\"\n)\n</code></pre> \u4e0a\u8ff0 <code>template</code> \u5c06\u751f\u6210\u5982\u4e0b\u683c\u5f0f\u7684\u8f93\u51fa\uff1a <pre><code>{\n    \"summary\": \"[\u6458\u8981\u5185\u5bb9]\",\n    \"sentiment\": \"[\u60c5\u611f\u5206\u6790]\"\n}\n</code></pre></p>"},{"location":"zh/modules/prompt_template.html#xml-\u683c\u5f0f","title":"XML \u683c\u5f0f","text":"<p><pre><code>template = StringTemplate(\n    instruction=\"Analyze the given text\",\n    inputs_format=TextAnalysisInput,\n    outputs_format=TextAnalysisOutput,\n    parse_mode=\"xml\"\n)\n</code></pre> \u4e0a\u8ff0 <code>template</code> \u5c06\u751f\u6210\u5982\u4e0b\u683c\u5f0f\u7684\u8f93\u51fa\uff1a <pre><code>&lt;summary&gt;\n[\u6458\u8981\u5185\u5bb9]\n&lt;/summary&gt;\n&lt;sentiment&gt;\n[\u60c5\u611f\u5206\u6790]\n&lt;/sentiment&gt;\n</code></pre></p> <p>Note</p> <ol> <li> <p>\u5bf9\u4e8e <code>parse_mode=\"str\" \u6216 \"custom\"</code>\uff0c\u6a21\u578b\u5c06\u9075\u5faa\u6307\u4ee4\u751f\u6210\u54cd\u5e94\u3002</p> </li> <li> <p>\u60a8\u53ef\u4ee5\u901a\u8fc7\u8bbe\u7f6e <code>template.format(custom_output_format=...)</code> \u6765\u8986\u76d6\u8f93\u51fa\u683c\u5f0f\uff0c\u8bf7\u53c2\u89c1\u81ea\u5b9a\u4e49\u8f93\u51fa\u683c\u5f0f\u3002</p> </li> </ol>"},{"location":"zh/modules/prompt_template.html#\u804a\u5929\u6a21\u677f","title":"\u804a\u5929\u6a21\u677f","text":"<p>\u5bf9\u4e8e\u57fa\u4e8e\u804a\u5929\u7684\u4ea4\u4e92\uff0c\u60a8\u53ef\u4ee5\u4f7f\u7528 <code>ChatTemplate</code> \u7c7b\uff1a</p> <pre><code>from evoagentx.prompts import ChatTemplate\n\ntemplate = ChatTemplate(\n    instruction=\"You are a helpful programming assistant\",\n    context=\"You help users write Python code\",\n    constraints=[\"Always include comments\", \"Follow PEP 8 style guide\"]\n)\n\n# format \u5c06\u8fd4\u56de\u804a\u5929\u6d88\u606f\u5217\u8868\nmessages = template.format(\n    inputs_format=CodeInputs,\n    outputs_format=CodeOutputs,\n    values={\"requirement\": \"Write a sorting function\"}\n)\n</code></pre> <p>\u683c\u5f0f\u5316\u7684\u8f93\u51fa\u5c06\u662f\u4e00\u4e2a\u9002\u5408\u57fa\u4e8e\u804a\u5929\u7684\u6a21\u578b\u7684\u6d88\u606f\u5217\u8868\uff1a</p> <pre><code>[\n    {\"role\": \"system\", \"content\": \"...\"},\n    {\"role\": \"user\", \"content\": \"...\"},\n    {\"role\": \"assistant\", \"content\": \"...\"}\n]\n</code></pre>"},{"location":"zh/modules/prompt_template.html#\u6700\u4f73\u5b9e\u8df5","title":"\u6700\u4f73\u5b9e\u8df5","text":"<ol> <li>\u6e05\u6670\u7684\u6307\u4ee4\uff1a\u4f7f\u60a8\u7684\u6307\u4ee4\u5177\u4f53\u4e14\u660e\u786e</li> <li>\u76f8\u5173\u7684\u4e0a\u4e0b\u6587\uff1a\u4ec5\u5305\u542b\u4e0e\u4efb\u52a1\u76f4\u63a5\u76f8\u5173\u7684\u4e0a\u4e0b\u6587</li> <li>\u5177\u4f53\u7684\u7ea6\u675f\uff1a\u5217\u51fa\u6709\u610f\u4e49\u5730\u6307\u5bfc\u8f93\u51fa\u7684\u7ea6\u675f</li> <li>\u4ee3\u8868\u6027\u7684\u793a\u4f8b\uff1a\u9009\u62e9\u6db5\u76d6\u4e0d\u540c\u60c5\u51b5\u7684\u793a\u4f8b</li> <li>\u9002\u5f53\u7684\u8f93\u51fa\u683c\u5f0f\uff1a\u9009\u62e9\u6700\u9002\u5408\u60a8\u9700\u6c42\u7684\u89e3\u6790\u6a21\u5f0f</li> </ol>"},{"location":"zh/modules/prompt_template.html#\u9ad8\u7ea7\u529f\u80fd","title":"\u9ad8\u7ea7\u529f\u80fd","text":""},{"location":"zh/modules/prompt_template.html#\u81ea\u5b9a\u4e49\u8f93\u51fa\u683c\u5f0f","title":"\u81ea\u5b9a\u4e49\u8f93\u51fa\u683c\u5f0f","text":"<p>\u60a8\u53ef\u4ee5\u6307\u5b9a\u81ea\u5b9a\u4e49\u8f93\u51fa\u683c\u5f0f\uff1a</p> <pre><code>template = StringTemplate(\n    instruction=\"Generate code documentation\"\n)\n\nprompt = template.format(\n    values={\"code\": \"...\"},\n    custom_output_format=\"\"\"\n    Please provide your response in the following format:\n\n    # Usage\n    [Code usage examples]\n\n    # API\n    [API documentation]\n\n    # Notes\n    [Additional notes and warnings]\n    \"\"\"\n)\n</code></pre>"},{"location":"zh/modules/rag.html","title":"RAGEngine \u6a21\u5757\u6587\u6863","text":""},{"location":"zh/modules/rag.html#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p><code>RAGEngine</code> \u6a21\u5757\u662f\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u7684\u6838\u5fc3\u7ec4\u4ef6\uff0c\u8bbe\u8ba1\u7528\u4e8e\u7ba1\u7406\u6587\u6863\u7684\u7d22\u5f15\u3001\u5b58\u50a8\u548c\u68c0\u7d22\uff0c\u4ee5\u5b9e\u73b0\u9ad8\u6548\u7684\u4fe1\u606f\u8bbf\u95ee\u3002\u57fa\u4e8e LlamaIndex \u6784\u5efa\uff0c\u5b83\u4e0e\u591a\u79cd\u5b58\u50a8\u540e\u7aef\uff08\u5982 SQLite\u3001FAISS\u3001Neo4j\uff09\u96c6\u6210\uff0c\u5e76\u652f\u6301\u591a\u79cd\u7d22\u5f15\u7c7b\u578b\uff08\u5982\u5411\u91cf\u7d22\u5f15\u3001\u56fe\u7d22\u5f15\uff09\u3002\u8be5\u6a21\u5757\u662f\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406\u6846\u67b6\u7684\u4e00\u90e8\u5206\uff0c\u9002\u7528\u4e8e\u9700\u8981\u9ad8\u6548\u5904\u7406\u548c\u67e5\u8be2\u5927\u578b\u6570\u636e\u96c6\u7684\u4ee3\u7406\u7cfb\u7edf\u3002</p>"},{"location":"zh/modules/rag.html#\u7528\u9014","title":"\u7528\u9014","text":"<p><code>RAGEngine</code> \u63d0\u4f9b\u4ee5\u4e0b\u6838\u5fc3\u529f\u80fd\uff1a - \u6587\u6863\u5904\u7406\uff1a\u4ece\u591a\u79cd\u683c\u5f0f\uff08\u5982 PDF\u3001\u6587\u672c\uff09\u52a0\u8f7d\u5e76\u5206\u5757\u6587\u6863\u3002 - \u7d22\u5f15\u7ba1\u7406\uff1a\u521b\u5efa\u548c\u7ba1\u7406\u7d22\u5f15\u4ee5\u652f\u6301\u9ad8\u6548\u68c0\u7d22\u3002 - \u68c0\u7d22\u529f\u80fd\uff1a\u652f\u6301\u57fa\u4e8e\u5143\u6570\u636e\u8fc7\u6ee4\u548c\u76f8\u4f3c\u6027\u641c\u7d22\u7684\u67e5\u8be2\u3002 - \u5b58\u50a8\u7ba1\u7406\uff1a\u5c06\u7d22\u5f15\u6301\u4e45\u5316\u5230\u6587\u4ef6\u6216\u6570\u636e\u5e93\u4ee5\u5b9e\u73b0\u53ef\u6269\u5c55\u6027\u3002</p> <p>\u5b83\u8bbe\u8ba1\u4e3a\u4e0e <code>StorageHandler</code> \u548c <code>MemoryManager</code> \u7ec4\u4ef6\u96c6\u6210\uff0c\u9002\u7528\u4e8e\u9700\u8981\u4e0a\u4e0b\u6587\u77e5\u8bc6\u68c0\u7d22\u7684\u4ee3\u7406\u7cfb\u7edf\u3002</p>"},{"location":"zh/modules/rag.html#\u4e3b\u8981\u7279\u6027","title":"\u4e3b\u8981\u7279\u6027","text":"<ul> <li>\u7075\u6d3b\u7684\u6587\u6863\u52a0\u8f7d\uff1a\u652f\u6301\u4ece\u76ee\u5f55\u52a0\u8f7d\u6587\u4ef6\uff0c\u63d0\u4f9b\u53ef\u5b9a\u5236\u7684\u8fc7\u6ee4\u9009\u9879\uff08\u5982\u6587\u4ef6\u540e\u7f00\u3001\u6392\u9664\u5217\u8868\uff09\u3002</li> <li>\u5206\u5757\u4e0e\u5d4c\u5165\uff1a\u81ea\u52a8\u5c06\u6587\u6863\u5206\u5757\uff0c\u5e76\u4f7f\u7528\u53ef\u914d\u7f6e\u7684\u6a21\u578b\uff08\u5982 OpenAI \u7684 <code>text-embedding-ada-002</code>\uff09\u751f\u6210\u5d4c\u5165\u3002</li> <li>\u591a\u7d22\u5f15\u652f\u6301\uff1a\u652f\u6301\u591a\u79cd\u7d22\u5f15\u7c7b\u578b\uff08\u5411\u91cf\u3001\u56fe\uff09\u4ee5\u6ee1\u8db3\u4e0d\u540c\u68c0\u7d22\u9700\u6c42\u3002</li> <li>\u9ad8\u7ea7\u68c0\u7d22\uff1a\u652f\u6301\u5143\u6570\u636e\u8fc7\u6ee4\u3001\u76f8\u4f3c\u6027\u9608\u503c\u548c\u57fa\u4e8e\u5173\u952e\u5b57\u7684\u67e5\u8be2\uff0c\u91c7\u7528\u5f02\u6b65\u548c\u591a\u7ebf\u7a0b\u68c0\u7d22\u3002</li> <li>\u5b58\u50a8\u96c6\u6210\uff1a\u4e0e SQLite\uff08\u7528\u4e8e\u5143\u6570\u636e\uff09\u3001FAISS\uff08\u7528\u4e8e\u5411\u91cf\u5d4c\u5165\uff09\u548c Neo4j\uff08\u7528\u4e8e\u5b9e\u4f53\u5173\u7cfb\uff09\u65e0\u7f1d\u96c6\u6210\u3002</li> <li>\u6301\u4e45\u5316\uff1a\u652f\u6301\u5c06\u7d22\u5f15\u4fdd\u5b58\u5230\u6587\u4ef6\u6216\u6570\u636e\u5e93\uff0c\u786e\u4fdd\u6570\u636e\u6301\u4e45\u6027\u3002</li> <li>\u9519\u8bef\u5904\u7406\uff1a\u63d0\u4f9b\u5065\u58ee\u7684\u65e5\u5fd7\u8bb0\u5f55\u548c\u5f02\u5e38\u5904\u7406\uff0c\u786e\u4fdd\u64cd\u4f5c\u53ef\u9760\u6027\u3002</li> </ul>"},{"location":"zh/modules/rag.html#rag-\u6d41\u7a0b\u6982\u8ff0","title":"RAG \u6d41\u7a0b\u6982\u8ff0","text":"<p><code>RAGEngine</code> \u7684 RAG \u6d41\u7a0b\u5305\u62ec\u56db\u4e2a\u4e3b\u8981\u9636\u6bb5\uff0c\u786e\u4fdd\u4fe1\u606f\u7684\u9ad8\u6548\u5904\u7406\u548c\u68c0\u7d22\uff1a</p> <ol> <li>\u6587\u6863\u8bfb\u53d6\uff1a\u4f7f\u7528 <code>LLamaIndexReader</code> \u4ece\u6307\u5b9a\u6587\u4ef6\u8def\u5f84\u6216\u76ee\u5f55\u52a0\u8f7d\u6587\u6863\u3002\u652f\u6301\u5904\u7406\u591a\u79cd\u6587\u4ef6\u683c\u5f0f\uff08\u5982 PDF\u3001\u6587\u672c\u3001Markdown\uff09\uff0c\u5e76\u63d0\u4f9b\u53ef\u914d\u7f6e\u9009\u9879\uff0c\u5982\u9012\u5f52\u76ee\u5f55\u8bfb\u53d6\u3001\u6587\u4ef6\u540e\u7f00\u8fc7\u6ee4\u548c\u81ea\u5b9a\u4e49\u5143\u6570\u636e\u63d0\u53d6\u3002</li> <li>\u5206\u5757\uff1a\u4f7f\u7528\u6307\u5b9a\u7684\u5206\u5757\u7b56\u7565\uff08\u5982\u7b80\u5355\u5206\u5757\u3001\u8bed\u4e49\u5206\u5757\u3001\u5c42\u6b21\u5206\u5757\uff09\u5c06\u6587\u6863\u5206\u5272\u6210\u8f83\u5c0f\u7684\u7247\u6bb5\uff0c\u786e\u4fdd\u5d4c\u5165\u548c\u68c0\u7d22\u7684\u6587\u672c\u6bb5\u53ef\u7ba1\u7406\u3002</li> <li>\u5411\u91cf\u7d22\u5f15\u6784\u5efa\uff1a\u4f7f\u7528\u5d4c\u5165\u6a21\u578b\uff08\u5982 OpenAI\u3001ollama \u6216 Hugging Face\uff09\u4e3a\u5206\u5757\u751f\u6210\u5d4c\u5165\uff0c\u5e76\u6784\u5efa\u7d22\u5f15\uff08\u5982\u5411\u91cf\u7d22\u5f15\u6216\u56fe\u7d22\u5f15\uff09\u4ee5\u5b9e\u73b0\u9ad8\u6548\u5b58\u50a8\u548c\u68c0\u7d22\u3002\u7d22\u5f15\u5b58\u50a8\u5728 FAISS\u3001Neo4j \u6216 SQLite \u7b49\u540e\u7aef\u4e2d\u3002</li> <li>\u68c0\u7d22\uff1a\u4f7f\u7528\u68c0\u7d22\u5668\uff08\u5982\u5411\u91cf\u68c0\u7d22\u5668\u6216\u56fe\u68c0\u7d22\u5668\uff09\u5904\u7406\u67e5\u8be2\uff0c\u5e94\u7528\u9884\u5904\u7406\uff08\u5982 HyDE \u67e5\u8be2\u8f6c\u6362\uff09\uff0c\u68c0\u7d22\u76f8\u5173\u5206\u5757\uff0c\u5e76\u5bf9\u7ed3\u679c\u8fdb\u884c\u540e\u5904\u7406\uff08\u5982\u91cd\u65b0\u6392\u5e8f\u6216\u5143\u6570\u636e\u8fc7\u6ee4\uff09\u3002</li> </ol> <p> </p>"},{"location":"zh/modules/rag.html#\u914d\u7f6e\u8be6\u60c5","title":"\u914d\u7f6e\u8be6\u60c5","text":"<p><code>RAGEngine</code> \u4f9d\u8d56 <code>RAGConfig</code> \u5bf9\u8c61\uff0c\u8be5\u5bf9\u8c61\u5c01\u88c5\u4e86 RAG \u6d41\u7a0b\u5404\u9636\u6bb5\u7684\u914d\u7f6e\u3002\u4ee5\u4e0b\u662f\u8be6\u7ec6\u7684\u914d\u7f6e\u53ca\u5176\u53c2\u6570\u8bf4\u660e\u3002</p>"},{"location":"zh/modules/rag.html#readerconfig","title":"ReaderConfig","text":"<p>\u63a7\u5236\u6587\u6863\u8bfb\u53d6\u9636\u6bb5\u3002</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>recursive</code> <code>bool</code> <code>False</code> \u662f\u5426\u9012\u5f52\u8bfb\u53d6\u76ee\u5f55\u3002\u8bbe\u7f6e\u4e3a <code>True</code> \u4ee5\u5305\u542b\u5b50\u76ee\u5f55\u3002 <code>exclude_hidden</code> <code>bool</code> <code>True</code> \u6392\u9664\u9690\u85cf\u6587\u4ef6\u548c\u76ee\u5f55\uff08\u4f8b\u5982\uff0c\u4ee5\u70b9\u5f00\u5934\u7684\u6587\u4ef6\uff09\u3002 <code>num_files_limit</code> <code>Optional[int]</code> <code>None</code> \u8bfb\u53d6\u6587\u4ef6\u7684\u6700\u5927\u6570\u91cf\u3002\u8bbe\u7f6e\u4e3a <code>None</code> \u8868\u793a\u65e0\u9650\u5236\u3002 <code>custom_metadata_function</code> <code>Optional[Callable]</code> <code>None</code> \u4ece\u6587\u4ef6\u4e2d\u63d0\u53d6\u5143\u6570\u636e\u7684\u81ea\u5b9a\u4e49\u51fd\u6570\uff0c\u5141\u8bb8\u7528\u6237\u5b9a\u4e49\u5143\u6570\u636e\u5b57\u6bb5\u3002 <code>extern_file_extractor</code> <code>Optional[Dict[str, Any]]</code> <code>None</code> \u7279\u5b9a\u6587\u4ef6\u7c7b\u578b\uff08\u5982 PDF\u3001Word\uff09\u7684\u5916\u90e8\u6587\u4ef6\u63d0\u53d6\u5668\u3002 <code>errors</code> <code>str</code> <code>\"ignore\"</code> \u9519\u8bef\u5904\u7406\u7b56\u7565\uff1a<code>\"ignore\"</code> \u8df3\u8fc7\u65e0\u6548\u6587\u4ef6\uff0c<code>\"strict\"</code> \u629b\u51fa\u5f02\u5e38\u3002 <code>encoding</code> <code>str</code> <code>\"utf-8\"</code> \u8bfb\u53d6\u6587\u672c\u6587\u4ef6\u65f6\u4f7f\u7528\u7684\u7f16\u7801\u3002"},{"location":"zh/modules/rag.html#chunkerconfig","title":"ChunkerConfig","text":"<p>\u914d\u7f6e\u6587\u6863\u5206\u5757\u9636\u6bb5\u3002</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>strategy</code> <code>str</code> <code>\"simple\"</code> \u5206\u5757\u7b56\u7565\uff1a<code>\"simple\"</code>\u3001<code>\"semantic\"</code> \u6216 <code>\"hierarchical\"</code>\u3002\u51b3\u5b9a\u6587\u6863\u5982\u4f55\u5206\u5272\u3002 <code>chunk_size</code> <code>int</code> <code>1024</code> \u6bcf\u4e2a\u5206\u5757\u7684\u6700\u5927\u5b57\u7b26\u6570\u3002\u8f83\u5c0f\u7684\u5206\u5757\u63d0\u9ad8\u7c92\u5ea6\u4f46\u589e\u52a0\u5904\u7406\u65f6\u95f4\u3002 <code>chunk_overlap</code> <code>int</code> <code>20</code> \u5206\u5757\u4e4b\u95f4\u7684\u91cd\u53e0\u5b57\u7b26\u6570\uff0c\u4ee5\u4fdd\u6301\u4e0a\u4e0b\u6587\u3002 <code>max_chunks</code> <code>Optional[int]</code> <code>None</code> \u6bcf\u4e2a\u6587\u6863\u7684\u6700\u5927\u5206\u5757\u6570\u3002\u8bbe\u7f6e\u4e3a <code>None</code> \u8868\u793a\u65e0\u9650\u5236\u3002"},{"location":"zh/modules/rag.html#embeddingconfig","title":"EmbeddingConfig","text":"<p>\u7ba1\u7406\u5d4c\u5165\u751f\u6210\u9636\u6bb5\u3002</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>provider</code> <code>str</code> <code>\"openai\"</code> \u5d4c\u5165\u63d0\u4f9b\u8005\uff1a<code>\"openai\"</code>\u3001<code>\"ollama\"</code> \u6216 <code>\"huggingface\"</code>\u3002\u51b3\u5b9a\u5d4c\u5165\u6a21\u578b\u7684\u6765\u6e90\u3002 <code>model_name</code> <code>str</code> <code>\"text-embedding-ada-002\"</code> \u5d4c\u5165\u6a21\u578b\u540d\u79f0\uff08\u4f8b\u5982\uff0cOpenAI \u7684 <code>\"text-embedding-ada-002\"</code>\uff09\u3002 <code>api_key</code> <code>Optional[str]</code> <code>None</code> \u5d4c\u5165\u63d0\u4f9b\u8005\u7684 API \u5bc6\u94a5\uff0c\u67d0\u4e9b\u63d0\u4f9b\u8005\uff08\u5982 OpenAI\uff09\u9700\u8981\u3002 <code>api_url</code> <code>str</code> <code>\"https://api.openai.com/v1\"</code> \u5d4c\u5165\u6a21\u578b\u7684 API URL\uff0c\u7528\u4e8e\u81ea\u5b9a\u4e49\u7aef\u70b9\uff08\u4f8b\u5982\uff0cOpenAI \u4f7f\u7528 <code>\"https://api.openai.com/v1\"</code>\uff0collama \u901a\u5e38\u4f7f\u7528 <code>\"http://localhost:11434\"</code>\uff09\u3002 <code>dimensions</code> <code>Optional[int]</code> <code>None</code> \u5d4c\u5165\u6a21\u578b\u7684\u7ef4\u5ea6\u3002\u5fc5\u987b\u4e0e\u5411\u91cf\u5b58\u50a8\u914d\u7f6e\u5339\u914d\u3002 <code>normalize</code> <code>Optional[bool]</code> <code>True</code> \u662f\u5426\u6807\u51c6\u5316\u5d4c\u5165\uff08\u9002\u7528\u4e8e Hugging Face \u6a21\u578b\uff09\u3002 <code>device</code> <code>Optional[str]</code> <code>None</code> \u5d4c\u5165\u8ba1\u7b97\u7684\u8bbe\u5907\uff08\u4f8b\u5982\uff0c<code>\"cuda\"</code>\u3001<code>\"cpu\"</code>\uff09\uff0c\u9002\u7528\u4e8e Hugging Face \u6a21\u578b\u3002"},{"location":"zh/modules/rag.html#indexconfig","title":"IndexConfig","text":"<p>\u914d\u7f6e\u7d22\u5f15\u9636\u6bb5\u3002</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>index_type</code> <code>str</code> <code>\"vector\"</code> \u7d22\u5f15\u7c7b\u578b\uff1a<code>\"vector\"</code>\u3001<code>\"graph\"</code>\u3001<code>\"summary\"</code> \u6216 <code>\"tree\"</code>\u3002\u51b3\u5b9a\u5b58\u50a8\u548c\u68c0\u7d22\u6570\u636e\u7684\u7ed3\u6784\u3002"},{"location":"zh/modules/rag.html#retrievalconfig","title":"RetrievalConfig","text":"<p>\u63a7\u5236\u68c0\u7d22\u9636\u6bb5\uff0c\u5305\u62ec\u9884\u5904\u7406\u3001\u68c0\u7d22\u548c\u540e\u5904\u7406\u3002</p> \u53c2\u6570 \u7c7b\u578b \u9ed8\u8ba4\u503c \u63cf\u8ff0 <code>retrieval_type</code> <code>str</code> <code>\"vector\"</code> \u68c0\u7d22\u5668\u7c7b\u578b\uff1a<code>\"vector\"</code> \u6216 <code>\"graph\"</code>\u3002\u6307\u5b9a\u68c0\u7d22\u673a\u5236\u3002 <code>postprocessor_type</code> <code>str</code> <code>\"simple\"</code> \u540e\u5904\u7406\u5668\u7c7b\u578b\uff0c\u7528\u4e8e\u91cd\u65b0\u6392\u5e8f\u6216\u8fc7\u6ee4\u7ed3\u679c\uff08\u4f8b\u5982\uff0c<code>\"simple\"</code>\uff09\u3002 <code>top_k</code> <code>int</code> <code>5</code> \u67e5\u8be2\u4e2d\u68c0\u7d22\u7684\u9876\u90e8\u7ed3\u679c\u6570\u91cf\u3002 <code>similarity_cutoff</code> <code>Optional[float]</code> <code>0.7</code> \u68c0\u7d22\u5206\u5757\u7684\u6700\u5c0f\u76f8\u4f3c\u5ea6\u5f97\u5206\uff0c\u8fc7\u6ee4\u4f4e\u76f8\u5173\u6027\u7ed3\u679c\u3002 <code>keyword_filters</code> <code>Optional[List[str]]</code> <code>None</code> \u7528\u4e8e\u8fc7\u6ee4\u68c0\u7d22\u5206\u5757\u7684\u5173\u952e\u5b57\uff0c\u786e\u4fdd\u4e0e\u7279\u5b9a\u672f\u8bed\u7684\u76f8\u5173\u6027\u3002 <code>metadata_filters</code> <code>Optional[Dict[str, Any]]</code> <code>None</code> \u7528\u4e8e\u7cbe\u786e\u68c0\u7d22\u7684\u5143\u6570\u636e\u8fc7\u6ee4\u5668\uff08\u4f8b\u5982\uff0c<code>{\"file_name\": \"doc.txt\"}</code>\uff09\u3002"},{"location":"zh/modules/rag.html#\u4f7f\u7528\u8bf4\u660e","title":"\u4f7f\u7528\u8bf4\u660e","text":""},{"location":"zh/modules/rag.html#\u524d\u7f6e\u6761\u4ef6","title":"\u524d\u7f6e\u6761\u4ef6","text":"<ul> <li>\u5b89\u88c5\u4f9d\u8d56\uff1a<code>llama_index</code>\u3001<code>pydantic</code> \u4ee5\u53ca\u5176\u4ed6\u6240\u9700\u5e93\u3002</li> <li>\u914d\u7f6e\u73af\u5883\u53d8\u91cf\uff08\u4f8b\u5982\uff0c\u5d4c\u5165\u6a21\u578b\u7684 <code>OPENAI_API_KEY</code>\uff09\u3002\u53c2\u89c1 EvoAgentX \u5b89\u88c5\u6307\u5357\u3002</li> <li>\u786e\u4fdd <code>StorageHandler</code> \u5b9e\u4f8b\u5df2\u914d\u7f6e\u9002\u5f53\u7684\u5b58\u50a8\u540e\u7aef\uff08\u5982 SQLite\u3001FAISS\u3001Neo4j\uff09\u3002</li> </ul>"},{"location":"zh/modules/rag.html#\u521d\u59cb\u5316","title":"\u521d\u59cb\u5316","text":"<p>\u4f7f\u7528 <code>RAGConfig</code> \u548c <code>StorageHandler</code> \u521d\u59cb\u5316 <code>RAGEngine</code>\u3002</p> <pre><code>from evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# \u914d\u7f6e\u5b58\u50a8\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# \u914d\u7f6e RAG\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=0),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=\"your-api-key\"),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=10, similarity_cutoff=0.3)\n)\n\n# \u521d\u59cb\u5316 RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n</code></pre>"},{"location":"zh/modules/rag.html#\u6838\u5fc3\u529f\u80fd\u4f7f\u7528","title":"\u6838\u5fc3\u529f\u80fd\u4f7f\u7528","text":"<ol> <li>\u52a0\u8f7d\u6587\u6863\uff1a</li> <li>\u4f7f\u7528 <code>read</code> \u65b9\u6cd5\u52a0\u8f7d\u5e76\u5206\u5757\u6587\u6863\u5230\u8bed\u6599\u5e93\u3002</li> <li> <p>\u793a\u4f8b\uff1a      <pre><code>corpus = rag_engine.read(\n    file_paths=\"./data/docs\",\n    filter_file_by_suffix=[\".txt\", \".pdf\"],\n    merge_by_file=False,\n    show_progress=True,\n    corpus_id=\"doc_corpus\"\n)\n</code></pre></p> </li> <li> <p>\u7d22\u5f15\u6587\u6863\uff1a</p> </li> <li>\u4f7f\u7528 <code>add</code> \u65b9\u6cd5\u5c06\u8bed\u6599\u5e93\u6216\u8282\u70b9\u5217\u8868\u7d22\u5f15\u5230\u6307\u5b9a\u7d22\u5f15\u7c7b\u578b\u3002</li> <li> <p>\u793a\u4f8b\uff1a      <pre><code>rag_engine.add(index_type=\"vector\", nodes=corpus, corpus_id=\"doc_corpus\")\n</code></pre></p> </li> <li> <p>\u67e5\u8be2\uff1a</p> </li> <li>\u4f7f\u7528 <code>query</code> \u65b9\u6cd5\u6839\u636e\u67e5\u8be2\u5b57\u7b26\u4e32\u6216 <code>Query</code> \u5bf9\u8c61\u68c0\u7d22\u76f8\u5173\u5206\u5757\u3002</li> <li> <p>\u793a\u4f8b\uff1a      <pre><code>from evoagentx.rag.schema import Query\nquery = Query(query_str=\"\u6cd5\u56fd\u7684\u9996\u90fd\u662f\u4ec0\u4e48\uff1f\", top_k=5)\nresult = rag_engine.query(query, corpus_id=\"doc_corpus\")\nprint(result.corpus.chunks)  # \u68c0\u7d22\u5230\u7684\u5206\u5757\n</code></pre></p> </li> <li> <p>\u5220\u9664\u8282\u70b9\u6216\u7d22\u5f15\uff1a</p> </li> <li>\u4f7f\u7528 <code>delete</code> \u65b9\u6cd5\u5220\u9664\u7279\u5b9a\u8282\u70b9\u6216\u6574\u4e2a\u7d22\u5f15\u3002</li> <li> <p>\u793a\u4f8b\uff1a      <pre><code>rag_engine.delete(corpus_id=\"doc_corpus\", node_ids=[\"node_1\", \"node_2\"])\nrag_engine.delete(corpus_id=\"doc_corpus\", index_type=\"vector\")  # \u5220\u9664\u6574\u4e2a\u7d22\u5f15\n</code></pre></p> </li> <li> <p>\u6e05\u9664\u7d22\u5f15\uff1a</p> </li> <li>\u4f7f\u7528 <code>clear</code> \u65b9\u6cd5\u6e05\u9664\u7279\u5b9a\u8bed\u6599\u5e93\u6216\u6240\u6709\u8bed\u6599\u5e93\u7684\u7d22\u5f15\u3002</li> <li> <p>\u793a\u4f8b\uff1a      <pre><code>rag_engine.clear(corpus_id=\"doc_corpus\")  # \u6e05\u9664\u7279\u5b9a\u8bed\u6599\u5e93\nrag_engine.clear()  # \u6e05\u9664\u6240\u6709\u8bed\u6599\u5e93\n</code></pre></p> </li> <li> <p>\u4fdd\u5b58\u7d22\u5f15\uff1a</p> </li> <li>\u4f7f\u7528 <code>save</code> \u65b9\u6cd5\u5c06\u7d22\u5f15\u6301\u4e45\u5316\u5230\u6587\u4ef6\u6216\u6570\u636e\u5e93\u3002</li> <li> <p>\u793a\u4f8b\uff1a      <pre><code>rag_engine.save(output_path=\"./data/indexing\", corpus_id=\"doc_corpus\", index_type=\"vector\")\nrag_engine.save(corpus_id=\"doc_corpus\", table=\"indexing\")  # \u4fdd\u5b58\u5230\u6570\u636e\u5e93\n</code></pre></p> </li> <li> <p>\u52a0\u8f7d\u7d22\u5f15\uff1a</p> </li> <li>\u4f7f\u7528 <code>load</code> \u65b9\u6cd5\u4ece\u6587\u4ef6\u6216\u6570\u636e\u5e93\u91cd\u5efa\u7d22\u5f15\u3002</li> <li>\u793a\u4f8b\uff1a      <pre><code>rag_engine.load(source=\"./data/indexing\", corpus_id=\"doc_corpus\", index_type=\"vector\")\nrag_engine.load(corpus_id=\"doc_corpus\", table=\"indexing\")  # \u4ece\u6570\u636e\u5e93\u52a0\u8f7d\n</code></pre></li> </ol>"},{"location":"zh/modules/rag.html#\u4f7f\u7528\u793a\u4f8b","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u793a\u4f8b\u5c55\u793a\u5982\u4f55\u4f7f\u7528 <code>RAGEngine</code> \u5904\u7406 HotPotQA \u6570\u636e\u96c6\uff0c\u6267\u884c\u6587\u6863\u7d22\u5f15\u3001\u67e5\u8be2\u548c\u68c0\u7d22\u6027\u80fd\u8bc4\u4f30\u3002examples/rag_engine.py</p> <pre><code>import os\nimport json\nimport logging\nfrom typing import List, Dict\nfrom collections import defaultdict\nfrom dotenv import load_dotenv\n\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.core.logging import logger\nfrom evoagentx.storages.storages_config import VectorStoreConfig, DBConfig, StoreConfig\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, IndexConfig, EmbeddingConfig, RetrievalConfig\nfrom evoagentx.rag.schema import Query, Corpus, Chunk, ChunkMetadata\nfrom evoagentx.benchmark.hotpotqa import HotPotQA, download_raw_hotpotqa_data\n\n# \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\nload_dotenv()\n\n# \u4e0b\u8f7d\u6570\u636e\u96c6\ndownload_raw_hotpotqa_data(\"hotpot_dev_distractor_v1.json\", \"./debug/data/hotpotqa\")\ndatasets = HotPotQA(\"./debug/data/hotpotqa\")\n\n# \u521d\u59cb\u5316 StorageHandler\nstore_config = StoreConfig(\n    dbConfig=DBConfig(\n        db_name=\"sqlite\",\n        path=\"./debug/data/hotpotqa/cache/test_hotpotQA.sql\"\n    ),\n    vectorConfig=VectorStoreConfig(\n        vector_name=\"faiss\",\n        dimensions=1536,\n        index_type=\"flat_l2\",\n    ),\n    graphConfig=None,\n    path=\"./debug/data/hotpotqa/cache/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# \u521d\u59cb\u5316 RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(\n        recursive=False, exclude_hidden=True,\n        num_files_limit=None, custom_metadata_function=None,\n        extern_file_extractor=None,\n        errors=\"ignore\", encoding=\"utf-8\"\n    ),\n    chunker=ChunkerConfig(\n        strategy=\"simple\",\n        chunk_size=512,\n        chunk_overlap=0,\n        max_chunks=None\n    ),\n    embedding=EmbeddingConfig(\n        provider=\"openai\",\n        model_name=\"text-embedding-ada-002\",\n        api_key=os.environ[\"OPENAI_API_KEY\"],\n    ),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(\n        retrieval_type=\"vector\",\n        postprocessor_type=\"simple\",\n        top_k=10,  # \u68c0\u7d22\u524d 10 \u4e2a\u4e0a\u4e0b\u6587\n        similarity_cutoff=0.3,\n        keyword_filters=None,\n        metadata_filters=None\n    )\n)\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\ndef create_corpus_from_context(context: List[List], corpus_id: str) -&gt; Corpus:\n    \"\"\"\u5c06 HotPotQA \u4e0a\u4e0b\u6587\u8f6c\u6362\u4e3a\u7528\u4e8e\u7d22\u5f15\u7684\u8bed\u6599\u5e93\u3002\"\"\"\n    chunks = []\n    for title, sentences in context:\n        for idx, sentence in enumerate(sentences):\n            chunk = Chunk(\n                chunk_id=f\"{title}_{idx}\",\n                text=sentence,\n                metadata=ChunkMetadata(\n                    doc_id=str(idx),\n                    corpus_id=corpus_id\n                ),\n                start_char_idx=0,\n                end_char_idx=len(sentence),\n                excluded_embed_metadata_keys=[],\n                excluded_llm_metadata_keys=[],\n                relationships={}\n            )\n            chunk.metadata.title = title\n            chunks.append(chunk)\n    return Corpus(chunks=chunks, corpus_id=corpus_id)\n\ndef evaluate_retrieval(retrieved_chunks: List[Chunk], supporting_facts: List[List], top_k: int) -&gt; Dict[str, float]:\n    \"\"\"\u8bc4\u4f30\u68c0\u7d22\u5206\u5757\u4e0e\u652f\u6301\u4e8b\u5b9e\u7684\u5339\u914d\u60c5\u51b5\u3002\"\"\"\n    relevant = {(fact[0], fact[1]) for fact in supporting_facts}\n    retrieved = []\n    for chunk in retrieved_chunks[:top_k]:\n        title = chunk.metadata.title\n        sentence_idx = int(chunk.metadata.doc_id)\n        retrieved.append((title, sentence_idx))\n    hits = sum(1 for r in retrieved if r in relevant)\n    precision = hits / top_k if top_k &gt; 0 else 0.0\n    recall = hits / len(relevant) if len(relevant) &gt; 0 else 0.0\n    f1 = 2 * (precision * recall) / (precision + recall) if (precision + recall) &gt; 0 else 0.0\n    mrr = 0.0\n    for rank, r in enumerate(retrieved, 1):\n        if r in relevant:\n            mrr = 1.0 / rank\n            break\n    hit = 1.0 if hits &gt; 0 else 0.0\n    intersection = set((r[0], r[1]) for r in retrieved) &amp; relevant\n    union = set((r[0], r[1]) for r in retrieved) | relevant\n    jaccard = len(intersection) / len(union) if union else 0.0\n    return {\n        \"precision@k\": precision,\n        \"recall@k\": recall,\n        \"f1@k\": f1,\n        \"mrr\": mrr,\n        \"hit@k\": hit,\n        \"jaccard\": jaccard\n    }\n\ndef run_evaluation(samples: List[Dict], top_k: int = 5) -&gt; Dict[str, float]:\n    \"\"\"\u5728 HotPotQA \u6837\u672c\u4e0a\u8fd0\u884c\u8bc4\u4f30\u3002\"\"\"\n    metrics = defaultdict(list)\n    for sample in samples:\n        question = sample[\"question\"]\n        context = sample[\"context\"]\n        supporting_facts = sample[\"supporting_facts\"]\n        corpus_id = sample[\"_id\"]\n        logger.info(f\"\u5904\u7406\u6837\u672c\uff1a{corpus_id}\uff0c\u95ee\u9898\uff1a{question}\")\n        corpus = create_corpus_from_context(context, corpus_id)\n        logger.info(f\"\u521b\u5efa\u4e86\u5305\u542b {len(corpus.chunks)} \u4e2a\u5206\u5757\u7684\u8bed\u6599\u5e93\")\n        rag_engine.add(index_type=\"vector\", nodes=corpus, corpus_id=corpus_id)\n        query = Query(query_str=question, top_k=top_k)\n        result = rag_engine.query(query, corpus_id=corpus_id)\n        retrieved_chunks = result.corpus.chunks\n        logger.info(f\"\u67e5\u8be2\u68c0\u7d22\u5230 {len(retrieved_chunks)} \u4e2a\u5206\u5757\")\n        sample_metrics = evaluate_retrieval(retrieved_chunks, supporting_facts, top_k)\n        for metric_name, value in sample_metrics.items():\n            metrics[metric_name].append(value)\n        logger.info(f\"\u6837\u672c {corpus_id} \u7684\u6307\u6807\uff1a{sample_metrics}\")\n        rag_engine.clear(corpus_id=corpus_id)\n    avg_metrics = {name: sum(values) / len(values) for name, values in metrics.items()}\n    return avg_metrics\n\nif __name__ == \"__main__\":\n    samples = datasets._dev_data[:20]\n    print(len(datasets._dev_data))\n    avg_metrics = run_evaluation(samples, top_k=5)\n    logger.info(\"\u5e73\u5747\u6307\u6807\uff1a\")\n    for metric_name, value in avg_metrics.items():\n        logger.info(f\"{metric_name}: {value:.4f}\")\n    with open(\"./debug/data/hotpotqa/evaluation_results.json\", \"w\") as f:\n        json.dump(avg_metrics, f, indent=2)\n</code></pre>"},{"location":"zh/modules/rag.html#\u63a5\u53e3\u5217\u8868","title":"\u63a5\u53e3\u5217\u8868","text":"\u65b9\u6cd5 \u63cf\u8ff0 \u53c2\u6570 \u8fd4\u56de\u503c <code>__init__</code> \u521d\u59cb\u5316 RAGEngine\uff0c\u914d\u7f6e RAG \u548c\u5b58\u50a8\u5904\u7406\u7a0b\u5e8f\u3002 <code>config: RAGConfig</code>, <code>storage_handler: StorageHandler</code> \u65e0 <code>read</code> \u52a0\u8f7d\u5e76\u5206\u5757\u6587\u6863\u5230\u8bed\u6599\u5e93\u3002 <code>file_paths</code>, <code>exclude_files</code>, <code>filter_file_by_suffix</code>, <code>merge_by_file</code>, <code>show_progress</code>, <code>corpus_id</code> <code>Corpus</code> <code>add</code> \u5c06\u8282\u70b9\u6dfb\u52a0\u5230\u6307\u5b9a\u8bed\u6599\u5e93\u7684\u7d22\u5f15\u4e2d\u3002 <code>index_type</code>, <code>nodes</code>, <code>corpus_id</code> \u65e0 <code>delete</code> \u5220\u9664\u8bed\u6599\u5e93\u4e2d\u7684\u8282\u70b9\u6216\u6574\u4e2a\u7d22\u5f15\u3002 <code>corpus_id</code>, <code>index_type</code>, <code>node_ids</code>, <code>metadata_filters</code> \u65e0 <code>clear</code> \u6e05\u9664\u7279\u5b9a\u8bed\u6599\u5e93\u6216\u6240\u6709\u8bed\u6599\u5e93\u7684\u7d22\u5f15\u3002 <code>corpus_id</code> \u65e0 <code>save</code> \u5c06\u7d22\u5f15\u4fdd\u5b58\u5230\u6587\u4ef6\u6216\u6570\u636e\u5e93\u3002 <code>output_path</code>, <code>corpus_id</code>, <code>index_type</code>, <code>table</code> \u65e0 <code>load</code> \u4ece\u6587\u4ef6\u6216\u6570\u636e\u5e93\u52a0\u8f7d\u7d22\u5f15\u3002 <code>source</code>, <code>corpus_id</code>, <code>index_type</code>, <code>table</code> \u65e0 <code>query</code> \u6267\u884c\u67e5\u8be2\u5e76\u8fd4\u56de\u5904\u7406\u540e\u7684\u7ed3\u679c\u3002 <code>query</code>, <code>corpus_id</code>, <code>query_transforms</code> <code>RagResult</code>"},{"location":"zh/modules/rag.html#\u6ce8\u610f\u4e8b\u9879","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li>\u67e5\u8be2\u8f6c\u6362\uff1a<code>query</code> \u65b9\u6cd5\u652f\u6301\u53ef\u9009\u7684\u67e5\u8be2\u8f6c\u6362\u7528\u4e8e\u9884\u5904\u7406\uff0c\u53ef\u81ea\u5b9a\u4e49\u4ee5\u589e\u5f3a\u68c0\u7d22\u6548\u679c\uff08\u4f8b\u5982\uff0cHyDE\u3001\u67e5\u8be2\u5206\u89e3\uff09\u3002</li> <li>\u5b58\u50a8\u540e\u7aef\uff1a\u786e\u4fdd <code>StorageHandler</code> \u6b63\u786e\u914d\u7f6e\u4ee5\u5904\u7406\u5411\u91cf\u548c\u5143\u6570\u636e\u5b58\u50a8\u3002</li> <li>\u8b66\u544a\uff1a\u591a\u6b21\u52a0\u8f7d\u7d22\u5f15\u53ef\u80fd\u5bfc\u81f4\u95ee\u9898\uff08\u5982\u5411\u91cf\u5b58\u50a8\u4e2d\u91cd\u590d\u63d2\u5165\u8282\u70b9\uff09\u3002\u5728\u91cd\u65b0\u52a0\u8f7d\u524d\u6e05\u7a7a\u7d22\u5f15\u3002</li> </ul>"},{"location":"zh/modules/storages.html","title":"StorageHandler \u6587\u6863","text":""},{"location":"zh/modules/storages.html#\u6982\u8ff0","title":"\u6982\u8ff0","text":"<p><code>StorageHandler</code> \u7c7b\u662f\u4e00\u4e2a\u4e3a\u7ba1\u7406\u591a\u79cd\u5b58\u50a8\u540e\u7aef\u800c\u8bbe\u8ba1\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u7528\u4e8e\u5b58\u50a8\u548c\u68c0\u7d22\u5404\u7c7b\u6570\u636e\uff0c\u5982\u4ee3\u7406\u914d\u7f6e\u3001\u5de5\u4f5c\u6d41\u3001\u8bb0\u5fc6\u6761\u76ee\u548c\u7d22\u5f15\u6570\u636e\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7edf\u4e00\u7684\u63a5\u53e3\uff0c\u4e0e\u4e0d\u540c\u7c7b\u578b\u7684\u5b58\u50a8\u7cfb\u7edf\u4ea4\u4e92\uff0c\u5305\u62ec\u5173\u7cfb\u578b\u6570\u636e\u5e93\uff08\u5982 SQLite\uff09\u3001\u5411\u91cf\u6570\u636e\u5e93\uff08\u5982 FAISS\uff09\u548c\u56fe\u6570\u636e\u5e93\uff08\u5982 Neo4j\uff09\u3002\u8be5\u7c7b\u5229\u7528Pydantic\u5e93\u8fdb\u884c\u914d\u7f6e\u9a8c\u8bc1\uff0c\u4ee5\u53ca\u5de5\u5382\u6a21\u5f0f\u521d\u59cb\u5316\u5b58\u50a8\u540e\u7aef\u3002</p> <p><code>StorageHandler</code> \u4e0e <code>RAGEngine</code> \u7c7b\u7d27\u5bc6\u96c6\u6210\uff0c\u652f\u6301\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u529f\u80fd\uff0c\u901a\u8fc7\u7ba1\u7406\u7d22\u5f15\u6587\u6863\u3001\u5d4c\u5165\u5411\u91cf\u53ca\u5176\u76f8\u5173\u5143\u6570\u636e\u7684\u5b58\u50a8\u6765\u5b9e\u73b0\u3002\u5b83\u62bd\u8c61\u4e86\u4e0e\u4e0d\u540c\u5b58\u50a8\u7cfb\u7edf\u4ea4\u4e92\u7684\u590d\u6742\u6027\uff0c\u786e\u4fdd\u4e86\u957f\u671f\u8bb0\u5fc6\u7ba1\u7406\u548c RAG \u6d41\u6c34\u7ebf\u7b49\u5e94\u7528\u7684\u6570\u636e\u64cd\u4f5c\u65e0\u7f1d\u8fdb\u884c\u3002</p>"},{"location":"zh/modules/storages.html#\u7c7b\u7ed3\u6784","title":"\u7c7b\u7ed3\u6784","text":"<p><code>StorageHandler</code> \u7c7b\u7ee7\u627f\u81ea <code>BaseModule</code>\uff0c\u4f7f\u7528 Pydantic \u7684 <code>Field</code> \u8fdb\u884c\u914d\u7f6e\u548c\u7c7b\u578b\u9a8c\u8bc1\u3002\u5b83\u652f\u6301\u4e09\u79cd\u5b58\u50a8\u540e\u7aef\uff1a - \u6570\u636e\u5e93\u5b58\u50a8\uff08<code>storageDB</code>\uff09\uff1a\u7ba1\u7406\u5173\u7cfb\u578b\u6570\u636e\u5e93\u64cd\u4f5c\uff0c\u5982 SQLite\uff0c\u7528\u4e8e\u7ed3\u6784\u5316\u6570\u636e\u5b58\u50a8\u3002 - \u5411\u91cf\u5b58\u50a8\uff08<code>vector_store</code>\uff09\uff1a\u5904\u7406\u7528\u4e8e\u8bed\u4e49\u641c\u7d22\u7684\u5411\u91cf\u5d4c\u5165\uff0c\u652f\u6301 FAISS\u7b49\u3002 - \u56fe\u5b58\u50a8\uff08<code>graph_store</code>\uff09\uff1a\u7ba1\u7406\u57fa\u4e8e\u56fe\u7684\u6570\u636e\uff0c\u5982 Neo4j\uff0c\u7528\u4e8e\u5173\u7cfb\u578b\u6216\u7f51\u7edc\u5316\u6570\u636e\u7ed3\u6784\u3002</p>"},{"location":"zh/modules/storages.html#\u5173\u952e\u5c5e\u6027","title":"\u5173\u952e\u5c5e\u6027","text":"<ul> <li><code>storageConfig: StoreConfig</code>\uff1a\u6240\u6709\u5b58\u50a8\u540e\u7aef\u7684\u914d\u7f6e\u5bf9\u8c61\uff0c\u5728 <code>storages_config.py</code> \u4e2d\u5b9a\u4e49\uff0c\u5305\u542b\u6570\u636e\u5e93\u3001\u5411\u91cf\u548c\u56fe\u5b58\u50a8\u7684\u8bbe\u7f6e\u3002</li> <li><code>storageDB: Optional[Union[DBStoreBase, Any]]</code>\uff1a\u6570\u636e\u5e93\u5b58\u50a8\u540e\u7aef\u7684\u5b9e\u4f8b\uff0c\u901a\u8fc7 <code>DBStoreFactory</code> \u521d\u59cb\u5316\u3002</li> <li><code>vector_store: Optional[Union[VectorStoreBase, Any]]</code>\uff1a\u5411\u91cf\u5b58\u50a8\u540e\u7aef\u7684\u5b9e\u4f8b\uff0c\u901a\u8fc7 <code>VectorStoreFactory</code> \u521d\u59cb\u5316\u3002</li> <li><code>graph_store: Optional[Union[GraphStoreBase, Any]]</code>\uff1a\u56fe\u5b58\u50a8\u540e\u7aef\u7684\u5b9e\u4f8b\uff0c\u901a\u8fc7 <code>GraphStoreFactory</code> \u521d\u59cb\u5316\u3002</li> </ul>"},{"location":"zh/modules/storages.html#\u4f9d\u8d56\u9879","title":"\u4f9d\u8d56\u9879","text":"<ul> <li>Pydantic\uff1a\u7528\u4e8e\u914d\u7f6e\u9a8c\u8bc1\u548c\u7c7b\u578b\u68c0\u67e5\u3002</li> <li>\u5de5\u5382\u6a21\u5f0f\uff1a<code>DBStoreFactory</code>\u3001<code>VectorStoreFactory</code> \u548c <code>GraphStoreFactory</code> \u7528\u4e8e\u521b\u5efa\u5b58\u50a8\u540e\u7aef\u5b9e\u4f8b\u3002</li> <li>\u914d\u7f6e\uff1a<code>storages_config.py</code> \u4e2d\u7684 <code>StoreConfig</code>\u3001<code>DBConfig</code>\u3001<code>VectorStoreConfig</code> \u548c <code>GraphStoreConfig</code> \u7528\u4e8e\u5b9a\u4e49\u5b58\u50a8\u8bbe\u7f6e\u3002</li> <li>\u6a21\u5f0f\uff1a<code>TableType</code>\u3001<code>AgentStore</code>\u3001<code>WorkflowStore</code>\u3001<code>MemoryStore</code>\u3001<code>HistoryStore</code> \u548c <code>IndexStore</code> \u7528\u4e8e\u6570\u636e\u9a8c\u8bc1\u548c\u7ed3\u6784\u5316\u3002</li> </ul>"},{"location":"zh/modules/storages.html#\u5173\u952e\u65b9\u6cd5","title":"\u5173\u952e\u65b9\u6cd5","text":""},{"location":"zh/modules/storages.html#\u521d\u59cb\u5316","title":"\u521d\u59cb\u5316","text":"<ul> <li><code>init_module(self)</code></li> <li>\u6839\u636e\u63d0\u4f9b\u7684 <code>storageConfig</code> \u521d\u59cb\u5316\u6240\u6709\u5b58\u50a8\u540e\u7aef\u3002</li> <li> <p>\u5982\u679c\u6307\u5b9a\u4e86\u5b58\u50a8\u8def\u5f84\uff0c\u5219\u521b\u5efa\u5b58\u50a8\u76ee\u5f55\uff0c\u5e76\u901a\u8fc7\u8c03\u7528\u5404\u81ea\u7684\u521d\u59cb\u5316\u65b9\u6cd5\u6765\u521d\u59cb\u5316\u6570\u636e\u5e93\u3001\u5411\u91cf\u548c\u56fe\u5b58\u50a8\u3002</p> </li> <li> <p><code>_init_db_store(self)</code></p> </li> <li>\u4f7f\u7528 <code>DBStoreFactory</code> \u548c <code>storageConfig</code> \u4e2d\u7684 <code>dbConfig</code> \u521d\u59cb\u5316\u6570\u636e\u5e93\u5b58\u50a8\u540e\u7aef\u3002</li> <li> <p>\u8bbe\u7f6e <code>storageDB</code> \u5c5e\u6027\u3002</p> </li> <li> <p><code>_init_vector_store(self)</code></p> </li> <li>\u5982\u679c\u63d0\u4f9b\u4e86 <code>vectorConfig</code>\uff0c\u5219\u4f7f\u7528 <code>VectorStoreFactory</code> \u521d\u59cb\u5316\u5411\u91cf\u5b58\u50a8\u540e\u7aef\u3002</li> <li> <p>\u8bbe\u7f6e <code>vector_store</code> \u5c5e\u6027\u3002</p> </li> <li> <p><code>_init_graph_store(self)</code></p> </li> <li>\u5982\u679c\u63d0\u4f9b\u4e86 <code>graphConfig</code>\uff0c\u5219\u4f7f\u7528 <code>GraphStoreFactory</code> \u521d\u59cb\u5316\u56fe\u5b58\u50a8\u540e\u7aef\u3002</li> <li>\u8bbe\u7f6e <code>graph_store</code> \u5c5e\u6027\u3002</li> </ul>"},{"location":"zh/modules/storages.html#\u6570\u636e\u64cd\u4f5c","title":"\u6570\u636e\u64cd\u4f5c","text":"<ul> <li><code>load(self, tables: Optional[List[str]] = None, *args, **kwargs) -&gt; Dict[str, Any]</code></li> <li>\u4ece\u6570\u636e\u5e93\u5b58\u50a8\u4e2d\u52a0\u8f7d\u6307\u5b9a\u8868\u6216 <code>TableType</code> \u4e2d\u5b9a\u4e49\u7684\u6240\u6709\u8868\u7684\u6570\u636e\u3002</li> <li>\u8fd4\u56de\u4e00\u4e2a\u5b57\u5178\uff0c\u952e\u4e3a\u8868\u540d\uff0c\u503c\u4e3a\u8bb0\u5f55\u5217\u8868\u3002</li> <li> <p>\u6bcf\u6761\u8bb0\u5f55\u662f\u4e00\u4e2a\u5c06\u5217\u540d\u6620\u5c04\u5230\u503c\u7684\u5b57\u5178\uff0cJSON \u5b57\u6bb5\u9700\u8981\u624b\u52a8\u89e3\u6790\u3002</p> </li> <li> <p><code>save(self, data: Dict[str, Any], *args, **kwargs)</code></p> </li> <li>\u5c06\u6570\u636e\u4fdd\u5b58\u5230\u6570\u636e\u5e93\u5b58\u50a8\u4e2d\u3002</li> <li>\u63a5\u53d7\u4e00\u4e2a\u5b57\u5178\uff0c\u952e\u4e3a\u8868\u540d\uff0c\u503c\u4e3a\u8981\u4fdd\u5b58\u7684\u8bb0\u5f55\u5217\u8868\u3002</li> <li> <p>\u9a8c\u8bc1\u8868\u540d\u662f\u5426\u7b26\u5408 <code>TableType</code>\uff0c\u5e76\u4f7f\u7528 <code>storageDB.insert</code> \u63d2\u5165\u8bb0\u5f55\u3002</p> </li> <li> <p><code>parse_result(self, results: Dict[str, str], store: Union[AgentStore, WorkflowStore, MemoryStore, HistoryStore]) -&gt; Dict[str, Any]</code></p> </li> <li>\u89e3\u6790\u539f\u59cb\u6570\u636e\u5e93\u7ed3\u679c\uff0c\u6839\u636e\u63d0\u4f9b\u7684 Pydantic \u6a21\u578b\uff08<code>store</code>\uff09\u5c06 JSON \u5b57\u7b26\u4e32\u53cd\u5e8f\u5217\u5316\u4e3a Python \u5bf9\u8c61\u3002</li> <li>\u8fd4\u56de\u89e3\u6790\u540e\u7684\u7ed3\u679c\u5b57\u5178\uff0c\u9002\u5f53\u5904\u7406\u975e\u5b57\u7b26\u4e32\u5b57\u6bb5\u3002</li> </ul>"},{"location":"zh/modules/storages.html#\u5b9e\u4f53\u7279\u5b9a\u64cd\u4f5c","title":"\u5b9e\u4f53\u7279\u5b9a\u64cd\u4f5c","text":"<ul> <li><code>load_memory(self, memory_id: str, table: Optional[str]=None, **kwargs) -&gt; Dict[str, Any]</code></li> <li>\u7528\u4e8e\u52a0\u8f7d\u5355\u4e2a\u957f\u671f\u8bb0\u5fc6\u6761\u76ee\u7684\u5360\u4f4d\u65b9\u6cd5\uff0c\u901a\u8fc7 <code>memory_id</code> \u68c0\u7d22\u3002</li> <li> <p>\u5982\u679c\u672a\u6307\u5b9a\u8868\u540d\uff0c\u9ed8\u8ba4\u4f7f\u7528 <code>memory</code> \u8868\u3002</p> </li> <li> <p><code>save_memory(self, memory_data: Dict[str, Any], table: Optional[str]=None, **kwargs)</code></p> </li> <li>\u7528\u4e8e\u4fdd\u5b58\u6216\u66f4\u65b0\u5355\u4e2a\u8bb0\u5fc6\u6761\u76ee\u7684\u5360\u4f4d\u65b9\u6cd5\u3002</li> <li> <p>\u5982\u679c\u672a\u6307\u5b9a\u8868\u540d\uff0c\u9ed8\u8ba4\u4f7f\u7528 <code>memory</code> \u8868\u3002</p> </li> <li> <p><code>load_agent(self, agent_name: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]</code></p> </li> <li>\u901a\u8fc7 <code>agent_name</code> \u4ece\u6570\u636e\u5e93\u52a0\u8f7d\u5355\u4e2a\u4ee3\u7406\u7684\u6570\u636e\u3002</li> <li>\u5982\u679c\u672a\u6307\u5b9a\u8868\u540d\uff0c\u9ed8\u8ba4\u4f7f\u7528 <code>agent</code> \u8868\u3002</li> <li>\u4f7f\u7528 <code>parse_result</code> \u548c <code>AgentStore</code> \u8fdb\u884c\u7ed3\u679c\u89e3\u6790\u548c\u9a8c\u8bc1\u3002</li> <li> <p>\u5982\u679c\u672a\u627e\u5230\u4ee3\u7406\uff0c\u8fd4\u56de <code>None</code>\u3002</p> </li> <li> <p><code>remove_agent(self, agent_name: str, table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>\u4ece\u6307\u5b9a\u8868\uff08\u9ed8\u8ba4 <code>agent</code> \u8868\uff09\u4e2d\u5220\u9664\u6307\u5b9a <code>agent_name</code> \u7684\u4ee3\u7406\u3002</li> <li> <p>\u5982\u679c\u4ee3\u7406\u4e0d\u5b58\u5728\uff0c\u629b\u51fa <code>ValueError</code>\u3002</p> </li> <li> <p><code>save_agent(self, agent_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>\u5728\u6570\u636e\u5e93\u4e2d\u4fdd\u5b58\u6216\u66f4\u65b0\u5355\u4e2a\u4ee3\u7406\u7684\u6570\u636e\u3002</li> <li>\u8981\u6c42 <code>agent_data</code> \u5305\u542b <code>name</code> \u5b57\u6bb5\u3002</li> <li> <p>\u4f7f\u7528 <code>storageDB.update</code> \u66f4\u65b0\u73b0\u6709\u8bb0\u5f55\uff0c\u6216\u4f7f\u7528 <code>storageDB.insert</code> \u63d2\u5165\u65b0\u8bb0\u5f55\u3002</p> </li> <li> <p><code>load_workflow(self, workflow_id: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]</code></p> </li> <li>\u901a\u8fc7 <code>workflow_id</code> \u4ece\u6570\u636e\u5e93\u52a0\u8f7d\u5355\u4e2a\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u3002</li> <li>\u5982\u679c\u672a\u6307\u5b9a\u8868\u540d\uff0c\u9ed8\u8ba4\u4f7f\u7528 <code>workflow</code> \u8868\u3002</li> <li>\u4f7f\u7528 <code>parse_result</code> \u548c <code>WorkflowStore</code> \u8fdb\u884c\u7ed3\u679c\u89e3\u6790\u548c\u9a8c\u8bc1\u3002</li> <li> <p>\u5982\u679c\u672a\u627e\u5230\u5de5\u4f5c\u6d41\uff0c\u8fd4\u56de <code>None</code>\u3002</p> </li> <li> <p><code>save_workflow(self, workflow_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>\u5728\u6570\u636e\u5e93\u4e2d\u4fdd\u5b58\u6216\u66f4\u65b0\u5355\u4e2a\u5de5\u4f5c\u6d41\u7684\u6570\u636e\u3002</li> <li>\u8981\u6c42 <code>workflow_data</code> \u5305\u542b <code>name</code> \u5b57\u6bb5\u3002</li> <li> <p>\u4f7f\u7528 <code>storageDB.update</code> \u66f4\u65b0\u73b0\u6709\u8bb0\u5f55\uff0c\u6216\u4f7f\u7528 <code>storageDB.insert</code> \u63d2\u5165\u65b0\u8bb0\u5f55\u3002</p> </li> <li> <p><code>load_history(self, memory_id: str, table: Optional[str]=None, *args, **kwargs) -&gt; Dict[str, Any]</code></p> </li> <li>\u901a\u8fc7 <code>memory_id</code> \u4ece\u6570\u636e\u5e93\u52a0\u8f7d\u5355\u4e2a\u5386\u53f2\u6761\u76ee\u3002</li> <li>\u5982\u679c\u672a\u6307\u5b9a\u8868\u540d\uff0c\u9ed8\u8ba4\u4f7f\u7528 <code>history</code> \u8868\u3002</li> <li>\u4f7f\u7528 <code>parse_result</code> \u548c <code>HistoryStore</code> \u8fdb\u884c\u7ed3\u679c\u89e3\u6790\u548c\u9a8c\u8bc1\u3002</li> <li> <p>\u5982\u679c\u672a\u627e\u5230\u5386\u53f2\u6761\u76ee\uff0c\u8fd4\u56de <code>None</code>\u3002</p> </li> <li> <p><code>save_history(self, history_data: Dict[str, Any], table: Optional[str]=None, *args, **kwargs)</code></p> </li> <li>\u5728\u6570\u636e\u5e93\u4e2d\u4fdd\u5b58\u6216\u66f4\u65b0\u5355\u4e2a\u5386\u53f2\u6761\u76ee\u3002</li> <li>\u8981\u6c42 <code>history_data</code> \u5305\u542b <code>memory_id</code> \u5b57\u6bb5\u3002</li> <li> <p>\u66f4\u65b0\u73b0\u6709\u8bb0\u5f55\u65f6\u4fdd\u7559 <code>old_memory</code>\uff0c\u6216\u63d2\u5165\u65b0\u8bb0\u5f55\u3002</p> </li> <li> <p><code>load_index(self, corpus_id: str, table: Optional[str]=None) -&gt; Optional[Dict[str, Any]]</code></p> </li> <li>\u901a\u8fc7 <code>corpus_id</code> \u4ece\u6570\u636e\u5e93\u52a0\u8f7d\u7d22\u5f15\u6570\u636e\u3002</li> <li>\u4f7f\u7528 <code>parse_result</code> \u548c <code>IndexStore</code> \u8fdb\u884c\u7ed3\u679c\u89e3\u6790\u548c\u9a8c\u8bc1\u3002</li> <li> <p>\u5982\u679c\u672a\u627e\u5230\u7d22\u5f15\uff0c\u8fd4\u56de <code>None</code>\u3002</p> </li> <li> <p><code>save_index(self, index_data: Dict[str, Any], table: Optional[str]=None)</code></p> </li> <li>\u5728\u6570\u636e\u5e93\u4e2d\u4fdd\u5b58\u6216\u66f4\u65b0\u7d22\u5f15\u6570\u636e\u3002</li> <li>\u8981\u6c42 <code>index_data</code> \u5305\u542b <code>corpus_id</code> \u5b57\u6bb5\u3002</li> <li>\u4f7f\u7528 <code>storageDB.update</code> \u66f4\u65b0\u73b0\u6709\u8bb0\u5f55\uff0c\u6216\u4f7f\u7528 <code>storageDB.insert</code> \u63d2\u5165\u65b0\u8bb0\u5f55\u3002</li> </ul>"},{"location":"zh/modules/storages.html#\u4e0e-ragengine-\u7684\u96c6\u6210","title":"\u4e0e RAGEngine \u7684\u96c6\u6210","text":"<p><code>StorageHandler</code> \u4e0e <code>RAGEngine</code> \u7c7b\u7d27\u5bc6\u96c6\u6210\uff0c\u652f\u6301 RAG \u529f\u80fd\uff0c\u4e3b\u8981\u7528\u4e8e\uff1a - \u521d\u59cb\u5316\u5411\u91cf\u5b58\u50a8\uff1a<code>RAGEngine</code> \u6784\u9020\u51fd\u6570\u68c0\u67e5\u5411\u91cf\u5b58\u50a8\u7684\u7ef4\u5ea6\u662f\u5426\u4e0e\u5d4c\u5165\u6a21\u578b\u7684\u7ef4\u5ea6\u4e00\u81f4\uff0c\u5982\u4e0d\u4e00\u81f4\u5219\u91cd\u65b0\u521d\u59cb\u5316\u5411\u91cf\u5b58\u50a8\u3002 - \u4fdd\u5b58\u7d22\u5f15\uff1a<code>RAGEngine</code> \u7684 <code>save</code> \u65b9\u6cd5\u4f7f\u7528 <code>StorageHandler.save_index</code> \u5c06\u7d22\u5f15\u6570\u636e\uff08\u4f8b\u5982\u8bed\u6599\u5e93\u5206\u5757\u548c\u5143\u6570\u636e\uff09\u6301\u4e45\u5316\u5230\u6570\u636e\u5e93\uff0c\u5f53\u672a\u6307\u5b9a\u6587\u4ef6\u8f93\u51fa\u8def\u5f84\u65f6\u3002 - \u52a0\u8f7d\u7d22\u5f15\uff1a<code>RAGEngine</code> \u7684 <code>load</code> \u65b9\u6cd5\u4f7f\u7528 <code>StorageHandler.load</code> \u548c <code>StorageHandler.parse_result</code> \u4ece\u6570\u636e\u5e93\u8bb0\u5f55\u4e2d\u91cd\u5efa\u7d22\u5f15\uff0c\u786e\u4fdd\u4e0e\u5d4c\u5165\u6a21\u578b\u548c\u7ef4\u5ea6\u7684\u517c\u5bb9\u6027\u3002</p>"},{"location":"zh/modules/storages.html#\u914d\u7f6e","title":"\u914d\u7f6e","text":"<p><code>StorageHandler</code> \u4f9d\u8d56 <code>storages_config.py</code> \u4e2d\u5b9a\u4e49\u7684 <code>StoreConfig</code> \u7c7b\u6765\u914d\u7f6e\u5176\u540e\u7aef\uff1a - <code>DBConfig</code>\uff1a\u914d\u7f6e\u5173\u7cfb\u578b\u6570\u636e\u5e93\uff08\u5982 SQLite\uff09\uff0c\u5305\u542b <code>db_name</code>\u3001<code>path</code>\u3001<code>ip</code> \u548c <code>port</code> \u7b49\u8bbe\u7f6e\u3002 - <code>VectorStoreConfig</code>\uff1a\u914d\u7f6e\u5411\u91cf\u6570\u636e\u5e93\uff08\u5982 FAISS\u3001Qdrant\uff09\uff0c\u5305\u542b <code>vector_name</code>\u3001<code>dimensions</code>\u3001<code>index_type</code>\u3001<code>qdrant_url</code> \u548c <code>qdrant_collection_name</code> \u7b49\u8bbe\u7f6e\u3002 - <code>GraphStoreConfig</code>\uff1a\u914d\u7f6e\u56fe\u6570\u636e\u5e93\uff08\u5982 Neo4j\uff09\uff0c\u5305\u542b <code>graph_name</code>\u3001<code>uri</code>\u3001<code>username</code>\u3001<code>password</code> \u548c <code>database</code> \u7b49\u8bbe\u7f6e\u3002</p> <p>\u914d\u7f6e\u901a\u8fc7 Pydantic \u8fdb\u884c\u9a8c\u8bc1\uff0c\u786e\u4fdd\u7c7b\u578b\u68c0\u67e5\u548c\u9ed8\u8ba4\u503c\u7684\u7a33\u5065\u6027\u3002</p>"},{"location":"zh/modules/storages.html#\u4f7f\u7528\u793a\u4f8b","title":"\u4f7f\u7528\u793a\u4f8b","text":"<p>\u4ee5\u4e0b\u662f\u5982\u4f55\u521d\u59cb\u5316\u548c\u4f7f\u7528 <code>StorageHandler</code> \u7684\u793a\u4f8b\uff1a</p> <pre><code>from evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, DBConfig, VectorStoreConfig\n\n# \u5b9a\u4e49\u914d\u7f6e\nconfig = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"data/storage.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536),\n    path=\"data/index_cache\"\n)\n\n# \u521d\u59cb\u5316 StorageHandler\nstorage_handler = StorageHandler(storageConfig=config)\nstorage_handler.init_module()\n\n# \u4fdd\u5b58\u4ee3\u7406\u6570\u636e\nagent_data = {\"name\": \"agent1\", \"content\": {\"role\": \"analyst\", \"tasks\": [\"data analysis\"]}}\nstorage_handler.save_agent(agent_data)\n\n# \u52a0\u8f7d\u4ee3\u7406\u6570\u636e\nagent = storage_handler.load_agent(\"agent1\")\nprint(agent)  # {'name': 'agent1', 'content': {'role': 'analyst', 'tasks': ['data analysis']}}\n\n# \u4fdd\u5b58\u7d22\u5f15\u6570\u636e\uff08\u5728 RAGEngine \u4e2d\u4f7f\u7528\uff09\nindex_data = {\n    \"corpus_id\": \"corpus1\",\n    \"content\": {\"chunks\": [{\"chunk_id\": \"c1\", \"text\": \"\u6837\u672c\u6587\u672c\", \"metadata\": {}}]},\n    \"metadata\": {\"index_type\": \"VECTOR\", \"dimension\": 1536}\n}\nstorage_handler.save_index(index_data)\n\n# \u52a0\u8f7d\u7d22\u5f15\u6570\u636e\nindex = storage_handler.load_index(\"corpus1\")\nprint(index)  # {'corpus_id': 'corpus1', 'content': {...}, 'metadata': {...}}\n</code></pre>"},{"location":"zh/modules/storages.html#\u6ce8\u610f\u4e8b\u9879","title":"\u6ce8\u610f\u4e8b\u9879","text":"<ul> <li><code>load_memory</code> \u548c <code>save_memory</code> \u65b9\u6cd5\u8fd8\u672a\u5b8c\u5168\u5b9e\u73b0\uff0c\u540e\u7eed\u5c06\u5728\u4e0eLongTermMemory\u4e00\u540c\u5b9e\u73b0\u3002</li> <li><code>StorageHandler</code> \u5047\u8bbe\u6570\u636e\u5e93\u6a21\u5f0f\u7531 <code>DBStoreBase</code> \u53ca\u5176\u5de5\u5382\u7ba1\u7406\uff0c\u786e\u4fdd\u4e0e <code>TableType</code> \u679a\u4e3e\u7684\u517c\u5bb9\u6027\u3002</li> <li>\u4e0e <code>RAGEngine</code> \u4e00\u8d77\u4f7f\u7528\u65f6\uff0c\u786e\u4fdd\u5411\u91cf\u5b58\u50a8\u7684\u7ef4\u5ea6\u4e0e\u5d4c\u5165\u6a21\u578b\u7684\u7ef4\u5ea6\u4e00\u81f4\uff0c\u4ee5\u907f\u514d\u91cd\u65b0\u521d\u59cb\u5316\u95ee\u9898\u3002</li> <li>\u9519\u8bef\u5904\u7406\u8d2f\u7a7f\u59cb\u7ec8\uff0c\u901a\u8fc7 <code>evoagentx.core.logging.logger</code> \u6a21\u5757\u751f\u6210\u65e5\u5fd7\u3002</li> </ul>"},{"location":"zh/modules/storages.html#\u7ed3\u8bba","title":"\u7ed3\u8bba","text":"<p><code>StorageHandler</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u53ef\u6269\u5c55\u7684\u63a5\u53e3\uff0c\u4ee5\u7edf\u4e00\u7684\u65b9\u5f0f\u7ba1\u7406\u591a\u79cd\u5b58\u50a8\u540e\u7aef\u3002\u5176\u4e0e <code>RAGEngine</code> \u7684\u96c6\u6210\u4f7f\u5176\u6210\u4e3a RAG \u6d41\u6c34\u7ebf\u7684\u5173\u952e\u7ec4\u4ef6\uff0c\u652f\u6301\u7d22\u5f15\u6570\u636e\u7684\u6709\u6548\u5b58\u50a8\u548c\u68c0\u7d22\u3002\u901a\u8fc7\u5229\u7528\u5de5\u5382\u6a21\u5f0f\u548c Pydantic \u9a8c\u8bc1\uff0c\u5b83\u786e\u4fdd\u4e86\u7a33\u5065\u6027\u548c\u53ef\u6269\u5c55\u6027\uff0c\u9002\u7528\u4e8e\u9700\u8981\u590d\u6742\u6570\u636e\u7ba1\u7406\u7684\u5e94\u7528\u3002</p>"},{"location":"zh/modules/workflow_graph.html","title":"\u5de5\u4f5c\u6d41\u56fe","text":""},{"location":"zh/modules/workflow_graph.html#\u7b80\u4ecb","title":"\u7b80\u4ecb","text":"<p><code>WorkFlowGraph</code> \u7c7b\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u57fa\u7840\u7ec4\u4ef6\uff0c\u7528\u4e8e\u521b\u5efa\u3001\u7ba1\u7406\u548c\u6267\u884c\u590d\u6742\u7684 AI \u4ee3\u7406\u5de5\u4f5c\u6d41\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7ed3\u6784\u5316\u7684\u65b9\u5f0f\u6765\u5b9a\u4e49\u4efb\u52a1\u4f9d\u8d56\u5173\u7cfb\u3001\u6267\u884c\u987a\u5e8f\u548c\u4efb\u52a1\u4e4b\u95f4\u7684\u6570\u636e\u6d41\u3002</p> <p>\u5de5\u4f5c\u6d41\u56fe\u8868\u793a\u4e00\u7ec4\u4efb\u52a1\uff08\u8282\u70b9\uff09\u53ca\u5176\u4f9d\u8d56\u5173\u7cfb\uff08\u8fb9\uff09\uff0c\u8fd9\u4e9b\u4efb\u52a1\u9700\u8981\u6309\u7279\u5b9a\u987a\u5e8f\u6267\u884c\u4ee5\u5b9e\u73b0\u76ee\u6807\u3002<code>SequentialWorkFlowGraph</code> \u662f\u4e00\u4e2a\u4e13\u95e8\u7684\u5b9e\u73b0\uff0c\u4e13\u6ce8\u4e8e\u4ece\u5f00\u59cb\u5230\u7ed3\u675f\u7684\u5355\u4e00\u8def\u5f84\u7684\u7ebf\u6027\u5de5\u4f5c\u6d41\u3002</p>"},{"location":"zh/modules/workflow_graph.html#\u67b6\u6784","title":"\u67b6\u6784","text":""},{"location":"zh/modules/workflow_graph.html#\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","title":"\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","text":"<p><code>WorkFlowGraph</code> \u7531\u51e0\u4e2a\u5173\u952e\u7ec4\u4ef6\u7ec4\u6210\uff1a</p> <ol> <li> <p>\u8282\u70b9\uff08WorkFlowNode\uff09\uff1a</p> <p>\u6bcf\u4e2a\u8282\u70b9\u4ee3\u8868\u5de5\u4f5c\u6d41\u4e2d\u7684\u4e00\u4e2a\u4efb\u52a1\u6216\u64cd\u4f5c\uff0c\u5177\u6709\u4ee5\u4e0b\u5c5e\u6027\uff1a</p> <ul> <li><code>name</code>\uff1a\u4efb\u52a1\u7684\u552f\u4e00\u6807\u8bc6\u7b26</li> <li><code>description</code>\uff1a\u4efb\u52a1\u529f\u80fd\u7684\u8be6\u7ec6\u63cf\u8ff0</li> <li><code>inputs</code>\uff1a\u4efb\u52a1\u6240\u9700\u7684\u8f93\u5165\u53c2\u6570\u5217\u8868\uff0c\u6bcf\u4e2a\u8f93\u5165\u53c2\u6570\u662f <code>Parameter</code> \u7c7b\u7684\u5b9e\u4f8b</li> <li><code>outputs</code>\uff1a\u4efb\u52a1\u4ea7\u751f\u7684\u8f93\u51fa\u53c2\u6570\u5217\u8868\uff0c\u6bcf\u4e2a\u8f93\u51fa\u53c2\u6570\u662f <code>Parameter</code> \u7c7b\u7684\u5b9e\u4f8b</li> <li><code>agents</code>\uff08\u53ef\u9009\uff09\uff1a\u53ef\u4ee5\u6267\u884c\u6b64\u4efb\u52a1\u7684\u4ee3\u7406\u5217\u8868\uff0c\u6bcf\u4e2a\u4ee3\u7406\u5e94\u8be5\u662f\u4e00\u4e2a\u4e0e <code>agent_manager</code> \u4e2d\u4ee3\u7406\u540d\u79f0\u5339\u914d\u7684\u5b57\u7b26\u4e32\uff0c\u6216\u8005\u662f\u4e00\u4e2a\u6307\u5b9a\u4ee3\u7406\u540d\u79f0\u548c\u914d\u7f6e\u7684\u5b57\u5178\uff0c\u8be5\u914d\u7f6e\u5c06\u7528\u4e8e\u5728 <code>agent_manager</code> \u4e2d\u521b\u5efa <code>CustomizeAgent</code> \u5b9e\u4f8b\u3002\u6709\u5173\u4ee3\u7406\u914d\u7f6e\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u81ea\u5b9a\u4e49\u4ee3\u7406\u6587\u6863\u3002</li> <li><code>action_graph</code>\uff08\u53ef\u9009\uff09\uff1a<code>ActionGraph</code> \u7c7b\u7684\u5b9e\u4f8b\uff0c\u5176\u4e2d\u6bcf\u4e2a\u52a8\u4f5c\u90fd\u662f <code>Operator</code> \u7c7b\u7684\u5b9e\u4f8b\u3002\u6709\u5173\u52a8\u4f5c\u56fe\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u52a8\u4f5c\u56fe\u6587\u6863\u3002</li> <li><code>status</code>\uff1a\u4efb\u52a1\u7684\u5f53\u524d\u6267\u884c\u72b6\u6001\uff08PENDING\u3001RUNNING\u3001COMPLETED\u3001FAILED\uff09</li> </ul> <p>Note</p> <ol> <li> <p>\u60a8\u5e94\u8be5\u63d0\u4f9b <code>agents</code> \u6216 <code>action_graph</code> \u6765\u6267\u884c\u4efb\u52a1\u3002\u5982\u679c\u4e24\u8005\u90fd\u63d0\u4f9b\uff0c\u5c06\u4f7f\u7528 <code>action_graph</code>\u3002</p> </li> <li> <p>\u5982\u679c\u60a8\u63d0\u4f9b\u4e00\u7ec4 <code>agents</code>\uff0c\u8fd9\u4e9b\u4ee3\u7406\u5c06\u534f\u540c\u5de5\u4f5c\u4ee5\u5b8c\u6210\u4efb\u52a1\u3002\u4f7f\u7528 <code>WorkFlow</code> \u6267\u884c\u4efb\u52a1\u65f6\uff0c\u7cfb\u7edf\u5c06\u6839\u636e\u4ee3\u7406\u4fe1\u606f\u548c\u6267\u884c\u5386\u53f2\u81ea\u52a8\u786e\u5b9a\u6267\u884c\u987a\u5e8f\uff08\u52a8\u4f5c\uff09\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u6267\u884c\u4efb\u52a1\u65f6\uff0c<code>WorkFlow</code> \u5c06\u5206\u6790\u8fd9\u4e9b\u4ee3\u7406\u4e2d\u7684\u6240\u6709\u53ef\u80fd\u52a8\u4f5c\uff0c\u5e76\u6839\u636e\u4efb\u52a1\u63cf\u8ff0\u548c\u6267\u884c\u5386\u53f2\u91cd\u590d\u9009\u62e9\u6700\u4f73\u52a8\u4f5c\u6267\u884c\u3002</p> </li> <li> <p>\u5982\u679c\u60a8\u63d0\u4f9b <code>action_graph</code>\uff0c\u5b83\u5c06\u76f4\u63a5\u7528\u4e8e\u5b8c\u6210\u4efb\u52a1\u3002\u4f7f\u7528 <code>WorkFlow</code> \u6267\u884c\u4efb\u52a1\u65f6\uff0c\u7cfb\u7edf\u5c06\u6309\u7167 <code>action_graph</code> \u5b9a\u4e49\u7684\u987a\u5e8f\u6267\u884c\u52a8\u4f5c\u5e76\u8fd4\u56de\u7ed3\u679c\u3002</p> </li> </ol> </li> <li> <p>\u8fb9\uff08WorkFlowEdge\uff09\uff1a</p> <p>\u8fb9\u8868\u793a\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\uff0c\u5b9a\u4e49\u6267\u884c\u987a\u5e8f\u548c\u6570\u636e\u6d41\u3002\u6bcf\u6761\u8fb9\u5177\u6709\uff1a</p> <ul> <li><code>source</code>\uff1a\u6e90\u8282\u70b9\u540d\u79f0\uff08\u8fb9\u7684\u8d77\u70b9\uff09</li> <li><code>target</code>\uff1a\u76ee\u6807\u8282\u70b9\u540d\u79f0\uff08\u8fb9\u7684\u7ec8\u70b9\uff09</li> <li><code>priority</code>\uff08\u53ef\u9009\uff09\uff1a\u5f71\u54cd\u6267\u884c\u987a\u5e8f\u7684\u6570\u503c\u4f18\u5148\u7ea7</li> </ul> </li> <li> <p>\u56fe\u7ed3\u6784\uff1a</p> <p>\u5728\u5185\u90e8\uff0c\u5de5\u4f5c\u6d41\u8868\u793a\u4e3a\u6709\u5411\u56fe\uff0c\u5176\u4e2d\uff1a</p> <ul> <li>\u8282\u70b9\u8868\u793a\u4efb\u52a1</li> <li>\u8fb9\u8868\u793a\u4efb\u52a1\u4e4b\u95f4\u7684\u4f9d\u8d56\u5173\u7cfb\u548c\u6570\u636e\u6d41</li> <li>\u56fe\u7ed3\u6784\u652f\u6301\u7ebf\u6027\u5e8f\u5217\u548c\u66f4\u590d\u6742\u7684\u6a21\u5f0f\uff1a<ul> <li>\u5206\u53c9-\u5408\u5e76\u6a21\u5f0f\uff08\u7a0d\u540e\u91cd\u65b0\u6c47\u5408\u7684\u5e76\u884c\u6267\u884c\u8def\u5f84\uff09</li> <li>\u6761\u4ef6\u5206\u652f</li> <li>\u5de5\u4f5c\u6d41\u4e2d\u6f5c\u5728\u7684\u5faa\u73af</li> </ul> </li> </ul> </li> <li> <p>\u8282\u70b9\u72b6\u6001\uff1a</p> <p>\u5de5\u4f5c\u6d41\u4e2d\u7684\u6bcf\u4e2a\u8282\u70b9\u53ef\u4ee5\u5904\u4e8e\u4ee5\u4e0b\u72b6\u6001\u4e4b\u4e00\uff1a</p> <ul> <li><code>PENDING</code>\uff1a\u4efb\u52a1\u7b49\u5f85\u6267\u884c</li> <li><code>RUNNING</code>\uff1a\u4efb\u52a1\u6b63\u5728\u6267\u884c</li> <li><code>COMPLETED</code>\uff1a\u4efb\u52a1\u5df2\u6210\u529f\u6267\u884c</li> <li><code>FAILED</code>\uff1a\u4efb\u52a1\u6267\u884c\u5931\u8d25</li> </ul> </li> </ol>"},{"location":"zh/modules/workflow_graph.html#\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","title":"\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe\u67b6\u6784","text":"<p><code>SequentialWorkFlowGraph</code> \u662f <code>WorkFlowGraph</code> \u7684\u4e13\u95e8\u5b9e\u73b0\uff0c\u5b83\u81ea\u52a8\u63a8\u65ad\u8282\u70b9\u8fde\u63a5\u4ee5\u521b\u5efa\u7ebf\u6027\u5de5\u4f5c\u6d41\u3002\u5b83\u4e13\u4e3a\u66f4\u7b80\u5355\u7684\u7528\u4f8b\u8bbe\u8ba1\uff0c\u5176\u4e2d\u4efb\u52a1\u9700\u8981\u6309\u987a\u5e8f\u6267\u884c\uff0c\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u51fa\u4f5c\u4e3a\u4e0b\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u5165\u3002</p>"},{"location":"zh/modules/workflow_graph.html#\u8f93\u5165\u683c\u5f0f","title":"\u8f93\u5165\u683c\u5f0f","text":"<p><code>SequentialWorkFlowGraph</code> \u63a5\u53d7\u7b80\u5316\u7684\u8f93\u5165\u683c\u5f0f\uff0c\u4f7f\u5b9a\u4e49\u7ebf\u6027\u5de5\u4f5c\u6d41\u53d8\u5f97\u5bb9\u6613\u3002\u60a8\u4e0d\u9700\u8981\u663e\u5f0f\u5b9a\u4e49\u8282\u70b9\u548c\u8fb9\uff0c\u800c\u662f\u6309\u7167\u6267\u884c\u987a\u5e8f\u63d0\u4f9b\u4efb\u52a1\u5217\u8868\u3002\u6bcf\u4e2a\u4efb\u52a1\u90fd\u5b9a\u4e49\u4e3a\u4e00\u4e2a\u5b57\u5178\uff0c\u5305\u542b\u4ee5\u4e0b\u5b57\u6bb5\uff1a</p> <ul> <li><code>name</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u7684\u552f\u4e00\u6807\u8bc6\u7b26</li> <li><code>description</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u529f\u80fd\u7684\u8be6\u7ec6\u63cf\u8ff0</li> <li><code>inputs</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u7684\u8f93\u5165\u53c2\u6570\u5217\u8868</li> <li><code>outputs</code>\uff08\u5fc5\u9700\uff09\uff1a\u4efb\u52a1\u4ea7\u751f\u7684\u8f93\u51fa\u53c2\u6570\u5217\u8868</li> <li><code>prompt</code>\uff08\u5fc5\u9700\uff09\uff1a\u6307\u5bfc\u4ee3\u7406\u884c\u4e3a\u7684\u63d0\u793a\u6a21\u677f</li> <li><code>system_prompt</code>\uff08\u53ef\u9009\uff09\uff1a\u4e3a\u4ee3\u7406\u63d0\u4f9b\u4e0a\u4e0b\u6587\u7684\u7cfb\u7edf\u6d88\u606f</li> <li><code>output_parser</code>\uff08\u53ef\u9009\uff09\uff1a\u7528\u4e8e\u89e3\u6790\u4efb\u52a1\u8f93\u51fa\u7684\u89e3\u6790\u5668</li> <li><code>parse_mode</code>\uff08\u53ef\u9009\uff09\uff1a\u89e3\u6790\u8f93\u51fa\u7684\u6a21\u5f0f\uff0c\u9ed8\u8ba4\u4e3a \"str\"</li> <li><code>parse_func</code>\uff08\u53ef\u9009\uff09\uff1a\u7528\u4e8e\u89e3\u6790\u8f93\u51fa\u7684\u81ea\u5b9a\u4e49\u51fd\u6570</li> <li><code>parse_title</code>\uff08\u53ef\u9009\uff09\uff1a\u89e3\u6790\u8f93\u51fa\u7684\u6807\u9898</li> </ul> <p>\u4e0e\u63d0\u793a\u548c\u89e3\u6790\u76f8\u5173\u7684\u53c2\u6570\u5c06\u7528\u4e8e\u5728 <code>agent_manager</code> \u4e2d\u521b\u5efa <code>CustomizeAgent</code> \u5b9e\u4f8b\u3002\u6709\u5173\u4ee3\u7406\u914d\u7f6e\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u81ea\u5b9a\u4e49\u4ee3\u7406\u6587\u6863\u3002</p>"},{"location":"zh/modules/workflow_graph.html#\u5185\u90e8\u8f6c\u6362\u4e3a\u5de5\u4f5c\u6d41\u56fe","title":"\u5185\u90e8\u8f6c\u6362\u4e3a\u5de5\u4f5c\u6d41\u56fe","text":"<p>\u5728\u5185\u90e8\uff0c<code>SequentialWorkFlowGraph</code> \u901a\u8fc7\u4ee5\u4e0b\u65b9\u5f0f\u81ea\u52a8\u5c06\u6b64\u7b80\u5316\u7684\u4efb\u52a1\u5217\u8868\u8f6c\u6362\u4e3a\u5b8c\u6574\u7684 <code>WorkFlowGraph</code>\uff1a</p> <ol> <li> <p>\u521b\u5efa WorkFlowNode \u5b9e\u4f8b\uff1a\u5bf9\u4e8e\u8f93\u5165\u5217\u8868\u4e2d\u7684\u6bcf\u4e2a\u4efb\u52a1\uff0c\u5b83\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u9002\u5f53\u5c5e\u6027\u7684 <code>WorkFlowNode</code>\u3002\u5728\u6b64\u8fc7\u7a0b\u4e2d\uff1a</p> <ul> <li>\u5b83\u5c06\u4efb\u52a1\u5b9a\u4e49\u8f6c\u6362\u4e3a\u5177\u6709\u8f93\u5165\u3001\u8f93\u51fa\u548c\u5173\u8054\u4ee3\u7406\u7684\u8282\u70b9</li> <li>\u5b83\u6839\u636e\u4efb\u52a1\u540d\u79f0\u81ea\u52a8\u751f\u6210\u552f\u4e00\u7684\u4ee3\u7406\u540d\u79f0</li> <li>\u5b83\u4f7f\u7528\u63d0\u4f9b\u7684\u63d0\u793a\u3001\u7cfb\u7edf\u63d0\u793a\u548c\u89e3\u6790\u9009\u9879\u914d\u7f6e\u4ee3\u7406</li> </ul> </li> <li> <p>\u63a8\u65ad\u8fb9\u8fde\u63a5\uff1a\u5b83\u68c0\u67e5\u6bcf\u4e2a\u4efb\u52a1\u7684\u8f93\u5165\u548c\u8f93\u51fa\u53c2\u6570\uff0c\u5e76\u81ea\u52a8\u521b\u5efa <code>WorkFlowEdge</code> \u5b9e\u4f8b\u6765\u8fde\u63a5\u4efb\u52a1\uff0c\u5176\u4e2d\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u51fa\u4e0e\u53e6\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u5165\u5339\u914d</p> </li> <li> <p>\u6784\u5efa\u56fe\u7ed3\u6784\uff1a\u6700\u540e\uff0c\u5b83\u6784\u5efa\u8868\u793a\u5de5\u4f5c\u6d41\u7684\u5b8c\u6574\u6709\u5411\u56fe\uff0c\u6240\u6709\u8282\u70b9\u548c\u8fb9\u90fd\u6b63\u786e\u8fde\u63a5</p> </li> </ol> <p>\u8fd9\u79cd\u81ea\u52a8\u8f6c\u6362\u8fc7\u7a0b\u4f7f\u5f97\u5b9a\u4e49\u987a\u5e8f\u5de5\u4f5c\u6d41\u53d8\u5f97\u66f4\u5bb9\u6613\uff0c\u65e0\u9700\u624b\u52a8\u6307\u5b9a\u6240\u6709\u56fe\u7ec4\u4ef6\u3002</p>"},{"location":"zh/modules/workflow_graph.html#\u4f7f\u7528\u65b9\u6cd5","title":"\u4f7f\u7528\u65b9\u6cd5","text":""},{"location":"zh/modules/workflow_graph.html#\u57fa\u672c\u5de5\u4f5c\u6d41\u56fe\u521b\u5efa\u4e0e\u6267\u884c","title":"\u57fa\u672c\u5de5\u4f5c\u6d41\u56fe\u521b\u5efa\u4e0e\u6267\u884c","text":"<pre><code>from evoagentx.workflow.workflow_graph import WorkFlowNode, WorkFlowGraph, WorkFlowEdge\nfrom evoagentx.workflow.workflow import WorkFlow \nfrom evoagentx.agents import AgentManager, CustomizeAgent \nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM \n\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"xxx\", stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\nagent_manager = AgentManager()\n\ndata_extraction_agent = CustomizeAgent(\n    name=\"DataExtractionAgent\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    prompt=\"Extract data from source: {data_source}\",\n    llm_config=llm_config\n)  \n\ndata_transformation_agent = CustomizeAgent(\n    name=\"DataTransformationAgent\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    prompt=\"Transform data: {extracted_data}\",\n    llm_config=llm_config\n)\n\n# \u5c06\u4ee3\u7406\u6dfb\u52a0\u5230\u4ee3\u7406\u7ba1\u7406\u5668\u4ee5\u6267\u884c\u5de5\u4f5c\u6d41\ndata_extraction_agent = agent_manager.add_agents(agents = [data_extraction_agent, data_transformation_agent])\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u8282\u70b9\ntask1 = WorkFlowNode(\n    name=\"Task1\",\n    description=\"Extract data from source\",\n    inputs=[{\"name\": \"data_source\", \"type\": \"string\", \"description\": \"Source data location\"}],\n    outputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Extracted data\"}],\n    agents=[\"DataExtractionAgent\"] # \u5e94\u8be5\u4e0e\u4ee3\u7406\u7ba1\u7406\u5668\u4e2d\u7684\u4ee3\u7406\u540d\u79f0\u5339\u914d\n)\n\ntask2 = WorkFlowNode(\n    name=\"Task2\",\n    description=\"Transform data\",\n    inputs=[{\"name\": \"extracted_data\", \"type\": \"string\", \"description\": \"Data to transform\"}],\n    outputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Transformed data\"}],\n    agents=[\"DataTransformationAgent\"] # \u5e94\u8be5\u4e0e\u4ee3\u7406\u7ba1\u7406\u5668\u4e2d\u7684\u4ee3\u7406\u540d\u79f0\u5339\u914d\n)\n\ntask3 = WorkFlowNode(\n    name=\"Task3\",\n    description=\"Analyze data and generate insights\",\n    inputs=[{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n    outputs=[{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n    agents=[\n        {\n            \"name\": \"DataAnalysisAgent\",\n            \"description\": \"Analyze data and generate insights\",\n            \"inputs\": [{\"name\": \"transformed_data\", \"type\": \"string\", \"description\": \"Data to analyze\"}],\n            \"outputs\": [{\"name\": \"insights\", \"type\": \"string\", \"description\": \"Generated insights\"}],\n            \"prompt\": \"Analyze data and generate insights: {transformed_data}\",\n            \"parse_mode\": \"str\",\n        } # \u5c06\u7528\u4e8e\u5728 agent_manager \u4e2d\u521b\u5efa CustomizeAgent \u5b9e\u4f8b\n    ]\n)\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u8fb9\nedge1 = WorkFlowEdge(source=\"Task1\", target=\"Task2\")\nedge2 = WorkFlowEdge(source=\"Task2\", target=\"Task3\")\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph = WorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    nodes=[task1, task2, task3],\n    edges=[edge1, edge2]\n)\n\n# \u5c06\u4ee3\u7406\u6dfb\u52a0\u5230\u4ee3\u7406\u7ba1\u7406\u5668\u4ee5\u6267\u884c\u5de5\u4f5c\u6d41\nagent_manager.add_agents_from_workflow(workflow_graph, llm_config=llm_config)\n\n# \u521b\u5efa\u5de5\u4f5c\u6d41\u5b9e\u4f8b\u4ee5\u6267\u884c\nworkflow = WorkFlow(graph=workflow_graph, agent_manager=agent_manager, llm=llm)\nworkflow.execute(inputs={\"data_source\": \"xxx\"})\n</code></pre>"},{"location":"zh/modules/workflow_graph.html#\u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe","title":"\u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe","text":"<pre><code>from evoagentx.workflow.workflow_graph import SequentialWorkFlowGraph\n\n# \u5b9a\u4e49\u4efb\u52a1\u53ca\u5176\u8f93\u5165\u3001\u8f93\u51fa\u548c\u63d0\u793a\ntasks = [\n    {\n        \"name\": \"DataExtraction\",\n        \"description\": \"Extract data from the specified source\",\n        \"inputs\": [\n            {\"name\": \"data_source\", \"type\": \"string\", \"required\": True, \"description\": \"Source data location\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Extracted data\"}\n        ],\n        \"prompt\": \"Extract data from the following source: {data_source}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataTransformation\",\n        \"description\": \"Transform the extracted data\",\n        \"inputs\": [\n            {\"name\": \"extracted_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to transform\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Transformed data\"}\n        ],\n        \"prompt\": \"Transform the following data: {extracted_data}\", \n        \"parse_mode\": \"str\"\n    },\n    {\n        \"name\": \"DataAnalysis\",\n        \"description\": \"Analyze data and generate insights\",\n        \"inputs\": [\n            {\"name\": \"transformed_data\", \"type\": \"string\", \"required\": True, \"description\": \"Data to analyze\"}\n        ],\n        \"outputs\": [\n            {\"name\": \"insights\", \"type\": \"string\", \"required\": True, \"description\": \"Generated insights\"}\n        ],\n        \"prompt\": \"Analyze the following data and generate insights: {transformed_data}\", \n        \"parse_mode\": \"str\"\n    }\n]\n\n# \u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\u56fe\nsequential_workflow_graph = SequentialWorkFlowGraph(\n    goal=\"Extract, transform, and analyze data to generate insights\",\n    tasks=tasks\n)\n</code></pre>"},{"location":"zh/modules/workflow_graph.html#\u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","title":"\u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","text":"<pre><code># \u4fdd\u5b58\u5de5\u4f5c\u6d41\nworkflow_graph.save_module(\"examples/output/my_workflow.json\")\n\n# \u5bf9\u4e8e SequentialWorkFlowGraph\uff0c\u4f7f\u7528 save_module \u548c get_graph_info\nsequential_workflow_graph.save_module(\"examples/output/my_sequential_workflow.json\")\n</code></pre>"},{"location":"zh/modules/workflow_graph.html#\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41","title":"\u53ef\u89c6\u5316\u5de5\u4f5c\u6d41","text":"<pre><code># \u4ee5\u53ef\u89c6\u65b9\u5f0f\u663e\u793a\u5de5\u4f5c\u6d41\u56fe\nworkflow_graph.display()\n</code></pre> <p><code>WorkFlowGraph</code> \u548c <code>SequentialWorkFlowGraph</code> \u7c7b\u63d0\u4f9b\u4e86\u4e00\u79cd\u7075\u6d3b\u800c\u5f3a\u5927\u7684\u65b9\u5f0f\u6765\u8bbe\u8ba1\u590d\u6742\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u3001\u8ddf\u8e2a\u5176\u6267\u884c\u5e76\u7ba1\u7406\u4efb\u52a1\u4e4b\u95f4\u7684\u6570\u636e\u6d41\u3002 </p>"},{"location":"zh/tutorial/aflow_optimizer.html","title":"AFlow \u4f18\u5316\u5668\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u5982\u4f55\u4f7f\u7528 EvoAgentX \u7684 AFlow \u4f18\u5316\u5668\u6765\u4f18\u5316\u4f60\u7684\u5de5\u4f5c\u6d41\u3002AFlow \u4f18\u5316\u5668\u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u53ef\u4ee5\u5e2e\u52a9\u4f60\u81ea\u52a8\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u3002</p>"},{"location":"zh/tutorial/aflow_optimizer.html#1-\u6982\u8ff0","title":"1. \u6982\u8ff0","text":"<p>AFlow \u4f18\u5316\u5668\u662f EvoAgentX \u6846\u67b6\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u4ef6\uff0c\u5b83\u63d0\u4f9b\u4e86\u4ee5\u4e0b\u529f\u80fd\uff1a</p> <ul> <li>\u81ea\u52a8\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u6027\u80fd</li> <li>\u652f\u6301\u591a\u79cd\u4f18\u5316\u7b56\u7565</li> <li>\u63d0\u4f9b\u8be6\u7ec6\u7684\u4f18\u5316\u62a5\u544a</li> <li>\u652f\u6301\u81ea\u5b9a\u4e49\u4f18\u5316\u76ee\u6807</li> </ul>"},{"location":"zh/tutorial/aflow_optimizer.html#2-\u8bbe\u7f6e-aflow-\u4f18\u5316\u5668","title":"2. \u8bbe\u7f6e AFlow \u4f18\u5316\u5668","text":"<p>\u9996\u5148\uff0c\u4f60\u9700\u8981\u5bfc\u5165\u76f8\u5173\u6a21\u5757\u5e76\u8bbe\u7f6e AFlow \u4f18\u5316\u5668\u3002</p> <pre><code>from evoagentx.optimizers import AFlowOptimizer\nfrom evoagentx.config import Config\nfrom evoagentx.models import OpenAIConfig, OpenAI\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer.html#\u914d\u7f6e-llm-\u6a21\u578b","title":"\u914d\u7f6e LLM \u6a21\u578b","text":"<p>\u4f60\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684 OpenAI API \u5bc6\u94a5\u6765\u521d\u59cb\u5316 LLM\u3002\u5efa\u8bae\u5c06 API \u5bc6\u94a5\u4fdd\u5b58\u5728 <code>.env</code> \u6587\u4ef6\u4e2d\uff0c\u5e76\u4f7f\u7528 <code>load_dotenv</code> \u51fd\u6570\u52a0\u8f7d\u5b83\uff1a <pre><code>import os\nfrom dotenv import load_dotenv\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nllm_config = OpenAIConfig(model=\"gpt-4\", openai_key=OPENAI_API_KEY)\nllm = OpenAI(config=llm_config)\n</code></pre></p>"},{"location":"zh/tutorial/aflow_optimizer.html#3-\u8bbe\u7f6e\u7ec4\u4ef6","title":"3. \u8bbe\u7f6e\u7ec4\u4ef6","text":""},{"location":"zh/tutorial/aflow_optimizer.html#\u7b2c\u4e00\u6b65\u5b9a\u4e49\u4efb\u52a1\u914d\u7f6e","title":"\u7b2c\u4e00\u6b65\uff1a\u5b9a\u4e49\u4efb\u52a1\u914d\u7f6e","text":"<p>AFlow \u4f18\u5316\u5668\u9700\u8981\u4e00\u4e2a\u914d\u7f6e\u6765\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u548c\u53ef\u7528\u7684\u64cd\u4f5c\u7b26\u3002\u4ee5\u4e0b\u662f\u4e0d\u540c\u4efb\u52a1\u7c7b\u578b\u7684\u793a\u4f8b\u914d\u7f6e\uff1a</p> <pre><code>EXPERIMENTAL_CONFIG = {\n    \"humaneval\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    }, \n    \"mbpp\": {\n        \"question_type\": \"code\", \n        \"operators\": [\"Custom\", \"CustomCodeGenerate\", \"Test\", \"ScEnsemble\"] \n    },\n    \"hotpotqa\": {\n        \"question_type\": \"qa\", \n        \"operators\": [\"Custom\", \"AnswerGenerate\", \"QAScEnsemble\"]\n    },\n    \"gsm8k\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    },\n    \"math\": {\n        \"question_type\": \"math\", \n        \"operators\": [\"Custom\", \"ScEnsemble\", \"Programmer\"]\n    }\n}\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer.html#\u7b2c\u4e8c\u6b65\u5b9a\u4e49\u521d\u59cb\u5de5\u4f5c\u6d41","title":"\u7b2c\u4e8c\u6b65\uff1a\u5b9a\u4e49\u521d\u59cb\u5de5\u4f5c\u6d41","text":"<p>AFlow \u4f18\u5316\u5668\u9700\u8981\u4e24\u4e2a\u6587\u4ef6\uff1a - <code>graph.py</code>\uff1a\u8be5\u6587\u4ef6\u7528 Python \u4ee3\u7801\u5b9a\u4e49\u521d\u59cb\u5de5\u4f5c\u6d41\u56fe\u3002 - <code>prompt.py</code>\uff1a\u8be5\u6587\u4ef6\u5b9a\u4e49\u5de5\u4f5c\u6d41\u4e2d\u4f7f\u7528\u7684\u63d0\u793a\u3002</p> <p>\u4ee5\u4e0b\u662f HumanEval \u57fa\u51c6\u7684 <code>graph.py</code> \u6587\u4ef6\u793a\u4f8b\uff1a</p> <pre><code>import evoagentx.workflow.operators as operator\nimport examples.aflow.code_generation.prompt as prompt_custom # noqa: F401\nfrom evoagentx.models.model_configs import LLMConfig\nfrom evoagentx.benchmark.benchmark import Benchmark\nfrom evoagentx.models.model_utils import create_llm_instance\n\nclass Workflow:\n\n    def __init__(\n        self,\n        name: str,\n        llm_config: LLMConfig,\n        benchmark: Benchmark\n    ):\n        self.name = name\n        self.llm = create_llm_instance(llm_config)\n        self.benchmark = benchmark \n        self.custom = operator.Custom(self.llm)\n        self.custom_code_generate = operator.CustomCodeGenerate(self.llm)\n\n    async def __call__(self, problem: str, entry_point: str):\n        \"\"\"\n        \u5de5\u4f5c\u6d41\u7684\u5b9e\u73b0\n        Custom \u64cd\u4f5c\u7b26\u53ef\u4ee5\u751f\u6210\u4efb\u4f55\u4f60\u60f3\u8981\u7684\u5185\u5bb9\u3002\n        \u4f46\u5f53\u4f60\u60f3\u83b7\u53d6\u6807\u51c6\u4ee3\u7801\u65f6\uff0c\u5e94\u8be5\u4f7f\u7528 custom_code_generate \u64cd\u4f5c\u7b26\u3002\n        \"\"\"\n        # await self.custom(input=, instruction=\"\")\n        solution = await self.custom_code_generate(problem=problem, entry_point=entry_point, instruction=prompt_custom.GENERATE_PYTHON_CODE_PROMPT) # \u4f46\u5f53\u4f60\u60f3\u83b7\u53d6\u6807\u51c6\u4ee3\u7801\u65f6\uff0c\u5e94\u8be5\u4f7f\u7528 customcodegenerator\n        return solution['response']\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5728\u5b9a\u4e49\u5de5\u4f5c\u6d41\u65f6\uff0c\u8bf7\u6ce8\u610f\u4ee5\u4e0b\u5173\u952e\u70b9\uff1a</p> <ol> <li> <p>\u63d0\u793a\u5bfc\u5165\u8def\u5f84\uff1a\u786e\u4fdd\u6b63\u786e\u6307\u5b9a <code>prompt.py</code> \u7684\u5bfc\u5165\u8def\u5f84\uff08\u4f8b\u5982 <code>examples.aflow.code_generation.prompt</code>\uff09\u3002\u6b64\u8def\u5f84\u5e94\u8be5\u4e0e\u4f60\u7684\u9879\u76ee\u7ed3\u6784\u5339\u914d\uff0c\u4ee5\u4fbf\u6b63\u786e\u52a0\u8f7d\u63d0\u793a\u3002</p> </li> <li> <p>\u64cd\u4f5c\u7b26\u521d\u59cb\u5316\uff1a\u5728 <code>__init__</code> \u51fd\u6570\u4e2d\uff0c\u4f60\u5fc5\u987b\u521d\u59cb\u5316\u5de5\u4f5c\u6d41\u4e2d\u5c06\u4f7f\u7528\u7684\u6240\u6709\u64cd\u4f5c\u7b26\u3002\u6bcf\u4e2a\u64cd\u4f5c\u7b26\u90fd\u5e94\u8be5\u4f7f\u7528\u9002\u5f53\u7684 LLM \u5b9e\u4f8b\u8fdb\u884c\u5b9e\u4f8b\u5316\u3002</p> </li> <li> <p>\u5de5\u4f5c\u6d41\u6267\u884c\uff1a<code>__call__</code> \u51fd\u6570\u662f\u5de5\u4f5c\u6d41\u6267\u884c\u7684\u4e3b\u8981\u5165\u53e3\u70b9\u3002\u5b83\u5e94\u8be5\u5b9a\u4e49\u5de5\u4f5c\u6d41\u7684\u5b8c\u6574\u6267\u884c\u903b\u8f91\uff0c\u5e76\u8fd4\u56de\u5c06\u7528\u4e8e\u8bc4\u4f30\u7684\u6700\u7ec8\u8f93\u51fa\u3002</p> </li> </ol> <p>\u4ee5\u4e0b\u662f HumanEval \u57fa\u51c6\u7684 <code>prompt.py</code> \u6587\u4ef6\u793a\u4f8b\uff1a</p> <pre><code>GENERATE_PYTHON_CODE_PROMPT = \"\"\"\nGenerate a functional and correct Python code for the given problem.\n\nProblem: \"\"\"\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5982\u679c\u5de5\u4f5c\u6d41\u4e0d\u9700\u8981\u4efb\u4f55\u63d0\u793a\uff0c\u5219 <code>prompt.py</code> \u6587\u4ef6\u53ef\u4ee5\u4e3a\u7a7a\u3002</p>"},{"location":"zh/tutorial/aflow_optimizer.html#\u7b2c\u4e09\u6b65\u51c6\u5907\u57fa\u51c6","title":"\u7b2c\u4e09\u6b65\uff1a\u51c6\u5907\u57fa\u51c6","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 AFlowHumanEval \u57fa\u51c6\u3002\u5b83\u9075\u5faa\u4e0e \u539f\u59cb AFlow \u5b9e\u73b0 \u4e2d\u76f8\u540c\u7684\u6570\u636e\u5212\u5206\u548c\u683c\u5f0f\u3002</p> <pre><code># Initialize the benchmark\nhumaneval = AFlowHumanEval()\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer.html#4-\u914d\u7f6e\u548c\u8fd0\u884c-aflow-\u4f18\u5316\u5668","title":"4. \u914d\u7f6e\u548c\u8fd0\u884c AFlow \u4f18\u5316\u5668","text":"<p>AFlow \u4f18\u5316\u5668\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u53c2\u6570\u8fdb\u884c\u914d\u7f6e\uff0c\u4ee5\u63a7\u5236\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code>optimizer = AFlowOptimizer(\n    graph_path=\"examples/aflow/code_generation\",  # \u521d\u59cb\u5de5\u4f5c\u6d41\u56fe\u7684\u8def\u5f84\n    optimized_path=\"examples/aflow/humaneval/optimized\",  # \u4fdd\u5b58\u4f18\u5316\u5de5\u4f5c\u6d41\u7684\u8def\u5f84\n    optimizer_llm=optimizer_llm,  # \u7528\u4e8e\u4f18\u5316\u7684 LLM\n    executor_llm=executor_llm,    # \u7528\u4e8e\u6267\u884c\u7684 LLM\n    validation_rounds=3,          # \u4f18\u5316\u671f\u95f4\u5728\u5f00\u53d1\u96c6\u4e0a\u8fd0\u884c\u9a8c\u8bc1\u7684\u6b21\u6570\n    eval_rounds=3,               # \u6d4b\u8bd5\u671f\u95f4\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fd0\u884c\u8bc4\u4f30\u7684\u6b21\u6570\n    max_rounds=20,               # \u6700\u5927\u4f18\u5316\u8f6e\u6570\n    **EXPERIMENTAL_CONFIG[\"humaneval\"]  # \u7279\u5b9a\u4efb\u52a1\u7684\u914d\u7f6e\uff0c\u7528\u4e8e\u6307\u5b9a\u4efb\u52a1\u7c7b\u578b\u548c\u53ef\u7528\u64cd\u4f5c\u7b26\n)\n</code></pre>"},{"location":"zh/tutorial/aflow_optimizer.html#\u8fd0\u884c\u4f18\u5316","title":"\u8fd0\u884c\u4f18\u5316","text":"<p>\u8981\u5f00\u59cb\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code># \u4f18\u5316\u5de5\u4f5c\u6d41\noptimizer.optimize(humaneval)\n</code></pre> <p>\u6ce8\u610f</p> <p>\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c\u5de5\u4f5c\u6d41\u5c06\u5728\u6bcf\u4e00\u6b65\u9aa4\u4e0a\u5bf9\u5f00\u53d1\u96c6\u8fdb\u884c <code>validation_rounds</code> \u6b21\u9a8c\u8bc1\u3002\u786e\u4fdd\u57fa\u51c6 <code>humaneval</code> \u5305\u542b\u5f00\u53d1\u96c6\uff08\u5373 <code>self._dev_data</code> \u4e0d\u4e3a\u7a7a\uff09\u3002</p>"},{"location":"zh/tutorial/aflow_optimizer.html#\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41","title":"\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41","text":"<p>\u8981\u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41\uff1a</p> <p><pre><code># \u6d4b\u8bd5\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41\noptimizer.test(humaneval)\n</code></pre> \u9ed8\u8ba4\u60c5\u51b5\u4e0b\uff0c\u4f18\u5316\u5668\u5c06\u9009\u62e9\u9a8c\u8bc1\u6027\u80fd\u6700\u9ad8\u7684\u5de5\u4f5c\u6d41\u8fdb\u884c\u6d4b\u8bd5\u3002\u4f60\u8fd8\u53ef\u4ee5\u4f7f\u7528 <code>test_rounds: List[int]</code> \u53c2\u6570\u6307\u5b9a\u6d4b\u8bd5\u8f6e\u6b21\u3002\u4f8b\u5982\uff0c\u8981\u8bc4\u4f30\u7b2c\u4e8c\u8f6e\u548c\u7b2c\u4e09\u8f6e\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>optimizer.test(humaneval, test_rounds=[2, 3])</code>\u3002</p> <p>\u6ce8\u610f</p> <p>\u5728\u6d4b\u8bd5\u671f\u95f4\uff0c\u5de5\u4f5c\u6d41\u5c06\u5728\u6d4b\u8bd5\u96c6\u4e0a\u8fdb\u884c <code>eval_rounds</code> \u6b21\u8bc4\u4f30\u3002\u786e\u4fdd\u57fa\u51c6 <code>humaneval</code> \u5305\u542b\u6d4b\u8bd5\u96c6\uff08\u5373 <code>self._test_data</code> \u4e0d\u4e3a\u7a7a\uff09\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 aflow_humaneval.py\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html","title":"\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u4f7f\u7528 EvoAgentX \u8bbe\u7f6e\u548c\u8fd0\u884c\u57fa\u51c6\u6d4b\u8bd5\u8bc4\u4f30\u3002\u6211\u4eec\u5c06\u4f7f\u7528 HotpotQA \u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\uff0c\u6f14\u793a\u5982\u4f55\u8bbe\u7f6e\u548c\u8fd0\u884c\u8bc4\u4f30\u8fc7\u7a0b\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#1-\u6982\u8ff0","title":"1. \u6982\u8ff0","text":"<p>EvoAgentX \u63d0\u4f9b\u4e86\u4e00\u4e2a\u7075\u6d3b\u4e14\u6a21\u5757\u5316\u7684\u8bc4\u4f30\u6846\u67b6\uff0c\u4f7f\u4f60\u80fd\u591f\uff1a</p> <ul> <li>\u52a0\u8f7d\u548c\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u57fa\u51c6\u6570\u636e\u96c6</li> <li>\u81ea\u5b9a\u4e49\u6570\u636e\u52a0\u8f7d\u3001\u5904\u7406\u548c\u540e\u671f\u5904\u7406\u903b\u8f91</li> <li>\u8bc4\u4f30\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\u7684\u6027\u80fd</li> <li>\u5e76\u884c\u5904\u7406\u591a\u4e2a\u8bc4\u4f30\u4efb\u52a1</li> </ul>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#2-\u8bbe\u7f6e\u57fa\u51c6\u6d4b\u8bd5","title":"2. \u8bbe\u7f6e\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u9996\u5148\uff0c\u4f60\u9700\u8981\u5bfc\u5165\u76f8\u5173\u6a21\u5757\u5e76\u8bbe\u7f6e\u8bc4\u4f30\u8fc7\u7a0b\u4e2d\u4ee3\u7406\u5c06\u4f7f\u7528\u7684\u8bed\u8a00\u6a21\u578b\uff08LLM\uff09\u3002</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAIConfig, OpenAI \nfrom evoagentx.benchmark import HotpotQA\nfrom evoagentx.workflow import QAActionGraph \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u914d\u7f6e-llm-\u6a21\u578b","title":"\u914d\u7f6e LLM \u6a21\u578b","text":"<p>\u4f60\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684 OpenAI API \u5bc6\u94a5\u6765\u521d\u59cb\u5316 LLM\u3002\u5efa\u8bae\u5c06 API \u5bc6\u94a5\u4fdd\u5b58\u5728 <code>.env</code> \u6587\u4ef6\u4e2d\uff0c\u5e76\u4f7f\u7528 <code>load_dotenv</code> \u51fd\u6570\u52a0\u8f7d\u5b83\uff1a <pre><code>import os \nfrom dotenv import load_dotenv\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre></p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#3-\u521d\u59cb\u5316\u57fa\u51c6\u6d4b\u8bd5","title":"3. \u521d\u59cb\u5316\u57fa\u51c6\u6d4b\u8bd5","text":"<p>EvoAgentX \u5305\u542b\u591a\u4e2a\u9884\u5b9a\u4e49\u7684\u57fa\u51c6\u6d4b\u8bd5\uff0c\u7528\u4e8e\u95ee\u7b54\u3001\u6570\u5b66\u548c\u7f16\u7801\u7b49\u4efb\u52a1\u3002\u6709\u5173\u73b0\u6709\u57fa\u51c6\u6d4b\u8bd5\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 Benchmark README\u3002\u4f60\u8fd8\u53ef\u4ee5\u901a\u8fc7\u6269\u5c55\u57fa\u7840 <code>Benchmark</code> \u63a5\u53e3\u6765\u5b9a\u4e49\u81ea\u5df1\u7684\u57fa\u51c6\u6d4b\u8bd5\u7c7b\uff0c\u6211\u4eec\u5728 \u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5 \u90e8\u5206\u63d0\u4f9b\u4e86\u4e00\u4e2a\u793a\u4f8b\u3002</p> <p>\u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 <code>HotpotQA</code> \u57fa\u51c6\u6d4b\u8bd5\u3002 <pre><code>benchmark = HotPotQA(mode=\"dev\")\n</code></pre> \u5176\u4e2d <code>mode</code> \u53c2\u6570\u51b3\u5b9a\u52a0\u8f7d\u6570\u636e\u96c6\u7684\u54ea\u4e2a\u90e8\u5206\u3002\u9009\u9879\u5305\u62ec\uff1a</p> <ul> <li><code>\"train\"</code>\uff1a\u8bad\u7ec3\u6570\u636e</li> <li><code>\"dev\"</code>\uff1a\u5f00\u53d1/\u9a8c\u8bc1\u6570\u636e</li> <li><code>\"test\"</code>\uff1a\u6d4b\u8bd5\u6570\u636e</li> <li><code>\"all\"</code>\uff08\u9ed8\u8ba4\uff09\uff1a\u52a0\u8f7d\u6574\u4e2a\u6570\u636e\u96c6</li> </ul> <p>\u6570\u636e\u5c06\u81ea\u52a8\u4e0b\u8f7d\u5230\u9ed8\u8ba4\u7f13\u5b58\u6587\u4ef6\u5939\uff0c\u4f46\u4f60\u53ef\u4ee5\u901a\u8fc7\u6307\u5b9a <code>path</code> \u53c2\u6570\u6765\u66f4\u6539\u6b64\u4f4d\u7f6e\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#4-\u8fd0\u884c\u8bc4\u4f30","title":"4. \u8fd0\u884c\u8bc4\u4f30","text":"<p>\u4e00\u65e6\u4f60\u51c6\u5907\u597d\u4e86\u57fa\u51c6\u6d4b\u8bd5\u548c LLM\uff0c\u4e0b\u4e00\u6b65\u5c31\u662f\u5b9a\u4e49\u4f60\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u548c\u8bc4\u4f30\u903b\u8f91\u3002EvoAgentX \u652f\u6301\u5b8c\u5168\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b\u7684\u5904\u7406\u65b9\u5f0f\u548c\u8f93\u51fa\u7684\u89e3\u91ca\u65b9\u5f0f\u3002</p> <p>\u4ee5\u4e0b\u662f\u5982\u4f55\u4f7f\u7528 <code>HotpotQA</code> \u57fa\u51c6\u6d4b\u8bd5\u548c QA \u5de5\u4f5c\u6d41\u8fd0\u884c\u8bc4\u4f30\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u6b65\u9aa4-1\u5b9a\u4e49\u4ee3\u7406\u5de5\u4f5c\u6d41","title":"\u6b65\u9aa4 1\uff1a\u5b9a\u4e49\u4ee3\u7406\u5de5\u4f5c\u6d41","text":"<p>\u4f60\u53ef\u4ee5\u4f7f\u7528\u9884\u5b9a\u4e49\u7684\u5de5\u4f5c\u6d41\u4e4b\u4e00\u6216\u5b9e\u73b0\u81ea\u5df1\u7684\u5de5\u4f5c\u6d41\u3002\u5728\u8fd9\u4e2a\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u4f7f\u7528\u4e3a\u95ee\u7b54\u8bbe\u8ba1\u7684 <code>QAActionGraph</code>\uff0c\u5b83\u7b80\u5355\u5730\u4f7f\u7528\u81ea\u4e00\u81f4\u6027\u6765\u751f\u6210\u6700\u7ec8\u7b54\u6848\uff1a</p> <pre><code>workflow = QAActionGraph(\n    llm_config=llm_config,\n    description=\"This workflow aims to address multi-hop QA tasks.\"\n)\n</code></pre>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u6b65\u9aa4-2\u81ea\u5b9a\u4e49\u6570\u636e\u9884\u5904\u7406\u548c\u540e\u5904\u7406","title":"\u6b65\u9aa4 2\uff1a\u81ea\u5b9a\u4e49\u6570\u636e\u9884\u5904\u7406\u548c\u540e\u5904\u7406","text":"<p>\u8bc4\u4f30\u7684\u4e0b\u4e00\u4e2a\u5173\u952e\u65b9\u9762\u662f\u6b63\u786e\u5730\u5728\u57fa\u51c6\u6d4b\u8bd5\u3001\u5de5\u4f5c\u6d41\u548c\u8bc4\u4f30\u6307\u6807\u4e4b\u95f4\u8f6c\u6362\u6570\u636e\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u4e3a\u4ec0\u4e48\u9700\u8981\u9884\u5904\u7406\u548c\u540e\u5904\u7406","title":"\u4e3a\u4ec0\u4e48\u9700\u8981\u9884\u5904\u7406\u548c\u540e\u5904\u7406","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u9884\u5904\u7406\u548c\u540e\u5904\u7406\u662f\u786e\u4fdd\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\u3001\u5de5\u4f5c\u6d41\u548c\u8bc4\u4f30\u903b\u8f91\u4e4b\u95f4\u987a\u7545\u4ea4\u4e92\u7684\u91cd\u8981\u6b65\u9aa4\uff1a</p> <ul> <li> <p>\u9884\u5904\u7406\uff08<code>collate_func</code>\uff09\uff1a  </p> <p>\u6765\u81ea HotpotQA \u7b49\u57fa\u51c6\u6d4b\u8bd5\u7684\u539f\u59cb\u793a\u4f8b\u901a\u5e38\u5305\u542b\u7ed3\u6784\u5316\u5b57\u6bb5\uff0c\u5982\u95ee\u9898\u3001\u7b54\u6848\u548c\u4e0a\u4e0b\u6587\u3002\u4f46\u662f\uff0c\u4f60\u7684\u4ee3\u7406\u5de5\u4f5c\u6d41\u901a\u5e38\u671f\u671b\u4e00\u4e2a\u5355\u4e00\u7684\u63d0\u793a\u5b57\u7b26\u4e32\u6216\u5176\u4ed6\u7ed3\u6784\u5316\u8f93\u5165\u3002<code>collate_func</code> \u7528\u4e8e\u5c06\u6bcf\u4e2a\u539f\u59cb\u793a\u4f8b\u8f6c\u6362\u4e3a\u4f60\u7684\uff08\u81ea\u5b9a\u4e49\uff09\u5de5\u4f5c\u6d41\u53ef\u4ee5\u4f7f\u7528\u7684\u683c\u5f0f\u3002</p> </li> <li> <p>\u540e\u5904\u7406\uff08<code>output_postprocess_func</code>\uff09\uff1a</p> <p>\u5de5\u4f5c\u6d41\u8f93\u51fa\u53ef\u80fd\u5305\u62ec\u63a8\u7406\u6b65\u9aa4\u6216\u8d85\u51fa\u6700\u7ec8\u7b54\u6848\u7684\u989d\u5916\u683c\u5f0f\u3002\u7531\u4e8e <code>Evaluator</code> \u5185\u90e8\u8c03\u7528\u57fa\u51c6\u6d4b\u8bd5\u7684 <code>evaluate</code> \u65b9\u6cd5\u6765\u8ba1\u7b97\u6307\u6807\uff08\u4f8b\u5982\uff0c\u7cbe\u786e\u5339\u914d\u6216 F1\uff09\uff0c\u901a\u5e38\u9700\u8981\u4ee5\u5e72\u51c0\u7684\u683c\u5f0f\u63d0\u53d6\u6700\u7ec8\u7b54\u6848\u3002<code>output_postprocess_func</code> \u5904\u7406\u8fd9\u4e00\u70b9\uff0c\u786e\u4fdd\u8f93\u51fa\u9002\u5408\u8bc4\u4f30\u3002</p> </li> </ul> <p>\u7b80\u800c\u8a00\u4e4b\uff0c\u9884\u5904\u7406\u4e3a\u5de5\u4f5c\u6d41\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5\u793a\u4f8b\uff0c\u800c\u540e\u5904\u7406\u4e3a\u8bc4\u4f30\u51c6\u5907\u5de5\u4f5c\u6d41\u8f93\u51fa\u3002</p> <p>\u5728\u4ee5\u4e0b\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u5b9a\u4e49\u4e86\u4e00\u4e2a <code>collate_func</code> \u6765\u5c06\u539f\u59cb\u793a\u4f8b\u683c\u5f0f\u5316\u4e3a\u5de5\u4f5c\u6d41\u7684\u63d0\u793a\uff0c\u4ee5\u53ca\u4e00\u4e2a <code>output_postprocess_func</code> \u6765\u4ece\u5de5\u4f5c\u6d41\u8f93\u51fa\u4e2d\u63d0\u53d6\u6700\u7ec8\u7b54\u6848\u3002</p> <p>\u53ef\u4ee5\u4f7f\u7528 <code>collate_func</code> \u683c\u5f0f\u5316\u57fa\u51c6\u6d4b\u8bd5\u4e2d\u7684\u6bcf\u4e2a\u793a\u4f8b\uff0c\u5b83\u5c06\u539f\u59cb\u793a\u4f8b\u8f6c\u6362\u4e3a\u4ee3\u7406\u7684\u63d0\u793a\u6216\u7ed3\u6784\u5316\u8f93\u5165\u3002</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        example (dict): A dictionary containing the raw example data.\n\n    Returns: \n        The expected input for the (custom) workflow.\n    \"\"\"\n    problem = \"Question: {}\\n\\n\".format(example[\"question\"])\n    context_list = []\n    for item in example[\"context\"]:\n        context = \"Title: {}\\nText: {}\".format(item[0], \" \".join([t.strip() for t in item[1]]))\n        context_list.append(context)\n    context = \"\\n\\n\".join(context_list)\n    problem += \"Context: {}\\n\\n\".format(context)\n    problem += \"Answer:\" \n    return {\"problem\": problem}\n</code></pre> <p>\u5728\u4ee3\u7406\u751f\u6210\u8f93\u51fa\u540e\uff0c\u4f60\u53ef\u4ee5\u5b9a\u4e49\u5982\u4f55\u4f7f\u7528 <code>output_postprocess_func</code> \u63d0\u53d6\u6700\u7ec8\u7b54\u6848\u3002 <pre><code>def output_postprocess_func(output: dict) -&gt; dict:\n    \"\"\"\n    Args:\n        output (dict): The output from the workflow.\n\n    Returns: \n        The processed output that can be used to compute the metrics. The output will be directly passed to the benchmark's `evaluate` method. \n    \"\"\"\n    return output[\"answer\"]\n</code></pre></p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u6b65\u9aa4-3\u521d\u59cb\u5316\u8bc4\u4f30\u5668","title":"\u6b65\u9aa4 3\uff1a\u521d\u59cb\u5316\u8bc4\u4f30\u5668","text":"<p>\u8bc4\u4f30\u5668\u5c06\u6240\u6709\u5185\u5bb9\u8054\u7cfb\u5728\u4e00\u8d77\u2014\u2014\u5b83\u5728\u57fa\u51c6\u6d4b\u8bd5\u4e0a\u8fd0\u884c\u5de5\u4f5c\u6d41\u5e76\u8ba1\u7b97\u6027\u80fd\u6307\u6807\u3002</p> <p><pre><code>evaluator = Evaluator(\n    llm=llm, \n    collate_func=collate_func,\n    output_postprocess_func=output_postprocess_func,\n    verbose=True, \n    num_workers=3 \n)\n</code></pre> \u5982\u679c <code>num_workers</code> \u5927\u4e8e 1\uff0c\u8bc4\u4f30\u5c06\u5728\u591a\u4e2a\u7ebf\u7a0b\u4e0a\u5e76\u884c\u8fdb\u884c\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u6b65\u9aa4-4\u8fd0\u884c\u8bc4\u4f30","title":"\u6b65\u9aa4 4\uff1a\u8fd0\u884c\u8bc4\u4f30","text":"<p>\u73b0\u5728\uff0c\u4f60\u53ef\u4ee5\u901a\u8fc7\u5411\u8bc4\u4f30\u5668\u63d0\u4f9b\u5de5\u4f5c\u6d41\u548c\u57fa\u51c6\u6d4b\u8bd5\u6765\u8fd0\u884c\u8bc4\u4f30\uff1a</p> <p><pre><code>with suppress_logger_info():\n    results = evaluator.evaluate(\n        graph=workflow, \n        benchmark=benchmark, \n        eval_mode=\"dev\", # Evaluation split: train / dev / test \n        sample_k=10 # If set, randomly sample k examples from the benchmark for evaluation  \n    )\n\nprint(\"Evaluation metrics: \", results)\n</code></pre> \u5176\u4e2d <code>suppress_logger_info</code> \u7528\u4e8e\u6291\u5236\u65e5\u5fd7\u4fe1\u606f\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 \u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorial/benchmark_and_evaluation.html#\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5","title":"\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u8981\u5b9a\u4e49\u81ea\u5b9a\u4e49\u57fa\u51c6\u6d4b\u8bd5\uff0c\u4f60\u9700\u8981\u6269\u5c55 <code>Benchmark</code> \u7c7b\u5e76\u5b9e\u73b0\u4ee5\u4e0b\u65b9\u6cd5\uff1a</p> <ul> <li> <p><code>_load_data(self)</code>\uff1a </p> <p>\u52a0\u8f7d\u57fa\u51c6\u6d4b\u8bd5\u6570\u636e\uff0c\u5e76\u8bbe\u7f6e <code>self._train_data</code>\u3001<code>self._dev_data</code> \u548c <code>self._test_data</code> \u5c5e\u6027\u3002</p> </li> <li> <p><code>_get_id(self, example: Any) -&gt; Any</code>\uff1a </p> <p>\u8fd4\u56de\u793a\u4f8b\u7684\u552f\u4e00\u6807\u8bc6\u7b26\u3002</p> </li> <li> <p><code>_get_label(self, example: Any) -&gt; Any</code>\uff1a</p> <p>\u8fd4\u56de\u4e0e\u7ed9\u5b9a\u793a\u4f8b\u5173\u8054\u7684\u6807\u7b7e\u6216\u771f\u5b9e\u503c\u3002</p> <p>\u8fd9\u7528\u4e8e\u5728\u8bc4\u4f30\u671f\u95f4\u5c06\u9884\u6d4b\u4e0e\u6b63\u786e\u7b54\u6848\u8fdb\u884c\u6bd4\u8f83\u3002\u8f93\u51fa\u5c06\u76f4\u63a5\u4f20\u9012\u7ed9 <code>evaluate</code> \u65b9\u6cd5\u3002</p> </li> <li> <p><code>evaluate(self, prediction: Any, label: Any) -&gt; dict</code>\uff1a </p> <p>\u57fa\u4e8e\u9884\u6d4b\u548c\u771f\u5b9e\u6807\u7b7e\uff08\u4ece <code>_get_label</code> \u83b7\u53d6\uff09\u8ba1\u7b97\u5355\u4e2a\u793a\u4f8b\u7684\u8bc4\u4f30\u6307\u6807\u3002 \u6b64\u65b9\u6cd5\u5e94\u8fd4\u56de\u6307\u6807\u540d\u79f0\u548c\u503c\u7684\u5b57\u5178\u3002</p> </li> <li> <p><code>evaluate(self, prediction: Any, label: Any) -&gt; dict</code>: </p> <p>Compute the evaluation metrics for a single example, based on its prediction and ground-truth label (obtained from <code>_get_label</code>). This method should return a dictionary of metric name(s) and value(s).</p> </li> </ul> <p>For a complete example of a benchmark implementation, please refer to the HotPotQA class.</p>"},{"location":"zh/tutorial/first_agent.html","title":"\u6784\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u4ee3\u7406","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u4ee3\u7406\u662f\u8bbe\u8ba1\u7528\u6765\u81ea\u4e3b\u5b8c\u6210\u7279\u5b9a\u4efb\u52a1\u7684\u667a\u80fd\u7ec4\u4ef6\u3002\u672c\u6559\u7a0b\u5c06\u5f15\u5bfc\u4f60\u4e86\u89e3\u5728 EvoAgentX \u4e2d\u521b\u5efa\u548c\u4f7f\u7528\u4ee3\u7406\u7684\u57fa\u672c\u6982\u5ff5\uff1a</p> <ol> <li>\u4f7f\u7528 CustomizeAgent \u521b\u5efa\u7b80\u5355\u4ee3\u7406\uff1a\u5b66\u4e60\u5982\u4f55\u4f7f\u7528\u81ea\u5b9a\u4e49\u63d0\u793a\u521b\u5efa\u57fa\u672c\u4ee3\u7406</li> <li>\u4f7f\u7528\u591a\u4e2a\u52a8\u4f5c\uff1a\u521b\u5efa\u53ef\u4ee5\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u7684\u66f4\u590d\u6742\u7684\u4ee3\u7406</li> <li>\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406\uff1a\u5b66\u4e60\u5982\u4f55\u4fdd\u5b58\u548c\u52a0\u8f7d\u4f60\u7684\u4ee3\u7406</li> </ol> <p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u80fd\u591f\u521b\u5efa\u7b80\u5355\u548c\u590d\u6742\u7684\u4ee3\u7406\uff0c\u4e86\u89e3\u5b83\u4eec\u5982\u4f55\u5904\u7406\u8f93\u5165\u548c\u8f93\u51fa\uff0c\u4ee5\u53ca\u5982\u4f55\u5728\u9879\u76ee\u4e2d\u4fdd\u5b58\u548c\u91cd\u7528\u5b83\u4eec\u3002</p>"},{"location":"zh/tutorial/first_agent.html#1-\u4f7f\u7528-customizeagent-\u521b\u5efa\u7b80\u5355\u4ee3\u7406","title":"1. \u4f7f\u7528 CustomizeAgent \u521b\u5efa\u7b80\u5355\u4ee3\u7406","text":"<p>\u521b\u5efa\u4ee3\u7406\u6700\u7b80\u5355\u7684\u65b9\u6cd5\u662f\u4f7f\u7528 <code>CustomizeAgent</code>\uff0c\u5b83\u5141\u8bb8\u4f60\u5feb\u901f\u5b9a\u4e49\u4e00\u4e2a\u5177\u6709\u7279\u5b9a\u63d0\u793a\u7684\u4ee3\u7406\u3002</p> <p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5bfc\u5165\u5fc5\u8981\u7684\u7ec4\u4ef6\u5e76\u8bbe\u7f6e LLM\uff1a</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.models import OpenAILLMConfig\nfrom evoagentx.agents import CustomizeAgent\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# Configure LLM\nopenai_config = OpenAILLMConfig(\n    model=\"gpt-4o-mini\", \n    openai_key=OPENAI_API_KEY, \n    stream=True\n)\n</code></pre> <p>\u73b0\u5728\uff0c\u8ba9\u6211\u4eec\u521b\u5efa\u4e00\u4e2a\u6253\u5370 hello world \u7684\u7b80\u5355\u4ee3\u7406\u3002\u6709\u4e24\u79cd\u65b9\u6cd5\u53ef\u4ee5\u521b\u5efa CustomizeAgent\uff1a</p>"},{"location":"zh/tutorial/first_agent.html#\u65b9\u6cd5-1\u76f4\u63a5\u521d\u59cb\u5316","title":"\u65b9\u6cd5 1\uff1a\u76f4\u63a5\u521d\u59cb\u5316","text":"<p>\u4f60\u53ef\u4ee5\u76f4\u63a5\u4f7f\u7528 <code>CustomizeAgent</code> \u7c7b\u521d\u59cb\u5316\u4ee3\u7406\uff1a <pre><code>first_agent = CustomizeAgent(\n    name=\"FirstAgent\",\n    description=\"A simple agent that prints hello world\",\n    prompt=\"Print 'hello world'\", \n    llm_config=openai_config # specify the LLM configuration \n)\n</code></pre></p>"},{"location":"zh/tutorial/first_agent.html#\u65b9\u6cd5-2\u4ece\u5b57\u5178\u521b\u5efa","title":"\u65b9\u6cd5 2\uff1a\u4ece\u5b57\u5178\u521b\u5efa","text":"<p>\u4f60\u4e5f\u53ef\u4ee5\u901a\u8fc7\u5b9a\u4e49\u5b57\u5178\u4e2d\u7684\u914d\u7f6e\u6765\u521b\u5efa\u4ee3\u7406\uff1a</p> <pre><code>agent_data = {\n    \"name\": \"FirstAgent\",\n    \"description\": \"A simple agent that prints hello world\",\n    \"prompt\": \"Print 'hello world'\",\n    \"llm_config\": openai_config\n}\nfirst_agent = CustomizeAgent.from_dict(agent_data) # use .from_dict() to create an agent. \n</code></pre>"},{"location":"zh/tutorial/first_agent.html#\u4f7f\u7528\u4ee3\u7406","title":"\u4f7f\u7528\u4ee3\u7406","text":"<p>\u521b\u5efa\u5b8c\u6210\u540e\uff0c\u4f60\u53ef\u4ee5\u4f7f\u7528\u4ee3\u7406\u6765\u6253\u5370 hello world\u3002</p> <pre><code># Execute the agent without input. The agent will return a Message object containing the results. \nmessage = first_agent()\n\nprint(f\"Response from {first_agent.name}:\")\nprint(message.content.content) # the content of a Message object is a LLMOutputParser object, where the `content` attribute is the raw LLM output. \n</code></pre> <p>\u6709\u5173\u5b8c\u6574\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 CustomizeAgent \u793a\u4f8b\u3002</p> <p>CustomizeAgent \u8fd8\u63d0\u4f9b\u5176\u4ed6\u529f\u80fd\uff0c\u5305\u62ec\u7ed3\u6784\u5316\u8f93\u5165/\u8f93\u51fa\u548c\u591a\u79cd\u89e3\u6790\u7b56\u7565\u3002\u6709\u5173\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 CustomizeAgent \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/first_agent.html#2-\u521b\u5efa\u5177\u6709\u591a\u4e2a\u52a8\u4f5c\u7684\u4ee3\u7406","title":"2. \u521b\u5efa\u5177\u6709\u591a\u4e2a\u52a8\u4f5c\u7684\u4ee3\u7406","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u4f60\u53ef\u4ee5\u521b\u5efa\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u9884\u5b9a\u4e49\u52a8\u4f5c\u7684\u4ee3\u7406\u3002\u8fd9\u5141\u8bb8\u4f60\u6784\u5efa\u53ef\u4ee5\u6267\u884c\u591a\u4e2a\u4efb\u52a1\u7684\u66f4\u590d\u6742\u7684\u4ee3\u7406\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u793a\u4f8b\uff0c\u5c55\u793a\u5982\u4f55\u521b\u5efa\u4e00\u4e2a\u5177\u6709 <code>TestCodeGeneration</code> \u548c <code>TestCodeReview</code> \u52a8\u4f5c\u7684\u4ee3\u7406\uff1a</p>"},{"location":"zh/tutorial/first_agent.html#\u5b9a\u4e49\u52a8\u4f5c","title":"\u5b9a\u4e49\u52a8\u4f5c","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u5b9a\u4e49\u52a8\u4f5c\uff0c\u5b83\u4eec\u662f <code>Action</code> \u7684\u5b50\u7c7b\uff1a <pre><code>from evoagentx.agents import Agent\nfrom evoagentx.actions import Action, ActionInput, ActionOutput\n\n# Define the CodeGeneration action inputs\nclass TestCodeGenerationInput(ActionInput):\n    requirement: str = Field(description=\"The requirement for the code generation\")\n\n# Define the CodeGeneration action outputs\nclass TestCodeGenerationOutput(ActionOutput):\n    code: str = Field(description=\"The generated code\")\n\n# Define the CodeGeneration action\nclass TestCodeGeneration(Action): \n\n    def __init__(\n        self, \n        name: str=\"TestCodeGeneration\", \n        description: str=\"Generate code based on requirements\", \n        prompt: str=\"Generate code based on requirements: {requirement}\",\n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeGenerationInput\n        outputs_format = outputs_format or TestCodeGenerationOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeGenerationOutput:\n        action_input_attrs = self.inputs_format.get_attrs() # obtain the attributes of the action input \n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data) # format the prompt with the action input data \n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg, \n            parser=self.outputs_format, \n            parse_mode=\"str\" # specify how to parse the output \n        )\n        if return_prompt:\n            return output, prompt\n        return output\n\n\n# Define the CodeReview action inputs\nclass TestCodeReviewInput(ActionInput):\n    code: str = Field(description=\"The code to be reviewed\")\n    requirements: str = Field(description=\"The requirements for the code review\")\n\n# Define the CodeReview action outputs\nclass TestCodeReviewOutput(ActionOutput):\n    review: str = Field(description=\"The review of the code\")\n\n# Define the CodeReview action\nclass TestCodeReview(Action):\n    def __init__(\n        self, \n        name: str=\"TestCodeReview\", \n        description: str=\"Review the code based on requirements\", \n        prompt: str=\"Review the following code based on the requirements:\\n\\nRequirements: {requirements}\\n\\nCode:\\n{code}.\\n\\nYou should output a JSON object with the following format:\\n```json\\n{{\\n'review': '...'\\n}}\\n```\", \n        inputs_format: ActionInput=None, \n        outputs_format: ActionOutput=None, \n        **kwargs\n    ):\n        inputs_format = inputs_format or TestCodeReviewInput\n        outputs_format = outputs_format or TestCodeReviewOutput\n        super().__init__(\n            name=name, \n            description=description, \n            prompt=prompt, \n            inputs_format=inputs_format, \n            outputs_format=outputs_format, \n            **kwargs\n        )\n\n    def execute(self, llm: Optional[BaseLLM] = None, inputs: Optional[dict] = None, sys_msg: Optional[str]=None, return_prompt: bool = False, **kwargs) -&gt; TestCodeReviewOutput:\n        action_input_attrs = self.inputs_format.get_attrs()\n        action_input_data = {attr: inputs.get(attr, \"undefined\") for attr in action_input_attrs}\n        prompt = self.prompt.format(**action_input_data)\n        output = llm.generate(\n            prompt=prompt, \n            system_message=sys_msg,\n            parser=self.outputs_format, \n            parse_mode=\"json\" # specify how to parse the output \n        ) \n        if return_prompt:\n            return output, prompt\n        return output\n</code></pre></p> <p>\u4ece\u4e0a\u9762\u7684\u793a\u4f8b\u4e2d\uff0c\u6211\u4eec\u53ef\u4ee5\u770b\u5230\uff0c\u4e3a\u4e86\u5b9a\u4e49\u4e00\u4e2a\u52a8\u4f5c\uff0c\u6211\u4eec\u9700\u8981\uff1a</p> <ol> <li>\u4f7f\u7528 <code>ActionInput</code> \u548c <code>ActionOutput</code> \u7c7b\u5b9a\u4e49\u52a8\u4f5c\u7684\u8f93\u5165\u548c\u8f93\u51fa</li> <li>\u521b\u5efa\u4e00\u4e2a\u7ee7\u627f\u81ea <code>Action</code> \u7684\u52a8\u4f5c\u7c7b</li> <li>\u5b9e\u73b0 <code>execute</code> \u65b9\u6cd5\uff0c\u8be5\u65b9\u6cd5\u4f7f\u7528\u52a8\u4f5c\u8f93\u5165\u6570\u636e\u683c\u5f0f\u5316\u63d0\u793a\uff0c\u5e76\u4f7f\u7528 LLM \u751f\u6210\u8f93\u51fa\uff0c\u5e76\u901a\u8fc7 <code>parse_mode</code> \u6307\u5b9a\u5982\u4f55\u89e3\u6790\u8f93\u51fa\u3002</li> </ol>"},{"location":"zh/tutorial/first_agent.html#\u5b9a\u4e49\u4ee3\u7406","title":"\u5b9a\u4e49\u4ee3\u7406","text":"<p>\u4e00\u65e6\u6211\u4eec\u5b9a\u4e49\u4e86\u52a8\u4f5c\uff0c\u6211\u4eec\u5c31\u53ef\u4ee5\u901a\u8fc7\u5c06\u52a8\u4f5c\u6dfb\u52a0\u5230\u4ee3\u7406\u4e2d\u6765\u521b\u5efa\u4ee3\u7406\uff1a</p> <pre><code># Initialize the LLM\nopenai_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=os.getenv(\"OPENAI_API_KEY\"))\n\n# Define the agent \ndeveloper = Agent(\n    name=\"Developer\", \n    description=\"A developer who can write code and review code\",\n    actions=[TestCodeGeneration(), TestCodeReview()], \n    llm_config=openai_config\n)\n</code></pre>"},{"location":"zh/tutorial/first_agent.html#\u6267\u884c\u4e0d\u540c\u7684\u52a8\u4f5c","title":"\u6267\u884c\u4e0d\u540c\u7684\u52a8\u4f5c","text":"<p>\u4e00\u65e6\u4f60\u521b\u5efa\u4e86\u4e00\u4e2a\u5177\u6709\u591a\u4e2a\u52a8\u4f5c\u7684\u4ee3\u7406\uff0c\u4f60\u53ef\u4ee5\u6267\u884c\u7279\u5b9a\u7684\u52a8\u4f5c\uff1a</p> <pre><code># List all available actions on the agent\nactions = developer.get_all_actions()\nprint(f\"Available actions of agent {developer.name}:\")\nfor action in actions:\n    print(f\"- {action.name}: {action.description}\")\n\n# Generate some code using the CodeGeneration action\ngeneration_result = developer.execute(\n    action_name=\"TestCodeGeneration\", # specify the action name\n    action_input_data={ \n        \"requirement\": \"Write a function that returns the sum of two numbers\"\n    }\n)\n\n# Access the generated code\ngenerated_code = generation_result.content.code\nprint(\"Generated code:\")\nprint(generated_code)\n\n# Review the generated code using the CodeReview action\nreview_result = developer.execute(\n    action_name=\"TestCodeReview\",\n    action_input_data={\n        \"requirements\": \"Write a function that returns the sum of two numbers\",\n        \"code\": generated_code\n    }\n)\n\n# Access the review results\nreview = review_result.content.review\nprint(\"\\nReview:\")\nprint(review)\n</code></pre> <p>\u8fd9\u4e2a\u793a\u4f8b\u6f14\u793a\u4e86\u5982\u4f55\uff1a 1. \u5217\u51fa\u4ee3\u7406\u4e0a\u53ef\u7528\u7684\u6240\u6709\u52a8\u4f5c 2. \u4f7f\u7528 TestCodeGeneration \u52a8\u4f5c\u751f\u6210\u4ee3\u7801 3. \u4f7f\u7528 TestCodeReview \u52a8\u4f5c\u5ba1\u67e5\u751f\u6210\u7684\u4ee3\u7801 4. \u8bbf\u95ee\u6bcf\u4e2a\u52a8\u4f5c\u6267\u884c\u7684\u7ed3\u679c</p> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 Agent \u793a\u4f8b\u3002 </p>"},{"location":"zh/tutorial/first_agent.html#3-\u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","title":"3. \u4fdd\u5b58\u548c\u52a0\u8f7d\u4ee3\u7406","text":"<p>\u4f60\u53ef\u4ee5\u5c06\u4ee3\u7406\u4fdd\u5b58\u5230\u6587\u4ef6\u5e76\u5728\u7a0d\u540e\u52a0\u8f7d\u5b83\uff1a</p> <pre><code># \u4fdd\u5b58\u4ee3\u7406\ndeveloper.save_module(\"examples/output/developer.json\")\n\n# \u52a0\u8f7d\u4ee3\u7406\nloaded_developer = Agent.load_module(\"examples/output/developer.json\", llm_config=openai_config)\n</code></pre>"},{"location":"zh/tutorial/first_workflow.html","title":"\u6784\u5efa\u4f60\u7684\u7b2c\u4e00\u4e2a\u5de5\u4f5c\u6d41","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c\u5de5\u4f5c\u6d41\u5141\u8bb8\u591a\u4e2a\u4ee3\u7406\u6309\u987a\u5e8f\u534f\u4f5c\u5b8c\u6210\u590d\u6742\u4efb\u52a1\u3002\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u521b\u5efa\u548c\u4f7f\u7528\u5de5\u4f5c\u6d41\uff1a</p> <ol> <li>\u7406\u89e3\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a\u5b66\u4e60\u5de5\u4f5c\u6d41\u5982\u4f55\u5c06\u591a\u4e2a\u4efb\u52a1\u8fde\u63a5\u5728\u4e00\u8d77</li> <li>\u6784\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a\u521b\u5efa\u4e00\u4e2a\u5305\u542b\u89c4\u5212\u548c\u7f16\u7801\u6b65\u9aa4\u7684\u5de5\u4f5c\u6d41</li> <li>\u6267\u884c\u548c\u7ba1\u7406\u5de5\u4f5c\u6d41\uff1a\u4f7f\u7528\u7279\u5b9a\u8f93\u5165\u8fd0\u884c\u5de5\u4f5c\u6d41</li> </ol> <p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u80fd\u591f\u521b\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u534f\u8c03\u591a\u4e2a\u4ee3\u7406\u6765\u89e3\u51b3\u590d\u6742\u95ee\u9898\u3002</p>"},{"location":"zh/tutorial/first_workflow.html#1-\u7406\u89e3\u987a\u5e8f\u5de5\u4f5c\u6d41","title":"1. \u7406\u89e3\u987a\u5e8f\u5de5\u4f5c\u6d41","text":"<p>EvoAgentX \u4e2d\u7684\u5de5\u4f5c\u6d41\u4ee3\u8868\u4e00\u7cfb\u5217\u53ef\u4ee5\u7531\u4e0d\u540c\u4ee3\u7406\u6267\u884c\u7684\u4efb\u52a1\u3002\u6700\u7b80\u5355\u7684\u5de5\u4f5c\u6d41\u662f\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u5176\u4e2d\u4efb\u52a1\u4e00\u4e2a\u63a5\u4e00\u4e2a\u5730\u6267\u884c\uff0c\u524d\u4e00\u4e2a\u4efb\u52a1\u7684\u8f93\u51fa\u4f5c\u4e3a\u540e\u7eed\u4efb\u52a1\u7684\u8f93\u5165\u3002</p> <p>\u8ba9\u6211\u4eec\u4ece\u5bfc\u5165\u5fc5\u8981\u7684\u7ec4\u4ef6\u5f00\u59cb\uff1a</p> <pre><code>import os \nfrom dotenv import load_dotenv\nfrom evoagentx.workflow import SequentialWorkFlowGraph, WorkFlow\nfrom evoagentx.agents import AgentManager\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\n\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n</code></pre>"},{"location":"zh/tutorial/first_workflow.html#2-\u6784\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41","title":"2. \u6784\u5efa\u987a\u5e8f\u5de5\u4f5c\u6d41","text":"<p>\u987a\u5e8f\u5de5\u4f5c\u6d41\u7531\u4e00\u7cfb\u5217\u4efb\u52a1\u7ec4\u6210\uff0c\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6709\uff1a</p> <ul> <li>\u540d\u79f0\u548c\u63cf\u8ff0</li> <li>\u8f93\u5165\u548c\u8f93\u51fa\u5b9a\u4e49</li> <li>\u63d0\u793a\u6a21\u677f</li> <li>\u89e3\u6790\u6a21\u5f0f\u548c\u51fd\u6570\uff08\u53ef\u9009\uff09</li> </ul> <p>\u4ee5\u4e0b\u662f\u5982\u4f55\u6784\u5efa\u4e00\u4e2a\u5305\u542b\u89c4\u5212\u548c\u7f16\u7801\u4efb\u52a1\u7684\u987a\u5e8f\u5de5\u4f5c\u6d41\uff1a</p> <pre><code># Configure the LLM \nllm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY, stream=True, output_response=True)\nllm = OpenAILLM(llm_config)\n\n# Define a custom parsing function (if needed)\nfrom evoagentx.core.registry import register_parse_function\nfrom evoagentx.core.module_utils import extract_code_blocks\n\n# [optional] Define a custom parsing function (if needed)\n# It is suggested to use the `@register_parse_function` decorator to register a custom parsing function, so the workflow can be saved and loaded correctly.  \n\n@register_parse_function\ndef custom_parse_func(content: str) -&gt; str:\n    return {\"code\": extract_code_blocks(content)[0]}\n\n# Define sequential tasks\ntasks = [\n    {\n        \"name\": \"Planning\",\n        \"description\": \"Create a detailed plan for code generation\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed plan with steps, components, and architecture\"}\n        ],\n        \"prompt\": \"You are a software architect. Your task is to create a detailed implementation plan for the given problem.\\n\\nProblem: {problem}\\n\\nPlease provide a comprehensive implementation plan including:\\n1. Problem breakdown\\n2. Algorithm or approach selection\\n3. Implementation steps\\n4. Potential edge cases and solutions\",\n        \"parse_mode\": \"str\",\n        # \"llm_config\": specific_llm_config # if you want to use a specific LLM for a task, you can add a key `llm_config` in the task dict.  \n    },\n    {\n        \"name\": \"Coding\",\n        \"description\": \"Implement the code based on the implementation plan\",\n        \"inputs\": [\n            {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"Description of the problem to be solved\"},\n            {\"name\": \"plan\", \"type\": \"str\", \"required\": True, \"description\": \"Detailed implementation plan from the Planning phase\"},\n        ],\n        \"outputs\": [\n            {\"name\": \"code\", \"type\": \"str\", \"required\": True, \"description\": \"Implemented code with explanations\"}\n        ],\n        \"prompt\": \"You are a software developer. Your task is to implement the code based on the provided problem and implementation plan.\\n\\nProblem: {problem}\\nImplementation Plan: {plan}\\n\\nPlease provide the implementation code with appropriate comments.\",\n        \"parse_mode\": \"custom\",\n        \"parse_func\": custom_parse_func\n    }\n]\n\n# Create the sequential workflow graph\ngraph = SequentialWorkFlowGraph(\n    goal=\"Generate code to solve programming problems\",\n    tasks=tasks\n)\n</code></pre> <p>Note</p> <p>\u5f53\u4f60\u4f7f\u7528\u4efb\u52a1\u5217\u8868\u521b\u5efa <code>SequentialWorkFlowGraph</code> \u65f6\uff0c\u6846\u67b6\u4f1a\u4e3a\u6bcf\u4e2a\u4efb\u52a1\u521b\u5efa\u4e00\u4e2a <code>CustomizeAgent</code>\u3002\u5de5\u4f5c\u6d41\u4e2d\u7684\u6bcf\u4e2a\u4efb\u52a1\u90fd\u6210\u4e3a\u4e00\u4e2a\u4e13\u95e8\u7684\u4ee3\u7406\uff0c\u914d\u7f6e\u6709\u4f60\u5b9a\u4e49\u7684\u7279\u5b9a\u63d0\u793a\u3001\u8f93\u5165/\u8f93\u51fa\u683c\u5f0f\u548c\u89e3\u6790\u6a21\u5f0f\u3002\u8fd9\u4e9b\u4ee3\u7406\u6309\u987a\u5e8f\u8fde\u63a5\uff0c\u4e00\u4e2a\u4ee3\u7406\u7684\u8f93\u51fa\u6210\u4e3a\u4e0b\u4e00\u4e2a\u4ee3\u7406\u7684\u8f93\u5165\u3002</p> <p><code>parse_mode</code> \u63a7\u5236\u5982\u4f55\u5c06 LLM \u7684\u8f93\u51fa\u89e3\u6790\u4e3a\u7ed3\u6784\u5316\u683c\u5f0f\u3002\u53ef\u7528\u9009\u9879\u6709\uff1a[<code>'str'</code>\uff08\u9ed8\u8ba4\uff09\u3001<code>'json'</code>\u3001<code>'title'</code>\u3001<code>'xml'</code>\u3001<code>'custom'</code>]\u3002\u6709\u5173\u89e3\u6790\u6a21\u5f0f\u548c\u793a\u4f8b\u7684\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605 CustomizeAgent \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/first_workflow.html#3-\u6267\u884c\u548c\u7ba1\u7406\u5de5\u4f5c\u6d41","title":"3. \u6267\u884c\u548c\u7ba1\u7406\u5de5\u4f5c\u6d41","text":"<p>\u4e00\u65e6\u4f60\u521b\u5efa\u4e86\u5de5\u4f5c\u6d41\u56fe\uff0c\u4f60\u5c31\u53ef\u4ee5\u521b\u5efa\u5de5\u4f5c\u6d41\u5b9e\u4f8b\u5e76\u6267\u884c\u5b83\uff1a</p> <pre><code># Create agent manager and add agents from the workflow. It will create a `CustomizeAgent` for each task in the workflow. \nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(\n    graph, \n    llm_config=llm_config  # This config will be used for all tasks without `llm_config`. \n)\n\n# Create workflow instance\nworkflow = WorkFlow(graph=graph, agent_manager=agent_manager, llm=llm)\n\n# Execute the workflow with inputs\noutput = workflow.execute(\n    inputs = {\n        \"problem\": \"Write a function to find the longest palindromic substring in a given string.\"\n    }\n)\n\nprint(\"Workflow completed!\")\nprint(\"Workflow output:\\n\", output)\n</code></pre> <p>\u4f60\u5e94\u8be5\u5728 <code>execute</code> \u65b9\u6cd5\u7684 <code>inputs</code> \u53c2\u6570\u4e2d\u6307\u5b9a\u5de5\u4f5c\u6d41\u6240\u9700\u7684\u6240\u6709\u8f93\u5165\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u8003 \u987a\u5e8f\u5de5\u4f5c\u6d41\u793a\u4f8b\u3002</p>"},{"location":"zh/tutorial/first_workflow.html#4-\u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","title":"4. \u4fdd\u5b58\u548c\u52a0\u8f7d\u5de5\u4f5c\u6d41","text":"<p>\u4f60\u53ef\u4ee5\u4fdd\u5b58\u5de5\u4f5c\u6d41\u56fe\u4ee5\u4f9b\u5c06\u6765\u4f7f\u7528\uff1a</p> <pre><code># Save the workflow graph to a file\ngraph.save_module(\"examples/output/saved_sequential_workflow.json\")\n\n# Load the workflow graph from a file\nloaded_graph = SequentialWorkFlowGraph.from_file(\"examples/output/saved_sequential_workflow.json\")\n\n# Create a new workflow with the loaded graph\nnew_workflow = WorkFlow(graph=loaded_graph, agent_manager=agent_manager, llm=llm)\n</code></pre> <p>\u6709\u5173\u66f4\u590d\u6742\u7684\u5de5\u4f5c\u6d41\u6216\u4e0d\u540c\u7c7b\u578b\u7684\u5de5\u4f5c\u6d41\u56fe\uff0c\u8bf7\u53c2\u9605 \u5de5\u4f5c\u6d41\u56fe \u6587\u6863\u548c \u52a8\u4f5c\u56fe \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/rag.html","title":"\u4f7f\u7528 RAGEngine \u6784\u5efa\u60a8\u7684\u7b2c\u4e00\u4e2a RAG \u7cfb\u7edf","text":"<p>\u5728 EvoAgentX \u4e2d\uff0c<code>RAGEngine</code> \u662f\u4e00\u4e2a\u5f3a\u5927\u7684\u5de5\u5177\uff0c\u7528\u4e8e\u6784\u5efa\u68c0\u7d22\u589e\u5f3a\u751f\u6210\uff08RAG\uff09\u7cfb\u7edf\u3002\u5b83\u5141\u8bb8\u60a8\u52a0\u8f7d\u6587\u6863\u3001\u521b\u5efa\u53ef\u641c\u7d22\u7684\u7d22\u5f15\uff0c\u5e76\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\u6765\u56de\u7b54\u95ee\u9898\u3002\u672c\u6559\u7a0b\u4e13\u4e3a\u521d\u5b66\u8005\u8bbe\u8ba1\uff0c\u5c06\u6307\u5bfc\u60a8\u5b8c\u6210\u4f7f\u7528 <code>RAGEngine</code> \u521b\u5efa\u548c\u4f7f\u7528 RAG \u7cfb\u7edf\u7684\u57fa\u672c\u6b65\u9aa4\uff1a</p> <ol> <li>\u8bbe\u7f6e RAGEngine\uff1a\u5b66\u4e60\u5982\u4f55\u914d\u7f6e\u548c\u521d\u59cb\u5316\u5f15\u64ce\u3002</li> <li>\u7d22\u5f15\u548c\u67e5\u8be2\u6587\u6863\uff1a\u52a0\u8f7d\u4e00\u4e2a\u7b80\u5355\u6570\u636e\u96c6\u5e76\u67e5\u8be2\u4ee5\u627e\u5230\u7b54\u6848\u3002</li> <li>\u4fdd\u5b58\u548c\u52a0\u8f7d\u7d22\u5f15\uff1a\u4fdd\u5b58\u60a8\u7684\u7d22\u5f15\u6570\u636e\u5e76\u5728\u4ee5\u540e\u91cd\u590d\u4f7f\u7528\u3002</li> </ol> <p>\u5b8c\u6210\u672c\u6559\u7a0b\u540e\uff0c\u60a8\u5c06\u80fd\u591f\u8bbe\u7f6e\u4e00\u4e2a\u57fa\u7840\u7684 RAG \u7cfb\u7edf\uff0c\u7d22\u5f15\u6587\u6863\uff0c\u67e5\u8be2\u5b83\u4eec\uff0c\u5e76\u6301\u4e45\u5316\u60a8\u7684\u5de5\u4f5c\u4ee5\u4f9b\u672a\u6765\u4f7f\u7528\u3002\u6211\u4eec\u5c06\u4f7f\u7528 HotPotQA \u6570\u636e\u96c6\u7684\u4e00\u4e2a\u6837\u672c\u6765\u8fdb\u884c\u6d4b\u8bd5\u3002</p>"},{"location":"zh/tutorial/rag.html#1-\u8bbe\u7f6e-ragengine","title":"1. \u8bbe\u7f6e RAGEngine","text":"<p>\u7b2c\u4e00\u6b65\u662f\u8bbe\u7f6e\u60a8\u7684\u73af\u5883\u5e76\u521d\u59cb\u5316 <code>RAGEngine</code>\u3002\u8fd9\u5305\u62ec\u5b89\u88c5\u4f9d\u8d56\u9879\u3001\u914d\u7f6e\u5b58\u50a8\u540e\u7aef\u548c\u8bbe\u7f6e\u5d4c\u5165\u6a21\u578b\u3002</p>"},{"location":"zh/tutorial/rag.html#\u5b89\u88c5\u4f9d\u8d56\u9879","title":"\u5b89\u88c5\u4f9d\u8d56\u9879","text":"<p>\u786e\u4fdd\u60a8\u5df2\u5b89\u88c5 EvoAgentX\u3002\u60a8\u8fd8\u9700\u8981\u4e00\u4e2a OpenAI API \u5bc6\u94a5\u7528\u4e8e\u5d4c\u5165\u6587\u672c\u3002</p> <p>\u5728\u60a8\u7684\u9879\u76ee\u76ee\u5f55\u4e2d\u521b\u5efa\u4e00\u4e2a <code>.env</code> \u6587\u4ef6\uff0c\u5e76\u6dfb\u52a0\u60a8\u7684 OpenAI API \u5bc6\u94a5\uff1a</p> <pre><code>OPENAI_API_KEY=\u60a8\u7684-openai-api-\u5bc6\u94a5\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u914d\u7f6e\u73af\u5883","title":"\u914d\u7f6e\u73af\u5883","text":"<p>\u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e2a Python \u811a\u672c\u6765\u8bbe\u7f6e <code>RAGEngine</code>\u3002\u6211\u4eec\u5c06\u4f7f\u7528 SQLite \u5b58\u50a8\u5143\u6570\u636e\uff0cFAISS \u5b58\u50a8\u5411\u91cf\u5d4c\u5165\u3002</p> <pre><code>import os\nfrom dotenv import load_dotenv\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u914d\u7f6e\u5b58\u50a8\uff08SQLite \u7528\u4e8e\u5143\u6570\u636e\uff0cFAISS \u7528\u4e8e\u5411\u91cf\uff09\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# \u914d\u7f6e RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=50),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=3, similarity_cutoff=0.3)\n)\n\n# \u521d\u59cb\u5316 RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\nprint(\"RAGEngine \u5df2\u51c6\u5907\u5c31\u7eea\uff01\")\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u4ee3\u7801\u89e3\u6790","title":"\u4ee3\u7801\u89e3\u6790","text":"<ul> <li>\u73af\u5883\u8bbe\u7f6e\uff1a\u6211\u4eec\u4ece <code>.env</code> \u6587\u4ef6\u4e2d\u52a0\u8f7d OpenAI API \u5bc6\u94a5\u3002</li> <li>\u5b58\u50a8\u914d\u7f6e\uff1a\u8bbe\u7f6e SQLite \u5b58\u50a8\u5143\u6570\u636e\uff0cFAISS \u5b58\u50a8\u5411\u91cf\u5d4c\u5165\u3002<code>dimensions=1536</code> \u4e0e OpenAI \u5d4c\u5165\u6a21\u578b\u5339\u914d\u3002</li> <li>RAG \u914d\u7f6e\uff1a\u914d\u7f6e RAG \u6d41\u7a0b\uff1a</li> <li><code>ReaderConfig</code>\uff1a\u8bfb\u53d6\u6587\u4ef6\uff08\u7a0d\u540e\u6211\u4eec\u5c06\u4f7f\u7528 JSON \u6587\u4ef6\uff09\u3002</li> <li><code>ChunkerConfig</code>\uff1a\u5c06\u6587\u6863\u5206\u5272\u6210 512 \u5b57\u7b26\u7684\u5206\u5757\uff0c50 \u5b57\u7b26\u91cd\u53e0\u3002</li> <li><code>EmbeddingConfig</code>\uff1a\u4f7f\u7528 OpenAI \u7684 <code>text-embedding-ada-002</code> \u751f\u6210\u5d4c\u5165\u3002</li> <li><code>IndexConfig</code>\uff1a\u521b\u5efa\u5411\u91cf\u7d22\u5f15\u3002</li> <li><code>RetrievalConfig</code>\uff1a\u68c0\u7d22\u76f8\u4f3c\u5ea6\u5f97\u5206\u9ad8\u4e8e 0.3 \u7684\u524d 3 \u4e2a\u5206\u5757\u3002</li> <li>\u521d\u59cb\u5316\uff1a\u521b\u5efa <code>RAGEngine</code> \u5b9e\u4f8b\uff0c\u51c6\u5907\u5904\u7406\u6587\u6863\u3002</li> </ul> <p>\u6709\u5173\u914d\u7f6e\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RAGEngine \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/rag.html#2-\u7d22\u5f15\u548c\u67e5\u8be2\u6587\u6863","title":"2. \u7d22\u5f15\u548c\u67e5\u8be2\u6587\u6863","text":"<p>\u73b0\u5728\uff0c\u6211\u4eec\u5c06\u7d22\u5f15 HotPotQA \u6570\u636e\u96c6\u4e2d\u7684\u4e00\u4e2a\u6587\u6863\u5e76\u67e5\u8be2\u5b83\u3002\u6211\u4eec\u5c06\u4f7f\u7528\u60a8\u63d0\u4f9b\u7684\u7b2c\u4e00\u4e2a HotPotQA \u793a\u4f8b\uff0c\u5176\u4e2d\u5305\u542b\u5173\u4e8e Scott Derrickson \u548c Ed Wood \u7684\u4fe1\u606f\uff0c\u5e76\u56de\u7b54\u95ee\u9898\uff1a\u201cScott Derrickson \u548c Ed Wood \u662f\u540c\u4e00\u4e2a\u56fd\u7c4d\u5417\uff1f\u201d</p>"},{"location":"zh/tutorial/rag.html#\u51c6\u5907\u6570\u636e\u96c6","title":"\u51c6\u5907\u6570\u636e\u96c6","text":"<p>\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a <code>data</code> \u7684\u76ee\u5f55\uff0c\u5e76\u5c06 HotPotQA \u793a\u4f8b\u4fdd\u5b58\u4e3a <code>hotpotqa_sample.json</code>\uff1a</p> <pre><code>{\n  \"_id\": \"5a8b57f25542995d1e6f1371\",\n  \"answer\": \"yes\",\n  \"question\": \"Were Scott Derrickson and Ed Wood of the same nationality?\",\n  \"supporting_facts\": [\n    [\n      \"Scott Derrickson\",\n      0\n    ],\n    [\n      \"Ed Wood\",\n      0\n    ]\n  ],\n  \"context\": [\n    [\n      \"Ed Wood (film)\",\n      [\n        \"Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.\",\n        \" The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau.\",\n        \" Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\"\n      ]\n    ],\n    [\n      \"Scott Derrickson\",\n      [\n        \"Scott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\",\n        \" He lives in Los Angeles, California.\",\n        \" He is best known for directing horror films such as \\\"Sinister\\\", \\\"The Exorcism of Emily Rose\\\", and \\\"Deliver Us From Evil\\\", as well as the 2016 Marvel Cinematic Universe installment, \\\"Doctor Strange.\\\"\"\n      ]\n    ],\n    [\n      \"Woodson, Arkansas\",\n      [\n        \"Woodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.\",\n        \" Its population was 403 at the 2010 census.\",\n        \" It is part of the Little Rock\\u2013North Little Rock\\u2013Conway Metropolitan Statistical Area.\",\n        \" Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.\",\n        \" Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.\"\n      ]\n    ],\n    [\n      \"Tyler Bates\",\n      [\n        \"Tyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.\",\n        \" Much of his work is in the action and horror film genres, with films like \\\"Dawn of the Dead, 300, Sucker Punch,\\\" and \\\"John Wick.\\\"\",\n        \" He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.\",\n        \" With Gunn, he has scored every one of the director's films; including \\\"Guardians of the Galaxy\\\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.\",\n        \" In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \\\"The Pale Emperor\\\" and \\\"Heaven Upside Down\\\".\"\n      ]\n    ],\n    [\n      \"Ed Wood\",\n      [\n        \"Edward Davis Wood Jr. (October 10, 1924 \\u2013 December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\"\n      ]\n    ],\n    [\n      \"Deliver Us from Evil (2014 film)\",\n      [\n        \"Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.\",\n        \" The film is officially based on a 2001 non-fiction book entitled \\\"Beware the Night\\\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \\\"inspired by actual accounts\\\".\",\n        \" The film stars Eric Bana, \\u00c9dgar Ram\\u00edrez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.\"\n      ]\n    ],\n    [\n      \"Adam Collis\",\n      [\n        \"Adam Collis is an American filmmaker and actor.\",\n        \" He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.\",\n        \" He also studied cinema at the University of Southern California from 1991 to 1997.\",\n        \" Collis first work was the assistant director for the Scott Derrickson's short \\\"Love in the Ruins\\\" (1995).\",\n        \" In 1998, he played \\\"Crankshaft\\\" in Eric Koyanagi's \\\"Hundred Percent\\\".\"\n      ]\n    ],\n    [\n      \"Sinister (film)\",\n      [\n        \"Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.\",\n        \" It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.\"\n      ]\n    ],\n    [\n      \"Conrad Brooks\",\n      [\n        \"Conrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.\",\n        \" He moved to Hollywood, California in 1948 to pursue a career in acting.\",\n        \" He got his start in movies appearing in Ed Wood films such as \\\"Plan 9 from Outer Space\\\", \\\"Glen or Glenda\\\", and \\\"Jail Bait.\\\"\",\n        \" He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.\",\n        \" He also has since gone on to write, produce and direct several films.\"\n      ]\n    ],\n    [\n      \"Doctor Strange (2016 film)\",\n      [\n        \"Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.\",\n        \" It is the fourteenth film of the Marvel Cinematic Universe (MCU).\",\n        \" The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.\",\n        \" In \\\"Doctor Strange\\\", surgeon Strange learns the mystic arts after a career-ending car accident.\"\n      ]\n    ]\n  ],\n  \"type\": \"comparison\",\n  \"level\": \"hard\"\n}\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u7d22\u5f15\u6587\u6863","title":"\u7d22\u5f15\u6587\u6863","text":"<p>\u8ba9\u6211\u4eec\u7f16\u5199\u4e00\u4e2a\u811a\u672c\u52a0\u8f7d JSON \u6587\u4ef6\uff0c\u7d22\u5f15\u5176\u5185\u5bb9\u5e76\u67e5\u8be2\u5b83\u3002\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a <code>rag_tutorial.py</code> \u7684\u6587\u4ef6\uff1a</p> <pre><code>import os\nimport json\n\nfrom dotenv import load_dotenv\n\nfrom evoagentx.rag.schema import Query\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u914d\u7f6e\u5b58\u50a8\uff08SQLite \u7528\u4e8e\u5143\u6570\u636e\uff0cFAISS \u7528\u4e8e\u5411\u91cf\uff09\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# \u914d\u7f6e RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=50),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=3, similarity_cutoff=0.3)\n)\n\n# \u521d\u59cb\u5316 RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\nprint(\"RAGEngine \u5df2\u51c6\u5907\u5c31\u7eea\uff01\")\n\n\n# \u6b65\u9aa4 1\uff1a\u52a0\u8f7d\u5e76\u7d22\u5f15 HotPotQA \u793a\u4f8b\nwith open(\"./data/hotpotqa_sample.json\", \"r\", encoding=\"utf-8\") as f:\n    hotpotqa_data = json.load(f)\n\n# \u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u6587\u672c\u6587\u4ef6\uff0c\u5305\u542b\u4e0a\u4e0b\u6587\u7528\u4e8e\u7d22\u5f15\ncontext = hotpotqa_data[\"context\"]\nwith open(\"./data/hotpotqa_context.txt\", \"w\", encoding=\"utf-8\") as f:\n    for title, sentences in context:\n        f.write(f\"# {title}\")\n        for sentence in sentences:\n            f.write(f\"{sentence}\\n\")\n        f.write(f\"\\n\\n\")\n\n# \u7d22\u5f15\u6587\u672c\u6587\u4ef6\ncorpus = rag_engine.read(\n    file_paths=\"./data/hotpotqa_context.txt\",\n    filter_file_by_suffix=[\".txt\"],\n    merge_by_file=True,\n    show_progress=True,\n    corpus_id=\"hotpotqa_corpus\"\n)\nrag_engine.add(index_type=\"vector\", nodes=corpus, corpus_id=\"hotpotqa_corpus\")\n\nprint(\"\u6587\u6863\u7d22\u5f15\u6210\u529f\uff01\")\n\n# \u6b65\u9aa4 2\uff1a\u67e5\u8be2\u7d22\u5f15\nquery = Query(query_str=\"Were Scott Derrickson and Ed Wood of the same nationality?\", top_k=3)\nresult = rag_engine.query(query, corpus_id=\"hotpotqa_corpus\")\n\n# \u6253\u5370\u68c0\u7d22\u5230\u7684\u5206\u5757\nprint(\"\\n\u68c0\u7d22\u5230\u7684\u7b54\u6848\uff1a\")\nfor i, chunk in enumerate(result.corpus.chunks, 1):\n    print(f\"{i}. {chunk.text}\")\n\n# \u6e05\u7406\nrag_engine.clear(corpus_id=\"hotpotqa_corpus\")\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u8fd0\u884c\u811a\u672c","title":"\u8fd0\u884c\u811a\u672c","text":"<p>\u8fd0\u884c\u811a\u672c\uff1a</p> <pre><code>python rag_tutorial.py\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u4ee3\u7801\u89e3\u6790_1","title":"\u4ee3\u7801\u89e3\u6790","text":"<ul> <li>\u52a0\u8f7d\u6570\u636e\uff1a\u6211\u4eec\u8bfb\u53d6 HotPotQA JSON \u6587\u4ef6\u5e76\u63d0\u53d6\u5176 <code>context</code> \u5b57\u6bb5\uff0c\u5305\u542b\u5173\u4e8e Scott Derrickson \u548c Ed Wood \u7684\u6587\u672c\u3002</li> <li>\u7d22\u5f15\uff1a</li> <li>\u521b\u5efa\u4e00\u4e2a\u4e34\u65f6\u6587\u672c\u6587\u4ef6\uff08<code>hotpotqa_context.txt</code>\uff09\uff0c\u683c\u5f0f\u5316\u4e0a\u4e0b\u6587\u4ee5\u4fbf\u9605\u8bfb\u3002</li> <li><code>read</code> \u65b9\u6cd5\u5c06\u6587\u672c\u6587\u4ef6\u52a0\u8f7d\u5230 <code>Corpus</code> \u4e2d\uff0c\u6839\u636e\u4e4b\u524d\u8bbe\u7f6e\u7684 512 \u5b57\u7b26\u9650\u5236\u5206\u5272\u6210\u5757\u3002</li> <li><code>add</code> \u65b9\u6cd5\u4f7f\u7528 OpenAI \u5d4c\u5165\u5c06\u5206\u5757\u7d22\u5f15\u5230\u5411\u91cf\u7d22\u5f15\u4e2d\u3002</li> <li>\u67e5\u8be2\uff1a</li> <li>\u521b\u5efa\u4e00\u4e2a <code>Query</code> \u5bf9\u8c61\uff0c\u5305\u542b\u6570\u636e\u96c6\u4e2d\u7684\u95ee\u9898\u3002</li> <li><code>query</code> \u65b9\u6cd5\u68c0\u7d22\u4e0e\u95ee\u9898\u6700\u76f8\u4f3c\u7684 3 \u4e2a\u5206\u5757\u3002</li> <li>\u7ed3\u679c\u5305\u62ec\u8868\u660e\u4e24\u4eba\u90fd\u662f\u7f8e\u56fd\u4eba\u7684\u53e5\u5b50\uff08\u4f8b\u5982\uff0c\u201cScott Derrickson\u2026 \u662f\u7f8e\u56fd\u5bfc\u6f14\u201d\u548c\u201c\u7231\u5fb7\u534e\u00b7\u6234\u7ef4\u65af\u00b7\u4f0d\u5fb7\u4e8c\u4e16\u2026 \u662f\u7f8e\u56fd\u7535\u5f71\u5236\u7247\u4eba\u201d\uff09\u3002</li> <li>\u6e05\u7406\uff1a\u6e05\u9664\u7d22\u5f15\u4ee5\u91ca\u653e\u5185\u5b58\u3002</li> </ul>"},{"location":"zh/tutorial/rag.html#\u9884\u671f\u8f93\u51fa","title":"\u9884\u671f\u8f93\u51fa","text":"<p>\u60a8\u5e94\u8be5\u770b\u5230\u7c7b\u4f3c\u4ee5\u4e0b\u5185\u5bb9\uff1a</p> <pre><code>\u6587\u6863\u7d22\u5f15\u6210\u529f\uff01\n\n\u68c0\u7d22\u5230\u7684\u7b54\u6848\uff1a\n1. # Ed Wood (film)Ed Wood is a 1994 American biographical period comedy-drama film directed and produced by Tim Burton, and starring Johnny Depp as cult filmmaker Ed Wood.\n The film concerns the period in Wood's life when he made his best-known films as well as his relationship with actor Bela Lugosi, played by Martin Landau. \n Sarah Jessica Parker, Patricia Arquette, Jeffrey Jones, Lisa Marie, and Bill Murray are among the supporting cast.\n\n\n# Scott DerricksonScott Derrickson (born July 16, 1966) is an American director, screenwriter and producer.\n He lives in Los Angeles, California.\n He is best known for directing horror films such as \"Sinister\", \"The Exorcism of Emily Rose\", and \"Deliver Us From Evil\", as well as the 2016 Marvel Cinematic Universe installment, \"Doctor Strange.\"\n\n\n# Woodson, ArkansasWoodson is a census-designated place (CDP) in Pulaski County, Arkansas, in the United States.\n Its population was 403 at the 2010 census.\n It is part of the Little Rock\u2013North Little Rock\u2013Conway Metropolitan Statistical Area.\n Woodson and its accompanying Woodson Lake and Wood Hollow are the namesake for Ed Wood Sr., a prominent plantation owner, trader, and businessman at the turn of the 20th century.\n Woodson is adjacent to the Wood Plantation, the largest of the plantations own by Ed Wood Sr.\n\n\n# Tyler BatesTyler Bates (born June 5, 1965) is an American musician, music producer, and composer for films, television, and video games.\n Much of his work is in the action and horror film genres, with films like \"Dawn of the Dead, 300, Sucker Punch,\" and \"John Wick.\"\n He has collaborated with directors like Zack Snyder, Rob Zombie, Neil Marshall, William Friedkin, Scott Derrickson, and James Gunn.\n With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.\n2. With Gunn, he has scored every one of the director's films; including \"Guardians of the Galaxy\", which became one of the highest grossing domestic movies of 2014, and its 2017 sequel.\n In addition, he is also the lead guitarist of the American rock band Marilyn Manson, and produced its albums \"The Pale Emperor\" and \"Heaven Upside Down\".  \n\n\n# Ed WoodEdward Davis Wood Jr. (October 10, 1924 \u2013 December 10, 1978) was an American filmmaker, actor, writer, producer, and director.\n\n\n# Deliver Us from Evil (2014 film)Deliver Us from Evil is a 2014 American supernatural horror film directed by Scott Derrickson and produced by Jerry Bruckheimer.\n The film is officially based on a 2001 non-fiction book entitled \"Beware the Night\" by Ralph Sarchie and Lisa Collier Cool, and its marketing campaign highlighted that it was \"inspired by actual accounts\".\n The film stars Eric Bana, \u00c9dgar Ram\u00edrez, Sean Harris, Olivia Munn, and Joel McHale in the main roles and was released on July 2, 2014.\n\n\n# Adam CollisAdam Collis is an American filmmaker and actor.\n He attended the Duke University from 1986 to 1990 and the University of California, Los Angeles from 2007 to 2010.\n He also studied cinema at the University of Southern California from 1991 to 1997.\n Collis first work was the assistant director for the Scott Derrickson's short \"Love in the Ruins\" (1995).\n In 1998, he played \"Crankshaft\" in Eric Koyanagi's \"Hundred Percent\".\n\n\n# Sinister (film)Sinister is a 2012 supernatural horror film directed by Scott Derrickson and written by Derrickson and C. Robert Cargill.\n It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.\n3. It stars Ethan Hawke as fictional true-crime writer Ellison Oswalt who discovers a box of home movies in his attic that puts his family in danger.       \n\n\n# Conrad BrooksConrad Brooks (born Conrad Biedrzycki on January 3, 1931 in Baltimore, Maryland) is an American actor.\n He moved to Hollywood, California in 1948 to pursue a career in acting.\n He got his start in movies appearing in Ed Wood films such as \"Plan 9 from Outer Space\", \"Glen or Glenda\", and \"Jail Bait.\"\n He took a break from acting during the 1960s and 1970s but due to the ongoing interest in the films of Ed Wood, he reemerged in the 1980s and has become a prolific actor.\n He also has since gone on to write, produce and direct several films.\n\n\n# Doctor Strange (2016 film)Doctor Strange is a 2016 American superhero film based on the Marvel Comics character of the same name, produced by Marvel Studios and distributed by Walt Disney Studios Motion Pictures.\n It is the fourteenth film of the Marvel Cinematic Universe (MCU).\n The film was directed by Scott Derrickson, who wrote it with Jon Spaihts and C. Robert Cargill, and stars Benedict Cumberbatch as Stephen Strange, along with Chiwetel Ejiofor, Rachel McAdams, Benedict Wong, Michael Stuhlbarg, Benjamin Bratt, Scott Adkins, Mads Mikkelsen, and Tilda Swinton.\n In \"Doctor Strange\", surgeon Strange learns the mystic arts after a career-ending car accident.\n</code></pre> <p>\u8fd9\u4e9b\u5206\u5757\u7684top1\u53ef\u4ee5\u786e\u8ba4 Scott Derrickson \u548c Ed Wood \u90fd\u662f\u7f8e\u56fd\u4eba\u3002</p> <p>\u6709\u5173\u7d22\u5f15\u548c\u67e5\u8be2\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RAGEngine \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/rag.html#3-\u4fdd\u5b58\u548c\u52a0\u8f7d\u7d22\u5f15","title":"3. \u4fdd\u5b58\u548c\u52a0\u8f7d\u7d22\u5f15","text":"<p>\u7d22\u5f15\u6587\u6863\u540e\uff0c\u60a8\u53ef\u4ee5\u4fdd\u5b58\u7d22\u5f15\u4ee5\u4fbf\u4ee5\u540e\u91cd\u590d\u4f7f\u7528\uff0c\u800c\u65e0\u9700\u91cd\u65b0\u5904\u7406\u6570\u636e\u3002\u8fd9\u5bf9\u4e8e\u5927\u578b\u6570\u636e\u96c6\u6216\u751f\u4ea7\u73af\u5883\u975e\u5e38\u6709\u7528\u3002</p>"},{"location":"zh/tutorial/rag.html#\u4fdd\u5b58\u7d22\u5f15","title":"\u4fdd\u5b58\u7d22\u5f15","text":"<p>\u4fee\u6539 <code>rag_tutorial.py</code> \u811a\u672c\uff0c\u5728\u7d22\u5f15\u540e\u4fdd\u5b58\u7d22\u5f15\u3002\u5728 <code>rag_engine.add</code> \u540e\u6dfb\u52a0\u4ee5\u4e0b\u4ee3\u7801\uff1a</p> <pre><code># \u5c06\u7d22\u5f15\u4fdd\u5b58\u5230\u78c1\u76d8\nrag_engine.save(output_path=\"./data/indexing\", corpus_id=\"hotpotqa_corpus\", index_type=\"vector\")\nprint(\"\u7d22\u5f15\u5df2\u4fdd\u5b58\u5230 ./data/indexing\")\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u52a0\u8f7d\u7d22\u5f15","title":"\u52a0\u8f7d\u7d22\u5f15","text":"<p>\u8981\u52a0\u8f7d\u4fdd\u5b58\u7684\u7d22\u5f15\u5e76\u67e5\u8be2\u5b83\uff0c\u521b\u5efa\u4e00\u4e2a\u540d\u4e3a <code>rag_load.py</code> \u7684\u65b0\u811a\u672c\uff1a</p> <pre><code>import os\n\nfrom dotenv import load_dotenv\n\nfrom evoagentx.rag.schema import Query\nfrom evoagentx.rag.rag import RAGEngine\nfrom evoagentx.rag.rag_config import RAGConfig, ReaderConfig, ChunkerConfig, EmbeddingConfig, IndexConfig, RetrievalConfig\nfrom evoagentx.storages.base import StorageHandler\nfrom evoagentx.storages.storages_config import StoreConfig, VectorStoreConfig, DBConfig\n\n# \u52a0\u8f7d\u73af\u5883\u53d8\u91cf\nload_dotenv()\nOPENAI_API_KEY = os.getenv(\"OPENAI_API_KEY\")\n\n# \u914d\u7f6e\u5b58\u50a8\uff08SQLite \u7528\u4e8e\u5143\u6570\u636e\uff0cFAISS \u7528\u4e8e\u5411\u91cf\uff09\nstore_config = StoreConfig(\n    dbConfig=DBConfig(db_name=\"sqlite\", path=\"./data/cache.db\"),\n    vectorConfig=VectorStoreConfig(vector_name=\"faiss\", dimensions=1536, index_type=\"flat_l2\"),\n    graphConfig=None,\n    path=\"./data/indexing\"\n)\nstorage_handler = StorageHandler(storageConfig=store_config)\n\n# \u914d\u7f6e RAGEngine\nrag_config = RAGConfig(\n    reader=ReaderConfig(recursive=False, exclude_hidden=True),\n    chunker=ChunkerConfig(strategy=\"simple\", chunk_size=512, chunk_overlap=50),\n    embedding=EmbeddingConfig(provider=\"openai\", model_name=\"text-embedding-ada-002\", api_key=OPENAI_API_KEY),\n    index=IndexConfig(index_type=\"vector\"),\n    retrieval=RetrievalConfig(retrieval_type=\"vector\", postprocessor_type=\"simple\", top_k=3, similarity_cutoff=0.3)\n)\n\n# \u521d\u59cb\u5316 RAGEngine\nrag_engine = RAGEngine(config=rag_config, storage_handler=storage_handler)\n\nprint(\"RAGEngine \u5df2\u51c6\u5907\u5c31\u7eea\uff01\")\n\n# \u52a0\u8f7d\u4fdd\u5b58\u7684\u7d22\u5f15\nrag_engine.load(source=\"./data/indexing\", corpus_id=\"hotpotqa_corpus\", index_type=\"vector\")\nprint(\"\u7d22\u5f15\u52a0\u8f7d\u6210\u529f\uff01\")\n\n# \u67e5\u8be2\u52a0\u8f7d\u7684\u7d22\u5f15\nquery = Query(query_str=\"Were Scott Derrickson and Ed Wood of the same nationality?\", top_k=3)\nresult = rag_engine.query(query, corpus_id=\"hotpotqa_corpus\")\n\n# \u6253\u5370\u68c0\u7d22\u5230\u7684\u5206\u5757\nprint(\"\\n\u68c0\u7d22\u5230\u7684\u7b54\u6848\uff1a\")\nfor i, chunk in enumerate(result.corpus.chunks, 1):\n    print(f\"{i}. {chunk.text}\")\n\n# \u6e05\u7406\nrag_engine.clear(corpus_id=\"hotpotqa_corpus\")\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u8fd0\u884c\u811a\u672c_1","title":"\u8fd0\u884c\u811a\u672c","text":"<p>\u8fd0\u884c\u811a\u672c\uff1a</p> <pre><code>python rag_load.py\n</code></pre>"},{"location":"zh/tutorial/rag.html#\u4ee3\u7801\u89e3\u6790_2","title":"\u4ee3\u7801\u89e3\u6790","text":"<ul> <li>\u4fdd\u5b58\uff1a<code>save</code> \u65b9\u6cd5\u5c06\u5411\u91cf\u7d22\u5f15\u548c\u5143\u6570\u636e\u5b58\u50a8\u5230 <code>./data/indexing</code> \u76ee\u5f55\u3002</li> <li>\u52a0\u8f7d\uff1a<code>load</code> \u65b9\u6cd5\u4ece\u4fdd\u5b58\u7684\u6587\u4ef6\u91cd\u5efa\u7d22\u5f15\uff0c\u65e0\u9700\u91cd\u65b0\u7d22\u5f15\u5373\u53ef\u67e5\u8be2\u3002</li> <li>\u67e5\u8be2\uff1a\u6211\u4eec\u8fd0\u884c\u4e0e\u4e4b\u524d\u76f8\u540c\u7684\u67e5\u8be2\uff0c\u7ed3\u679c\u5e94\u4e00\u81f4\u3002</li> <li>\u6e05\u7406\uff1a\u6e05\u9664\u7d22\u5f15\u4ee5\u4fdd\u6301\u6574\u6d01\u3002</li> </ul>"},{"location":"zh/tutorial/rag.html#\u9884\u671f\u8f93\u51fa_1","title":"\u9884\u671f\u8f93\u51fa","text":"<p>\u8f93\u51fa\u5e94\u4e0e\u7b2c 2 \u8282\u76f8\u540c\uff0c\u786e\u8ba4\u52a0\u8f7d\u7684\u7d22\u5f15\u6b63\u5e38\u5de5\u4f5c\u3002</p> <p>\u6709\u5173\u4fdd\u5b58\u548c\u52a0\u8f7d\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u89c1 RAGEngine \u6587\u6863\u3002</p>"},{"location":"zh/tutorial/rag.html#\u4e0b\u4e00\u6b65","title":"\u4e0b\u4e00\u6b65","text":"<p>\u606d\u559c\u60a8\uff01\u60a8\u5df2\u4f7f\u7528 <code>RAGEngine</code> \u6784\u5efa\u4e86\u7b2c\u4e00\u4e2a RAG \u7cfb\u7edf\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u8fdb\u4e00\u6b65\u63a2\u7d22\u7684\u5efa\u8bae\uff1a - \u5c1d\u8bd5\u7d22\u5f15\u66f4\u5927\u7684\u6570\u636e\u96c6\u6216\u4e0d\u540c\u6587\u4ef6\u7c7b\u578b\uff08\u5982 PDF\uff09\u3002 - \u5b9e\u9a8c\u4e0d\u540c\u7684\u5206\u5757\u5927\u5c0f\u6216\u5d4c\u5165\u6a21\u578b\uff08\u5982 Hugging Face, ollama\uff09\u3002 - \u5c06 <code>RAGEngine</code> \u4e0e EvoAgentX \u4ee3\u7406\u7ed3\u5408\uff0c\u81ea\u52a8\u56de\u7b54\u95ee\u9898\u3002</p> <p>\u6709\u5173\u5b8c\u6574\u793a\u4f8b\uff0c\u8bf7\u53c2\u89c1 RAGEngine \u793a\u4f8b\u3002</p> <p>\u795d\u60a8\u5728 EvoAgentX \u4e2d\u6109\u5feb\u5730\u6784\u5efa\uff01</p>"},{"location":"zh/tutorial/sew_optimizer.html","title":"SEW\u4f18\u5316\u5668\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u60a8\u8bbe\u7f6e\u548c\u8fd0\u884cEvoAgentX\u4e2d\u7684SEW\uff08Self-Evolving Workflow\uff0c\u81ea\u8fdb\u5316\u5de5\u4f5c\u6d41\uff09\u4f18\u5316\u5668\u3002\u6211\u4eec\u5c06\u4f7f\u7528HumanEval\u57fa\u51c6\u4f5c\u4e3a\u793a\u4f8b\uff0c\u6f14\u793a\u5982\u4f55\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\u3002</p>"},{"location":"zh/tutorial/sew_optimizer.html#1-\u6982\u8ff0","title":"1. \u6982\u8ff0","text":"<p>SEW\u4f18\u5316\u5668\u662fEvoAgentX\u4e2d\u7684\u5f3a\u5927\u5de5\u5177\uff0c\u5b83\u4f7f\u60a8\u80fd\u591f\uff1a</p> <ul> <li>\u81ea\u52a8\u4f18\u5316\u591a\u667a\u80fd\u4f53\u5de5\u4f5c\u6d41\uff08\u63d0\u793a\u8bcd\u548c\u5de5\u4f5c\u6d41\u7ed3\u6784\uff09</li> <li>\u5728\u57fa\u51c6\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4f18\u5316\u7ed3\u679c</li> <li>\u652f\u6301\u4e0d\u540c\u7684\u5de5\u4f5c\u6d41\u8868\u793a\u65b9\u6848\uff08Python\u3001Yaml\u3001BPMN\u7b49\uff09</li> </ul>"},{"location":"zh/tutorial/sew_optimizer.html#2-\u8bbe\u7f6e\u73af\u5883","title":"2. \u8bbe\u7f6e\u73af\u5883","text":"<p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5bfc\u5165\u8bbe\u7f6eSEW\u4f18\u5316\u5668\u6240\u9700\u7684\u5fc5\u8981\u6a21\u5757\uff1a</p> <pre><code>from evoagentx.config import Config\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SEWWorkFlowGraph \nfrom evoagentx.agents import AgentManager\nfrom evoagentx.benchmark import HumanEval \nfrom evoagentx.evaluators import Evaluator \nfrom evoagentx.optimizers import SEWOptimizer \nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer.html#\u914d\u7f6ellm\u6a21\u578b","title":"\u914d\u7f6eLLM\u6a21\u578b","text":"<p>\u4e0eEvoAgentX\u4e2d\u7684\u5176\u4ed6\u7ec4\u4ef6\u7c7b\u4f3c\uff0c\u60a8\u9700\u8981\u4e00\u4e2a\u6709\u6548\u7684OpenAI API\u5bc6\u94a5\u6765\u521d\u59cb\u5316LLM\u3002</p> <pre><code>llm_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=OPENAI_API_KEY)\nllm = OpenAILLM(config=llm_config)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer.html#3-\u8bbe\u7f6e\u7ec4\u4ef6","title":"3. \u8bbe\u7f6e\u7ec4\u4ef6","text":""},{"location":"zh/tutorial/sew_optimizer.html#\u6b65\u9aa41\u521d\u59cb\u5316sew\u5de5\u4f5c\u6d41","title":"\u6b65\u9aa41\uff1a\u521d\u59cb\u5316SEW\u5de5\u4f5c\u6d41","text":"<p>SEW\u5de5\u4f5c\u6d41\u662f\u5c06\u88ab\u4f18\u5316\u7684\u6838\u5fc3\u7ec4\u4ef6\u3002\u5b83\u4ee3\u8868\u4e00\u4e2a\u987a\u5e8f\u5de5\u4f5c\u6d41\uff0c\u65e8\u5728\u89e3\u51b3\u4ee3\u7801\u751f\u6210\u4efb\u52a1\u3002</p> <pre><code>sew_graph = SEWWorkFlowGraph(llm_config=llm_config)\nagent_manager = AgentManager()\nagent_manager.add_agents_from_workflow(sew_graph)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer.html#\u6b65\u9aa42\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5","title":"\u6b65\u9aa42\uff1a\u51c6\u5907\u57fa\u51c6\u6d4b\u8bd5","text":"<p>\u5bf9\u4e8e\u672c\u6559\u7a0b\uff0c\u6211\u4eec\u5c06\u4f7f\u7528HumanEval\u57fa\u51c6\u7684\u4fee\u6539\u7248\u672c\uff0c\u8be5\u7248\u672c\u5c06\u6d4b\u8bd5\u6570\u636e\u5206\u4e3a\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff1a</p> <pre><code>class HumanEvalSplits(HumanEval):\n    def _load_data(self):\n        # \u52a0\u8f7d\u539f\u59cb\u6d4b\u8bd5\u6570\u636e\n        super()._load_data()\n        # \u5c06\u6570\u636e\u5206\u4e3a\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\n        import numpy as np \n        np.random.seed(42)\n        num_dev_samples = int(len(self._test_data) * 0.2)\n        random_indices = np.random.permutation(len(self._test_data))\n        self._dev_data = [self._test_data[i] for i in random_indices[:num_dev_samples]]\n        self._test_data = [self._test_data[i] for i in random_indices[num_dev_samples:]]\n\n# \u521d\u59cb\u5316\u57fa\u51c6\nhumaneval = HumanEvalSplits()\n</code></pre> <p>SEWOptimizer\u9ed8\u8ba4\u4f1a\u5728\u5f00\u53d1\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002\u8bf7\u786e\u4fdd\u57fa\u51c6\u6d4b\u8bd5\u6b63\u786e\u8bbe\u7f6e\u4e86\u5f00\u53d1\u96c6\u3002\u60a8\u53ef\u4ee5\uff1a    - \u4f7f\u7528\u5df2\u7ecf\u63d0\u4f9b\u5f00\u53d1\u96c6\u7684\u57fa\u51c6\uff08\u5982HotPotQA\uff09    - \u5c06\u6570\u636e\u96c6\u5206\u4e3a\u5f00\u53d1\u96c6\u548c\u6d4b\u8bd5\u96c6\uff08\u5982\u4e0a\u9762HumanEvalSplits\u793a\u4f8b\u6240\u793a\uff09    - \u5b9e\u73b0\u5e26\u6709\u5f00\u53d1\u96c6\u652f\u6301\u7684\u81ea\u5b9a\u4e49\u57fa\u51c6</p>"},{"location":"zh/tutorial/sew_optimizer.html#\u6b65\u9aa43\u8bbe\u7f6e\u8bc4\u4f30\u5668","title":"\u6b65\u9aa43\uff1a\u8bbe\u7f6e\u8bc4\u4f30\u5668","text":"<p>\u8bc4\u4f30\u5668\u8d1f\u8d23\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u3002\u6709\u5173\u5982\u4f55\u8bbe\u7f6e\u548c\u4f7f\u7528\u8bc4\u4f30\u5668\u7684\u66f4\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u57fa\u51c6\u548c\u8bc4\u4f30\u6559\u7a0b\u3002</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    # \u5c06\u539f\u59cb\u793a\u4f8b\u8f6c\u6362\u4e3aSEW\u5de5\u4f5c\u6d41\u7684\u9884\u671f\u8f93\u5165\n    return {\"question\": example[\"prompt\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer.html#4-\u914d\u7f6e\u548c\u8fd0\u884csew\u4f18\u5316\u5668","title":"4. \u914d\u7f6e\u548c\u8fd0\u884cSEW\u4f18\u5316\u5668","text":"<p>SEW\u4f18\u5316\u5668\u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u53c2\u6570\u914d\u7f6e\uff0c\u4ee5\u63a7\u5236\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code>optimizer = SEWOptimizer(\n    graph=sew_graph,           # \u8981\u4f18\u5316\u7684\u5de5\u4f5c\u6d41\u56fe\n    evaluator=evaluator,       # \u7528\u4e8e\u6027\u80fd\u8bc4\u4f30\u7684\u8bc4\u4f30\u5668\n    llm=llm,                   # \u8bed\u8a00\u6a21\u578b\n    max_steps=10,              # \u6700\u5927\u4f18\u5316\u6b65\u9aa4\u6570\n    eval_rounds=1,             # \u6bcf\u6b65\u8bc4\u4f30\u8f6e\u6570\n    repr_scheme=\"python\",      # \u5de5\u4f5c\u6d41\u7684\u8868\u793a\u65b9\u6848\n    optimize_mode=\"prompt\",    # \u8981\u4f18\u5316\u7684\u65b9\u9762\uff08\u63d0\u793a/\u7ed3\u6784/\u5168\u90e8\uff09\n    order=\"zero-order\"         # \u4f18\u5316\u7b97\u6cd5\u987a\u5e8f\uff08\u96f6\u9636/\u4e00\u9636\uff09\n)\n</code></pre>"},{"location":"zh/tutorial/sew_optimizer.html#\u8fd0\u884c\u4f18\u5316","title":"\u8fd0\u884c\u4f18\u5316","text":"<p>\u8981\u542f\u52a8\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <pre><code># \u4f18\u5316SEW\u5de5\u4f5c\u6d41\noptimizer.optimize(dataset=humaneval)\n\n# \u8bc4\u4f30\u4f18\u5316\u540e\u7684\u5de5\u4f5c\u6d41\nwith suppress_logger_info():\n    metrics = optimizer.evaluate(dataset=humaneval, eval_mode=\"test\")\nprint(\"Evaluation metrics: \", metrics)\n\n# \u4fdd\u5b58\u4f18\u5316\u540e\u7684SEW\u5de5\u4f5c\u6d41\noptimizer.save(\"debug/optimized_sew_workflow.json\")\n</code></pre> <p>\u6709\u5173\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605sew_optimizer.py\u3002</p>"},{"location":"zh/tutorial/textgrad_optimizer.html","title":"TextGrad \u4f18\u5316\u5668\u6559\u7a0b","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u60a8\u8bbe\u7f6e\u548c\u8fd0\u884c EvoAgentX \u4e2d\u7684 TextGrad \u4f18\u5316\u5668\u7684\u8fc7\u7a0b\u3002\u6211\u4eec\u5c06\u4f7f\u7528 MATH \u6570\u636e\u96c6\u4f5c\u4e3a\u793a\u4f8b\uff0c\u6f14\u793a\u5982\u4f55\u4f18\u5316\u5de5\u4f5c\u6d41\u4e2d\u7684\u63d0\u793a\u8bcd\u548c\u7cfb\u7edf\u63d0\u793a\u8bcd\u3002</p>"},{"location":"zh/tutorial/textgrad_optimizer.html#1-textgrad","title":"1. TextGrad","text":"<p>TextGrad \u4f7f\u7528\u6765\u81ea LLM \u7684\u6587\u672c\u53cd\u9988\u6765\u6539\u8fdb\u6587\u672c\u53d8\u91cf\u3002\u5728 EvoAgentX \u4e2d\uff0c\u6211\u4eec\u4f7f\u7528 TextGrad \u6765\u4f18\u5316\u4ee3\u7406\u7684\u63d0\u793a\u8bcd\u548c\u7cfb\u7edf\u63d0\u793a\u8bcd\u3002\u6709\u5173 TextGrad \u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u4ed6\u4eec\u7684\u8bba\u6587\u548c GitHub\u3002</p>"},{"location":"zh/tutorial/textgrad_optimizer.html#2-textgrad-\u4f18\u5316\u5668","title":"2. TextGrad \u4f18\u5316\u5668","text":"<p>EvoAgentX \u4e2d\u7684 TextGrad \u4f18\u5316\u5668\u4f7f\u60a8\u80fd\u591f\uff1a</p> <ul> <li>\u81ea\u52a8\u4f18\u5316\u591a\u4ee3\u7406\u5de5\u4f5c\u6d41\uff08\u63d0\u793a\u8bcd\u548c/\u6216\u7cfb\u7edf\u63d0\u793a\u8bcd\uff09</li> <li>\u5728\u6570\u636e\u96c6\u4e0a\u8bc4\u4f30\u4f18\u5316\u7ed3\u679c</li> </ul>"},{"location":"zh/tutorial/textgrad_optimizer.html#3-\u8bbe\u7f6e\u73af\u5883","title":"3. \u8bbe\u7f6e\u73af\u5883","text":"<p>\u9996\u5148\uff0c\u8ba9\u6211\u4eec\u5bfc\u5165\u8bbe\u7f6e TextGrad \u4f18\u5316\u5668\u6240\u9700\u7684\u6a21\u5757\uff1a</p> <pre><code>from evoagentx.benchmark import MATH\nfrom evoagentx.optimizers import TextGradOptimizer\nfrom evoagentx.models import OpenAILLMConfig, OpenAILLM\nfrom evoagentx.workflow import SequentialWorkFlowGraph\nfrom evoagentx.core.callbacks import suppress_logger_info\n</code></pre>"},{"location":"zh/tutorial/textgrad_optimizer.html#\u914d\u7f6e-llm-\u6a21\u578b","title":"\u914d\u7f6e LLM \u6a21\u578b","text":"<p>\u60a8\u9700\u8981\u6709\u6548\u7684 API \u5bc6\u94a5\u6765\u521d\u59cb\u5316 LLM\u3002\u6709\u5173\u5982\u4f55\u8bbe\u7f6e API \u5bc6\u94a5\u7684\u66f4\u591a\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5feb\u901f\u5165\u95e8\u3002</p> <p><code>TextGradOptimizer</code> \u5141\u8bb8\u5728\u5de5\u4f5c\u6d41\u6267\u884c\u548c\u4f18\u5316\u4e2d\u4f7f\u7528\u4e0d\u540c\u7684 LLM\u3002\u4f8b\u5982\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 GPT 4o-mini \u8fdb\u884c\u5de5\u4f5c\u6d41\u6267\u884c\uff0c\u4f7f\u7528 GPT 4o \u8fdb\u884c\u4f18\u5316\u3002</p> <pre><code>executor_config = OpenAILLMConfig(model=\"gpt-4o-mini\", openai_key=\"your_openai_api_key\")\nexecutor_llm = OpenAILLM(config=executor_config)\n\noptimizer_config = OpenAILLMConfig(model=\"gpt-4o\", openai_key=\"your_openai_api_key\")\noptimizer_llm = OpenAILLM(config=optimizer_config)\n</code></pre>"},{"location":"zh/tutorial/textgrad_optimizer.html#3-\u8bbe\u7f6e\u7ec4\u4ef6","title":"3. \u8bbe\u7f6e\u7ec4\u4ef6","text":""},{"location":"zh/tutorial/textgrad_optimizer.html#\u6b65\u9aa4-1\u521d\u59cb\u5316\u5de5\u4f5c\u6d41","title":"\u6b65\u9aa4 1\uff1a\u521d\u59cb\u5316\u5de5\u4f5c\u6d41","text":"<p>\u76ee\u524d\uff0c<code>TextGradOptimizer</code> \u4ec5\u652f\u6301 <code>SequentialWorkFlowGraph</code>\u3002\u6709\u5173 <code>SequentialWorkFlowGraph</code> \u7684\u66f4\u591a\u4fe1\u606f\uff0c\u8bf7\u53c2\u9605\u5de5\u4f5c\u6d41\u56fe\u3002\u5bf9\u4e8e\u6b64\u793a\u4f8b\uff0c\u8ba9\u6211\u4eec\u521b\u5efa\u53ea\u6709\u5355\u4e2a\u8282\u70b9\u7684\u6700\u7b80\u5355\u5de5\u4f5c\u6d41\u3002</p> <pre><code>math_graph_data = {\n    \"goal\": r\"Answer the math question. The answer should be in box format, e.g., \\boxed{123}\",\n    \"tasks\": [\n        {\n            \"name\": \"answer_generate\",\n            \"description\": \"Answer generation for Math.\",\n            \"inputs\": [\n                {\"name\": \"problem\", \"type\": \"str\", \"required\": True, \"description\": \"The problem to solve.\"}\n            ],\n            \"outputs\": [\n                {\"name\": \"answer\", \"type\": \"str\", \"required\": True, \"description\": \"The generated answer.\"}\n            ],\n            \"prompt\": \"Answer the math question. The answer should be in box format, e.g., \\\\boxed{{123}}\\n\\nProblem: {problem}\",\n            \"parse_mode\": \"str\"\n        }\n    ] \n}\n\nworkflow_graph = SequentialWorkFlowGraph.from_dict(math_graph_data)\n</code></pre>"},{"location":"zh/tutorial/textgrad_optimizer.html#\u6b65\u9aa4-2\u51c6\u5907\u6570\u636e\u96c6","title":"\u6b65\u9aa4 2\uff1a\u51c6\u5907\u6570\u636e\u96c6","text":"<p>\u5bf9\u4e8e\u672c\u6559\u7a0b\uff0c\u6211\u4eec\u5c06\u4f7f\u7528 MATH \u6570\u636e\u96c6\uff0c\u8be5\u6570\u636e\u96c6\u5305\u542b\u5404\u79cd\u96be\u5ea6\u7ea7\u522b\u548c\u4e3b\u9898\u9886\u57df\u7684\u5177\u6709\u6311\u6218\u6027\u7684\u7ade\u8d5b\u6570\u5b66\u95ee\u9898\u3002\u8be5\u6570\u636e\u96c6\u5206\u4e3a 7.5K \u8bad\u7ec3\u95ee\u9898\u548c 5K \u6d4b\u8bd5\u95ee\u9898\u3002\u51fa\u4e8e\u6f14\u793a\u76ee\u7684\uff0c\u8ba9\u6211\u4eec\u53d6\u6570\u636e\u96c6\u7684\u4e00\u4e2a\u8f83\u5c0f\u5b50\u96c6\uff0c\u4ee5\u52a0\u5feb\u9a8c\u8bc1\u548c\u8bc4\u4f30\u8fc7\u7a0b\u3002</p> <pre><code>class MathSplits(MATH):\n    def _load_data(self):\n        super()._load_data()\n        import numpy as np \n        np.random.seed(42)\n        permutation = np.random.permutation(len(self._test_data))\n        full_test_data = self._test_data\n        # \u968f\u673a\u9009\u62e910\u4e2a\u6837\u672c\u7528\u4e8e\u8bad\u7ec3\uff0c40\u4e2a\u7528\u4e8e\u5f00\u53d1\uff0c100\u4e2a\u7528\u4e8e\u6d4b\u8bd5\n        self._train_data = [full_test_data[idx] for idx in permutation[:10]]\n        self._dev_data = [full_test_data[idx] for idx in permutation[10:50]]\n        self._test_data = [full_test_data[idx] for idx in permutation[50:150]]\n\nmath_splits = MathSplits()\n</code></pre> <p>\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\uff0c<code>TextGradOptimizer</code> \u9ed8\u8ba4\u4f1a\u5728\u5f00\u53d1\u96c6\u4e0a\u8bc4\u4f30\u6027\u80fd\u3002\u8bf7\u786e\u4fdd\u6570\u636e\u96c6\u6709\u4e00\u4e2a\u6b63\u786e\u8bbe\u7f6e\u7684\u5f00\u53d1\u96c6\uff08\u5373 <code>benchmark._dev_data</code> \u4e0d\u4e3a None\uff09\u3002\u60a8\u53ef\u4ee5\uff1a    - \u4f7f\u7528\u5df2\u7ecf\u63d0\u4f9b\u5f00\u53d1\u96c6\u7684\u6570\u636e\u96c6    - \u62c6\u5206\u60a8\u7684\u6570\u636e\u96c6\u4ee5\u521b\u5efa\u4e00\u4e2a\u5f00\u53d1\u96c6\uff08\u5982\u4e0a\u4f8b\u6240\u793a\uff09    - \u5b9e\u73b0\u4e00\u4e2a\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\uff08\u7ee7\u627f\u81ea <code>evoagentx.benchmark.Benchmark</code>\uff09\uff0c\u6b63\u786e\u8bbe\u7f6e\u5f00\u53d1\u96c6</p>"},{"location":"zh/tutorial/textgrad_optimizer.html#\u6b65\u9aa4-3\u8bbe\u7f6e\u8bc4\u4f30\u5668","title":"\u6b65\u9aa4 3\uff1a\u8bbe\u7f6e\u8bc4\u4f30\u5668","text":"<p>\u8bc4\u4f30\u5668\u8d1f\u8d23\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u8bc4\u4f30\u5de5\u4f5c\u6d41\u7684\u6027\u80fd\u3002\u6709\u5173\u5982\u4f55\u8bbe\u7f6e\u548c\u4f7f\u7528\u8bc4\u4f30\u5668\u7684\u66f4\u8be6\u7ec6\u4fe1\u606f\uff0c\u8bf7\u53c2\u8003\u57fa\u51c6\u6d4b\u8bd5\u548c\u8bc4\u4f30\u6559\u7a0b\u3002</p> <pre><code>def collate_func(example: dict) -&gt; dict:\n    return {\"problem\": example[\"problem\"]}\n\nevaluator = Evaluator(\n    llm=llm, \n    agent_manager=agent_manager, \n    collate_func=collate_func, \n    num_workers=5, \n    verbose=True\n)\n</code></pre>"},{"location":"zh/tutorial/textgrad_optimizer.html#4-\u914d\u7f6e\u548c\u8fd0\u884c-textgrad-\u4f18\u5316\u5668","title":"4. \u914d\u7f6e\u548c\u8fd0\u884c TextGrad \u4f18\u5316\u5668","text":"<p>TextGradOptimizer \u53ef\u4ee5\u901a\u8fc7\u5404\u79cd\u53c2\u6570\u914d\u7f6e\u6765\u63a7\u5236\u4f18\u5316\u8fc7\u7a0b\uff1a</p> <ul> <li><code>graph</code>\uff1a\u8981\u4f18\u5316\u7684\u5de5\u4f5c\u6d41\u56fe</li> <li><code>optimize_mode</code>\uff1a\u4f18\u5316\u6a21\u5f0f\uff1a<ul> <li>\"all\"\uff1a\u4f18\u5316\u63d0\u793a\u8bcd\u548c\u7cfb\u7edf\u63d0\u793a\u8bcd</li> <li>\"prompt\"\uff1a\u4ec5\u4f18\u5316\u63d0\u793a\u8bcd</li> <li>\"system_prompt\"\uff1a\u4ec5\u4f18\u5316\u7cfb\u7edf\u63d0\u793a\u8bcd</li> </ul> </li> <li><code>executor_llm</code>\uff1a\u7528\u4e8e\u6267\u884c\u5de5\u4f5c\u6d41\u7684 LLM</li> <li><code>optimizer_llm</code>\uff1a\u7528\u4e8e\u4f18\u5316\u5de5\u4f5c\u6d41\u7684 LLM</li> <li><code>batch_size</code>\uff1a\u4f18\u5316\u7684\u6279\u91cf\u5927\u5c0f</li> <li><code>max_steps</code>\uff1a\u6700\u5927\u4f18\u5316\u6b65\u6570</li> <li><code>evaluator</code>\uff1a\u5728\u4f18\u5316\u671f\u95f4\u6267\u884c\u8bc4\u4f30\u7684\u8bc4\u4f30\u5668</li> <li><code>eval_interval</code>\uff1a\u8bc4\u4f30\u4e4b\u95f4\u7684\u6b65\u6570</li> <li><code>eval_rounds</code>\uff1a\u8bc4\u4f30\u8f6e\u6570</li> <li><code>eval_config</code>\uff1a\u4f18\u5316\u671f\u95f4\u7684\u8bc4\u4f30\u914d\u7f6e\uff08\u4f20\u9012\u7ed9 <code>TextGradOptimizer.evaluate()</code>\uff09\u3002\u4f8b\u5982\uff0c\u5982\u679c\u6211\u4eec\u4e0d\u60f3\u5728\u6574\u4e2a\u5f00\u53d1\u96c6\u4e0a\u8fdb\u884c\u8bc4\u4f30\uff0c\u53ef\u4ee5\u8bbe\u7f6e <code>eval_config = {\"sample_k\": 100}</code> \u4ee5\u4ec5\u5728\u5f00\u53d1\u96c6\u4e2d\u7684 100 \u4e2a\u968f\u673a\u6837\u672c\u4e0a\u8fdb\u884c\u8bc4\u4f30\u3002</li> <li><code>save_interval</code>\uff1a\u4fdd\u5b58\u5de5\u4f5c\u6d41\u56fe\u4e4b\u95f4\u7684\u6b65\u6570</li> <li><code>save_path</code>\uff1a\u4fdd\u5b58\u5de5\u4f5c\u6d41\u56fe\u7684\u8def\u5f84</li> <li><code>rollback</code>\uff1a\u662f\u5426\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u56de\u6eda\u5230\u6700\u4f73\u5de5\u4f5c\u6d41\u56fe</li> </ul> <pre><code>textgrad_optimizer = TextGradOptimizer(\n    graph=workflow_graph, \n    optimize_mode=\"all\",\n    executor_llm=executor_llm, \n    optimizer_llm=optimizer_llm,\n    batch_size=3,\n    max_steps=20,\n    evaluator=evaluator,\n    eval_interval=1,\n    eval_rounds=1,\n    save_interval=None,\n    save_path=\"./\",\n    rollback=True\n)\n</code></pre>"},{"location":"zh/tutorial/textgrad_optimizer.html#\u8fd0\u884c\u4f18\u5316","title":"\u8fd0\u884c\u4f18\u5316","text":"<p>\u8981\u5f00\u59cb\u4f18\u5316\u8fc7\u7a0b\uff1a <pre><code>textgrad_optimizer.optimize(dataset=math_splits, seed=8)\n</code></pre> <code>seed</code> \u7528\u4e8e\u968f\u673a\u6253\u4e71\u8bad\u7ec3\u6570\u636e\u3002\u8bad\u7ec3\u6570\u636e\u6bcf\u4e2a\u8f6e\u6b21\u4f1a\u81ea\u52a8\u91cd\u65b0\u6253\u4e71\u3002\u5982\u679c\u63d0\u4f9b\u4e86 <code>seed</code>\uff0c\u5219\u7528\u4e8e\u6253\u4e71\u8bad\u7ec3\u6570\u636e\u7684\u6709\u6548\u79cd\u5b50\u4e3a <code>seed + epoch</code>\u3002</p> <p>\u4f18\u5316\u7ed3\u675f\u65f6\u7684\u6700\u7ec8\u56fe\u4e0d\u4e00\u5b9a\u662f\u6700\u597d\u7684\u56fe\u3002\u5982\u679c\u60a8\u5e0c\u671b\u6062\u590d\u5728\u5f00\u53d1\u96c6\u4e0a\u8868\u73b0\u6700\u597d\u7684\u56fe\uff0c\u53ea\u9700\u8c03\u7528 <pre><code>textgrad_optimizer.restore_best_graph()\n</code></pre></p> <p>\u6211\u4eec\u53ef\u4ee5\u518d\u6b21\u8bc4\u4f30\u5de5\u4f5c\u6d41\uff0c\u770b\u770b\u4f18\u5316\u540e\u7684\u6539\u8fdb\u60c5\u51b5\u3002 <pre><code>with suppress_logger_info():\n    result = textgrad_optimizer.evaluate(dataset=math_splits, eval_mode=\"test\")\nprint(f\"Evaluation result (after optimization):\\n{result}\")\n</code></pre></p> <p><code>TextGradOptimizer</code> \u59cb\u7ec8\u5c06\u6700\u7ec8\u5de5\u4f5c\u6d41\u56fe\u548c\u6700\u4f73\u5de5\u4f5c\u6d41\u56fe\u4fdd\u5b58\u5230 <code>save_path</code>\u3002\u5982\u679c <code>save_interval</code> \u4e0d\u4e3a <code>None</code>\uff0c\u5b83\u4e5f\u4f1a\u5728\u4f18\u5316\u8fc7\u7a0b\u4e2d\u4fdd\u5b58\u56fe\u3002\u60a8\u4e5f\u53ef\u4ee5\u901a\u8fc7\u8c03\u7528 <code>textgrad_optimizer.save()</code> \u624b\u52a8\u4fdd\u5b58\u5de5\u4f5c\u6d41\u56fe\u3002</p> <p>\u8bf7\u6ce8\u610f\uff0c<code>TextGradOptimizer</code> \u4e0d\u4f1a\u66f4\u6539\u5de5\u4f5c\u6d41\u7ed3\u6784\uff0c\u4f46\u4fdd\u5b58\u5de5\u4f5c\u6d41\u56fe\u4e5f\u4f1a\u4fdd\u5b58\u5728\u4f18\u5316\u540e\u4f1a\u6709\u6240\u4e0d\u540c\u7684\u63d0\u793a\u8bcd\u548c\u7cfb\u7edf\u63d0\u793a\u8bcd\u3002 \u4ee5\u4e0b\u662f\u4f7f\u7528 <code>TextGradOptimizer</code> \u4f18\u5316\u540e\u4fdd\u5b58\u7684\u5de5\u4f5c\u6d41\u56fe\u793a\u4f8b\u3002</p> <pre><code>{\n    \"class_name\": \"SequentialWorkFlowGraph\",\n    \"goal\": \"Answer the math question. The answer should be in box format, e.g., \\\\boxed{123}\",\n    \"tasks\": [\n        {\n            \"name\": \"answer_generate\",\n            \"description\": \"Answer generation for Math.\",\n            \"inputs\": [\n                {\n                    \"name\": \"problem\",\n                    \"type\": \"str\",\n                    \"description\": \"The problem to solve.\",\n                    \"required\": true\n                }\n            ],\n            \"outputs\": [\n                {\n                    \"name\": \"answer\",\n                    \"type\": \"str\",\n                    \"description\": \"The generated answer.\",\n                    \"required\": true\n                }\n            ],\n            \"prompt\": \"To solve the math problem, follow these steps:\\n\\n1. **Contextual Overview**: Begin with a brief overview of the problem-solving strategy, using logical reasoning and mathematical principles to derive the solution. Include any relevant geometric or algebraic insights.\\n\\n2. **Key Steps Identification**: Break down the problem-solving process into distinct parts:\\n   - Identify the relevant mathematical operations and properties, such as symmetry, roots of unity, or trigonometric identities.\\n   - Perform the necessary calculations, ensuring each step logically follows from the previous one.\\n   - Present the final answer.\\n\\n3. **Conciseness and Clarity**: Provide a clear and concise explanation of your solution, avoiding unnecessary repetition. Use consistent formatting and notation throughout.\\n\\n4. **Mathematical Justification**: Explain the reasoning behind each step to ensure the solution is well-justified. Include explanations of reference angles, geometric interpretations, and any special conditions or edge cases.\\n\\n5. **Verification Step**: Include a quick verification step to confirm the accuracy of your calculations. Consider recalculating key values if initial assumptions were incorrect.\\n\\n6. **Visual Aids**: Where applicable, include diagrams or sketches to visually represent the problem and solution, enhancing understanding.\\n\\n7. **Final Answer Presentation**: Present the final answer clearly and ensure it is boxed, reflecting the correct solution. Verify that it aligns with the problem's requirements and any known correct solutions.\\n\\nProblem: &lt;input&gt;{problem}&lt;/input&gt;\",\n            \"system_prompt\": \"You are a math-focused assistant dedicated to providing clear, concise, and educational solutions to mathematical problems. Your goal is to deliver structured and pedagogically sound explanations, ensuring mathematical accuracy and logical reasoning. Begin with a brief overview of the problem-solving approach, followed by detailed calculations, and conclude with a verification step. Use precise mathematical notation and consider potential edge cases. Present the final answer clearly, using the specified format, and incorporate visual aids or analogies where appropriate to enhance understanding and engagement. \\n\\nExplicitly include geometric explanations when applicable, describing the geometric context and relationships. Emphasize the importance of visual aids, such as diagrams or sketches, to enhance understanding. Ensure consistency in formatting and mathematical notation. Provide a brief explanation of the reference angle concept and its significance. Include contextual explanations of trigonometric identities and their applications. Critically evaluate initial assumptions and verify geometric properties before proceeding. Highlight the use of symmetry and conjugate pairs in complex numbers. Encourage re-evaluation and verification of steps, ensuring logical flow and clarity. Focus on deriving the correct answer and consider problem-specific strategies or known techniques.\",\n            \"parse_mode\": \"str\",\n            \"parse_func\": null,\n            \"parse_title\": null\n        }\n    ]\n}\n</code></pre> <p>\u5b8c\u6574\u7684\u5de5\u4f5c\u793a\u4f8b\uff0c\u8bf7\u53c2\u9605 examples/textgrad/math_textgrad.py\u3002\u5176\u4ed6\u6570\u636e\u96c6\u7684 TextGrad \u4f18\u5316\u811a\u672c\uff08\u4f8b\u5982\uff0c<code>hotpotqa_textgrad.py</code> \u548c <code>mbqq_textgrad.py</code>\uff09\u53ef\u4ee5\u5728 examples/optimization/textgrad \u76ee\u5f55\u4e2d\u627e\u5230\u3002</p>"},{"location":"zh/tutorial/tools.html","title":"\u5728 EvoAgentX \u4e2d\u4f7f\u7528\u5de5\u5177","text":"<p>\u672c\u6559\u7a0b\u5c06\u6307\u5bfc\u4f60\u4f7f\u7528 EvoAgentX \u5f3a\u5927\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\u3002\u5de5\u5177\u5141\u8bb8\u4ee3\u7406\u4e0e\u5916\u90e8\u4e16\u754c\u4ea4\u4e92\u3001\u6267\u884c\u8ba1\u7b97\u548c\u8bbf\u95ee\u4fe1\u606f\u3002\u6211\u4eec\u5c06\u6db5\u76d6\uff1a</p> <ol> <li>\u7406\u89e3\u5de5\u5177\u67b6\u6784\uff1a\u4e86\u89e3\u57fa\u7840 Tool \u7c7b\u53ca\u5176\u529f\u80fd</li> <li>\u4ee3\u7801\u89e3\u91ca\u5668\uff1a\u4f7f\u7528 Python \u548c Docker \u89e3\u91ca\u5668\u5b89\u5168\u6267\u884c Python \u4ee3\u7801</li> <li>\u641c\u7d22\u5de5\u5177\uff1a\u4f7f\u7528 Wikipedia \u548c Google \u641c\u7d22\u5de5\u5177\u8bbf\u95ee\u7f51\u7edc\u4fe1\u606f</li> <li>MCP \u5de5\u5177\uff1a\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u8fde\u63a5\u5230\u5916\u90e8\u670d\u52a1</li> </ol> <p>\u901a\u8fc7\u672c\u6559\u7a0b\uff0c\u4f60\u5c06\u4e86\u89e3\u5982\u4f55\u5728\u81ea\u5df1\u7684\u4ee3\u7406\u548c\u5de5\u4f5c\u6d41\u4e2d\u5229\u7528\u8fd9\u4e9b\u5de5\u5177\u3002</p>"},{"location":"zh/tutorial/tools.html#1-\u7406\u89e3\u5de5\u5177\u67b6\u6784","title":"1. \u7406\u89e3\u5de5\u5177\u67b6\u6784","text":"<p>EvoAgentX \u5de5\u5177\u751f\u6001\u7cfb\u7edf\u7684\u6838\u5fc3\u662f <code>Tool</code> \u57fa\u7c7b\uff0c\u5b83\u4e3a\u6240\u6709\u5de5\u5177\u63d0\u4f9b\u4e86\u6807\u51c6\u5316\u63a5\u53e3\u3002</p> <pre><code>from evoagentx.tools.tool import Tool\n</code></pre> <p><code>Tool</code> \u7c7b\u5b9e\u73b0\u4e86\u4e09\u4e2a\u5173\u952e\u65b9\u6cd5\uff1a</p> <ul> <li><code>get_tool_schemas()</code>\uff1a\u8fd4\u56de\u5de5\u5177\u7684 OpenAI \u517c\u5bb9\u51fd\u6570\u6a21\u5f0f</li> <li><code>get_tools()</code>\uff1a\u8fd4\u56de\u5de5\u5177\u63d0\u4f9b\u7684\u53ef\u8c03\u7528\u51fd\u6570\u5217\u8868</li> <li><code>get_tool_descriptions()</code>\uff1a\u8fd4\u56de\u5de5\u5177\u529f\u80fd\u63cf\u8ff0</li> </ul> <p>EvoAgentX \u4e2d\u7684\u6240\u6709\u5de5\u5177\u90fd\u7ee7\u627f\u81ea\u8fd9\u4e2a\u57fa\u7c7b\uff0c\u786e\u4fdd\u4ee3\u7406\u4f7f\u7528\u5b83\u4eec\u65f6\u6709\u4e00\u81f4\u7684\u63a5\u53e3\u3002</p>"},{"location":"zh/tutorial/tools.html#\u5173\u952e\u6982\u5ff5","title":"\u5173\u952e\u6982\u5ff5","text":"<ul> <li>\u5de5\u5177\u96c6\u6210\uff1a\u5de5\u5177\u901a\u8fc7\u51fd\u6570\u8c03\u7528\u534f\u8bae\u4e0e\u4ee3\u7406\u65e0\u7f1d\u96c6\u6210</li> <li>\u6a21\u5f0f\uff1a\u6bcf\u4e2a\u5de5\u5177\u90fd\u63d0\u4f9b\u63cf\u8ff0\u5176\u529f\u80fd\u3001\u53c2\u6570\u548c\u8f93\u51fa\u7684\u6a21\u5f0f</li> <li>\u6a21\u5757\u5316\uff1a\u5de5\u5177\u53ef\u4ee5\u8f7b\u677e\u6dfb\u52a0\u5230\u4efb\u4f55\u652f\u6301\u51fd\u6570\u8c03\u7528\u7684\u4ee3\u7406\u4e2d</li> </ul>"},{"location":"zh/tutorial/tools.html#2-\u4ee3\u7801\u89e3\u91ca\u5668","title":"2. \u4ee3\u7801\u89e3\u91ca\u5668","text":"<p>EvoAgentX \u63d0\u4f9b\u4e24\u79cd\u4e3b\u8981\u7684\u4ee3\u7801\u89e3\u91ca\u5668\u5de5\u5177\uff1a</p> <ol> <li>PythonInterpreter\uff1a\u5728\u53d7\u63a7\u73af\u5883\u4e2d\u6267\u884c Python \u4ee3\u7801</li> <li>DockerInterpreter\uff1a\u5728\u9694\u79bb\u7684 Docker \u5bb9\u5668\u4e2d\u6267\u884c\u4ee3\u7801</li> </ol>"},{"location":"zh/tutorial/tools.html#21-pythoninterpreter","title":"2.1 PythonInterpreter","text":"<p>PythonInterpreter \u63d0\u4f9b\u4e86\u4e00\u4e2a\u5b89\u5168\u7684\u73af\u5883\u6765\u6267\u884c Python \u4ee3\u7801\uff0c\u53ef\u4ee5\u7cbe\u7ec6\u63a7\u5236\u5bfc\u5165\u3001\u76ee\u5f55\u8bbf\u95ee\u548c\u6267\u884c\u4e0a\u4e0b\u6587\u3002\u5b83\u4f7f\u7528\u6c99\u7bb1\u65b9\u6cd5\u6765\u9650\u5236\u6f5c\u5728\u7684\u6709\u5bb3\u64cd\u4f5c\u3002</p>"},{"location":"zh/tutorial/tools.html#211-\u8bbe\u7f6e","title":"2.1.1 \u8bbe\u7f6e","text":"<pre><code>from evoagentx.tools.interpreter_python import PythonInterpreter\n\n# \u4f7f\u7528\u7279\u5b9a\u7684\u5141\u8bb8\u5bfc\u5165\u548c\u76ee\u5f55\u8bbf\u95ee\u8fdb\u884c\u521d\u59cb\u5316\ninterpreter = PythonInterpreter(\n    project_path=\".\",  # \u9ed8\u8ba4\u4e3a\u5f53\u524d\u76ee\u5f55\n    directory_names=[\"examples\", \"evoagentx\"],\n    allowed_imports={\"os\", \"sys\", \"math\", \"random\", \"datetime\"}\n)\n</code></pre>"},{"location":"zh/tutorial/tools.html#212-\u53ef\u7528\u65b9\u6cd5","title":"2.1.2 \u53ef\u7528\u65b9\u6cd5","text":"<p><code>PythonInterpreter</code> \u63d0\u4f9b\u4ee5\u4e0b\u53ef\u8c03\u7528\u65b9\u6cd5\uff1a</p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-1-executecode-language","title":"\u65b9\u6cd5 1: execute(code, language)","text":"<p>\u63cf\u8ff0\uff1a\u5728\u5b89\u5168\u73af\u5883\u4e2d\u76f4\u63a5\u6267\u884c Python \u4ee3\u7801\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u6267\u884c\u7b80\u5355\u7684\u4ee3\u7801\u7247\u6bb5\nresult = interpreter.execute(\"\"\"\nprint(\"Hello, World!\")\nimport math\nprint(f\"The value of pi is: {math.pi:.4f}\")\n\"\"\", \"python\")\n\nprint(result)\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>str</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>Hello, World!\nThe value of pi is: 3.1416\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-2-execute_scriptfile_path-language","title":"\u65b9\u6cd5 2: execute_script(file_path, language)","text":"<p>\u63cf\u8ff0\uff1a\u5728\u5b89\u5168\u73af\u5883\u4e2d\u6267\u884c Python \u811a\u672c\u6587\u4ef6\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u6267\u884c Python \u811a\u672c\u6587\u4ef6\nscript_path = \"examples/hello_world.py\"\nscript_result = interpreter.execute_script(script_path, \"python\")\nprint(script_result)\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>str</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>Running hello_world.py...\nHello from the script file!\nScript execution completed.\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#213-\u8bbe\u7f6e\u63d0\u793a","title":"2.1.3 \u8bbe\u7f6e\u63d0\u793a","text":"<ul> <li> <p>\u9879\u76ee\u8def\u5f84\uff1a<code>project_path</code> \u53c2\u6570\u5e94\u6307\u5411\u9879\u76ee\u7684\u6839\u76ee\u5f55\uff0c\u4ee5\u786e\u4fdd\u6b63\u786e\u7684\u6587\u4ef6\u8bbf\u95ee\u3002\u9ed8\u8ba4\u4e3a\u5f53\u524d\u76ee\u5f55\uff08\".\"\uff09\u3002</p> </li> <li> <p>\u76ee\u5f55\u540d\u79f0\uff1a<code>directory_names</code> \u5217\u8868\u6307\u5b9a\u9879\u76ee\u4e2d\u53ef\u4ee5\u5bfc\u5165\u7684\u76ee\u5f55\u3002\u8fd9\u5bf9\u5b89\u5168\u6027\u5f88\u91cd\u8981\uff0c\u53ef\u4ee5\u9632\u6b62\u672a\u6388\u6743\u7684\u8bbf\u95ee\u3002\u9ed8\u8ba4\u4e3a\u7a7a\u5217\u8868 <code>[]</code>\u3002</p> </li> <li> <p>\u5141\u8bb8\u7684\u5bfc\u5165\uff1a<code>allowed_imports</code> \u96c6\u5408\u9650\u5236\u53ef\u4ee5\u5728\u6267\u884c\u4ee3\u7801\u4e2d\u5bfc\u5165\u7684 Python \u6a21\u5757\u3002\u9ed8\u8ba4\u4e3a\u7a7a\u5217\u8868 <code>[]</code>\u3002</p> </li> <li>\u91cd\u8981\uff1a\u5982\u679c <code>allowed_imports</code> \u8bbe\u7f6e\u4e3a\u7a7a\u5217\u8868\uff0c\u5219\u4e0d\u5e94\u7528\u5bfc\u5165\u9650\u5236\u3002</li> <li>\u6307\u5b9a\u65f6\uff0c\u53ea\u6dfb\u52a0\u4f60\u8ba4\u4e3a\u5b89\u5168\u7684\u6a21\u5757\uff1a</li> </ul> <pre><code># \u5e26\u6709\u5bfc\u5165\u9650\u5236\u7684\u793a\u4f8b\ninterpreter = PythonInterpreter(\n    project_path=os.getcwd(),\n    directory_names=[\"examples\", \"evoagentx\", \"tests\"],\n    allowed_imports={\n        \"os\", \"sys\", \"time\", \"datetime\", \"math\", \"random\", \n        \"json\", \"csv\", \"re\", \"collections\", \"itertools\"\n    }\n)\n\n# \u65e0\u5bfc\u5165\u9650\u5236\u7684\u793a\u4f8b\ninterpreter = PythonInterpreter(\n    project_path=os.getcwd(),\n    directory_names=[\"examples\", \"evoagentx\"],\n    allowed_imports=[]  # \u5141\u8bb8\u5bfc\u5165\u4efb\u4f55\u6a21\u5757\n)\n</code></pre>"},{"location":"zh/tutorial/tools.html#22-dockerinterpreter","title":"2.2 DockerInterpreter","text":"<p>DockerInterpreter \u5728\u9694\u79bb\u7684 Docker \u5bb9\u5668\u4e2d\u6267\u884c\u4ee3\u7801\uff0c\u63d0\u4f9b\u6700\u5927\u7684\u5b89\u5168\u6027\u548c\u73af\u5883\u9694\u79bb\u3002\u5b83\u5141\u8bb8\u5b89\u5168\u6267\u884c\u6f5c\u5728\u98ce\u9669\u7684\u4ee3\u7801\uff0c\u5177\u6709\u81ea\u5b9a\u4e49\u73af\u5883\u3001\u4f9d\u8d56\u9879\u548c\u5b8c\u6574\u7684\u8d44\u6e90\u9694\u79bb\u3002\u4f7f\u7528\u6b64\u5de5\u5177\u9700\u8981\u5728\u4f60\u7684\u673a\u5668\u4e0a\u5b89\u88c5\u5e76\u8fd0\u884c Docker\u3002</p>"},{"location":"zh/tutorial/tools.html#221-\u8bbe\u7f6e","title":"2.2.1 \u8bbe\u7f6e","text":"<pre><code>from evoagentx.tools.interpreter_docker import DockerInterpreter\n\n# \u4f7f\u7528\u7279\u5b9a\u7684 Docker \u955c\u50cf\u521d\u59cb\u5316\ninterpreter = DockerInterpreter(\n    image_tag=\"fundingsocietiesdocker/python3.9-slim\",\n    print_stdout=True,\n    print_stderr=True,\n    container_directory=\"/app\"\n)\n</code></pre>"},{"location":"zh/tutorial/tools.html#222-\u53ef\u7528\u65b9\u6cd5","title":"2.2.2 \u53ef\u7528\u65b9\u6cd5","text":"<p><code>DockerInterpreter</code> \u63d0\u4f9b\u4ee5\u4e0b\u53ef\u8c03\u7528\u65b9\u6cd5\uff1a</p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-1-executecode-language_1","title":"\u65b9\u6cd5 1: execute(code, language)","text":"<p>\u63cf\u8ff0\uff1a\u5728 Docker \u5bb9\u5668\u5185\u6267\u884c\u4ee3\u7801\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u5728 Docker \u5bb9\u5668\u4e2d\u6267\u884c Python \u4ee3\u7801\nresult = interpreter.execute(\"\"\"\nimport platform\nprint(f\"Python version: {platform.python_version()}\")\nprint(f\"Platform: {platform.system()} {platform.release()}\")\n\"\"\", \"python\")\n\nprint(result)\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>str</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>Python version: 3.9.16\nPlatform: Linux 5.15.0-1031-azure\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-2-execute_scriptfile_path-language_1","title":"\u65b9\u6cd5 2: execute_script(file_path, language)","text":"<p>\u63cf\u8ff0\uff1a\u5728 Docker \u5bb9\u5668\u5185\u6267\u884c\u811a\u672c\u6587\u4ef6\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u5728 Docker \u4e2d\u6267\u884c Python \u811a\u672c\u6587\u4ef6\nscript_path = \"examples/docker_test.py\"\nscript_result = interpreter.execute_script(script_path, \"python\")\nprint(script_result)\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>str</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>Running container with script: /app/script_12345.py\nHello from the Docker container!\nContainer execution completed.\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#223-\u8bbe\u7f6e\u63d0\u793a","title":"2.2.3 \u8bbe\u7f6e\u63d0\u793a","text":"<ul> <li> <p>Docker \u8981\u6c42\uff1a\u4f7f\u7528\u6b64\u89e3\u91ca\u5668\u524d\uff0c\u786e\u4fdd Docker \u5df2\u5b89\u88c5\u5e76\u5728\u7cfb\u7edf\u4e0a\u8fd0\u884c\u3002</p> </li> <li> <p>\u955c\u50cf\u7ba1\u7406\uff1a\u4f60\u9700\u8981\u63d0\u4f9b <code>image_tag</code> \u6216 <code>dockerfile_path</code> \u4e2d\u7684\u4e00\u4e2a\uff0c\u4e0d\u80fd\u540c\u65f6\u63d0\u4f9b\uff1a</p> </li> <li> <p>\u9009\u9879 1\uff1a\u4f7f\u7528\u73b0\u6709\u955c\u50cf <pre><code>interpreter = DockerInterpreter(\n    image_tag=\"python:3.9-slim\",  # \u4f7f\u7528\u73b0\u6709\u7684 Docker Hub \u955c\u50cf\n    container_directory=\"/app\"\n)\n</code></pre></p> </li> <li> <p>\u9009\u9879 2\uff1a\u4ece Dockerfile \u6784\u5efa <pre><code>interpreter = DockerInterpreter(\n    dockerfile_path=\"path/to/Dockerfile\",  # \u6784\u5efa\u81ea\u5b9a\u4e49\u955c\u50cf\n    image_tag=\"my-custom-image-name\",      # \u6784\u5efa\u955c\u50cf\u7684\u540d\u79f0\n    container_directory=\"/app\"\n)\n</code></pre></p> </li> <li> <p>\u6587\u4ef6\u8bbf\u95ee\uff1a</p> </li> <li>\u8981\u4f7f\u672c\u5730\u6587\u4ef6\u5728\u5bb9\u5668\u4e2d\u53ef\u7528\uff0c\u4f7f\u7528 <code>host_directory</code> \u53c2\u6570\uff1a   <pre><code>interpreter = DockerInterpreter(\n    image_tag=\"python:3.9-slim\",\n    host_directory=\"/path/to/local/files\",\n    container_directory=\"/app/data\"\n)\n</code></pre></li> <li> <p>\u8fd9\u5c06\u672c\u5730\u76ee\u5f55\u6302\u8f7d\u5230\u6307\u5b9a\u7684\u5bb9\u5668\u76ee\u5f55\uff0c\u4f7f\u6240\u6709\u6587\u4ef6\u53ef\u8bbf\u95ee\u3002</p> </li> <li> <p>\u5bb9\u5668\u751f\u547d\u5468\u671f\uff1a</p> </li> <li>Docker \u5bb9\u5668\u5728\u521d\u59cb\u5316\u89e3\u91ca\u5668\u65f6\u521b\u5efa\uff0c\u5728\u89e3\u91ca\u5668\u9500\u6bc1\u65f6\u79fb\u9664\u3002</li> <li> <p>\u5bf9\u4e8e\u957f\u65f6\u95f4\u8fd0\u884c\u7684\u4f1a\u8bdd\uff0c\u53ef\u4ee5\u8bbe\u7f6e <code>print_stdout</code> \u548c <code>print_stderr</code> \u4ee5\u67e5\u770b\u5b9e\u65f6\u8f93\u51fa\u3002</p> </li> <li> <p>\u6545\u969c\u6392\u9664\uff1a</p> </li> <li>\u5982\u679c\u9047\u5230\u6743\u9650\u95ee\u9898\uff0c\u786e\u4fdd\u4f60\u7684\u7528\u6237\u5177\u6709 Docker \u6743\u9650\u3002</li> <li>\u5bf9\u4e8e\u7f51\u7edc\u76f8\u5173\u9519\u8bef\uff0c\u68c0\u67e5 Docker \u5b88\u62a4\u8fdb\u7a0b\u662f\u5426\u6709\u9002\u5f53\u7684\u7f51\u7edc\u8bbf\u95ee\u6743\u9650\u3002</li> </ul>"},{"location":"zh/tutorial/tools.html#3-\u641c\u7d22\u5de5\u5177","title":"3. \u641c\u7d22\u5de5\u5177","text":"<p>EvoAgentX \u63d0\u4f9b\u591a\u79cd\u641c\u7d22\u5de5\u5177\u6765\u4ece\u5404\u79cd\u6765\u6e90\u68c0\u7d22\u4fe1\u606f\uff1a</p> <ol> <li>SearchWiki\uff1a\u641c\u7d22 Wikipedia \u83b7\u53d6\u4fe1\u606f</li> <li>SearchGoogle\uff1a\u4f7f\u7528\u5b98\u65b9 API \u641c\u7d22 Google</li> <li>SearchGoogleFree\uff1a\u65e0\u9700 API \u5bc6\u94a5\u5373\u53ef\u641c\u7d22 Google</li> </ol>"},{"location":"zh/tutorial/tools.html#31-searchwiki","title":"3.1 SearchWiki","text":"<p>SearchWiki \u5de5\u5177\u4ece Wikipedia \u6587\u7ae0\u68c0\u7d22\u4fe1\u606f\uff0c\u63d0\u4f9b\u6458\u8981\u3001\u5b8c\u6574\u5185\u5bb9\u548c\u5143\u6570\u636e\u3002\u5b83\u63d0\u4f9b\u4e86\u4e00\u79cd\u7b80\u5355\u7684\u65b9\u6cd5\uff0c\u65e0\u9700\u590d\u6742\u7684 API \u8bbe\u7f6e\u5373\u53ef\u5c06\u767e\u79d1\u5168\u4e66\u77e5\u8bc6\u6574\u5408\u5230\u4f60\u7684\u4ee3\u7406\u4e2d\u3002</p>"},{"location":"zh/tutorial/tools.html#311-\u8bbe\u7f6e","title":"3.1.1 \u8bbe\u7f6e","text":"<pre><code>from evoagentx.tools.search_wiki import SearchWiki\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u53c2\u6570\u521d\u59cb\u5316\nwiki_search = SearchWiki(max_sentences=3)\n</code></pre>"},{"location":"zh/tutorial/tools.html#312-\u53ef\u7528\u65b9\u6cd5","title":"3.1.2 \u53ef\u7528\u65b9\u6cd5","text":"<p><code>SearchWiki</code> \u63d0\u4f9b\u4ee5\u4e0b\u53ef\u8c03\u7528\u65b9\u6cd5\uff1a</p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-searchquery","title":"\u65b9\u6cd5: search(query)","text":"<p>\u63cf\u8ff0\uff1a\u641c\u7d22 Wikipedia \u83b7\u53d6\u4e0e\u67e5\u8be2\u5339\u914d\u7684\u6587\u7ae0\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u641c\u7d22 Wikipedia \u83b7\u53d6\u4fe1\u606f\nresults = wiki_search.search(\n    query=\"artificial intelligence agent architecture\"\n)\n\n# \u5904\u7406\u7ed3\u679c\nfor i, result in enumerate(results.get(\"results\", [])):\n    print(f\"\u7ed3\u679c {i+1}: {result['title']}\")\n    print(f\"\u6458\u8981: {result['summary']}\")\n    print(f\"URL: {result['url']}\")\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>dict</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>{\n    \"results\": [\n        {\n            \"title\": \"Artificial intelligence\",\n            \"summary\": \"Artificial intelligence (AI) is the intelligence of machines or software, as opposed to the intelligence of humans or animals. AI applications include advanced web search engines, recommendation systems, voice assistants...\",\n            \"url\": \"https://en.wikipedia.org/wiki/Artificial_intelligence\"\n        },\n        {\n            \"title\": \"Intelligent agent\",\n            \"summary\": \"In artificial intelligence, an intelligent agent (IA) is anything which can perceive its environment, process those perceptions, and respond in pursuit of its own goals...\",\n            \"url\": \"https://en.wikipedia.org/wiki/Intelligent_agent\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#32-searchgoogle","title":"3.2 SearchGoogle","text":"<p>SearchGoogle \u5de5\u5177\u901a\u8fc7 Google \u7684\u5b98\u65b9\u81ea\u5b9a\u4e49\u641c\u7d22 API \u5b9e\u73b0\u7f51\u7edc\u641c\u7d22\uff0c\u63d0\u4f9b\u9ad8\u8d28\u91cf\u641c\u7d22\u7ed3\u679c\u548c\u5185\u5bb9\u63d0\u53d6\u3002\u5b83\u9700\u8981 API \u51ed\u8bc1\uff0c\u4f46\u63d0\u4f9b\u66f4\u53ef\u9760\u548c\u5168\u9762\u7684\u641c\u7d22\u529f\u80fd\u3002</p>"},{"location":"zh/tutorial/tools.html#321-\u8bbe\u7f6e","title":"3.2.1 \u8bbe\u7f6e","text":"<pre><code>from evoagentx.tools.search_google import SearchGoogle\n\n# \u4f7f\u7528\u81ea\u5b9a\u4e49\u53c2\u6570\u521d\u59cb\u5316\ngoogle_search = SearchGoogle(\n    num_search_pages=3,\n    max_content_words=200\n)\n</code></pre>"},{"location":"zh/tutorial/tools.html#322-\u53ef\u7528\u65b9\u6cd5","title":"3.2.2 \u53ef\u7528\u65b9\u6cd5","text":"<p><code>SearchGoogle</code> \u63d0\u4f9b\u4ee5\u4e0b\u53ef\u8c03\u7528\u65b9\u6cd5\uff1a</p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-searchquery_1","title":"\u65b9\u6cd5: search(query)","text":"<p>\u63cf\u8ff0\uff1a\u641c\u7d22 Google \u83b7\u53d6\u4e0e\u67e5\u8be2\u5339\u914d\u7684\u5185\u5bb9\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u641c\u7d22 Google \u83b7\u53d6\u4fe1\u606f\nresults = google_search.search(\n    query=\"evolutionary algorithms for neural networks\"\n)\n\n# \u5904\u7406\u7ed3\u679c\nfor i, result in enumerate(results.get(\"results\", [])):\n    print(f\"\u7ed3\u679c {i+1}: {result['title']}\")\n    print(f\"URL: {result['url']}\")\n    print(f\"\u5185\u5bb9: {result['content'][:150]}...\")\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>dict</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>{\n    \"results\": [\n        {\n            \"title\": \"Evolutionary Algorithms for Neural Networks - A Systematic Review\",\n            \"url\": \"https://example.com/paper1\",\n            \"content\": \"This paper provides a comprehensive review of evolutionary algorithms applied to neural network optimization. Key approaches include genetic algorithms, particle swarm optimization, and differential evolution...\"\n        },\n        {\n            \"title\": \"Applying Genetic Algorithms to Neural Network Training\",\n            \"url\": \"https://example.com/article2\",\n            \"content\": \"Genetic algorithms offer a powerful approach to optimizing neural network architectures and weights. This article explores how evolutionary computation can overcome limitations of gradient-based methods...\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#323-\u8bbe\u7f6e\u63d0\u793a","title":"3.2.3 \u8bbe\u7f6e\u63d0\u793a","text":"<ul> <li> <p>API \u8981\u6c42\uff1a\u6b64\u5de5\u5177\u9700\u8981 Google \u81ea\u5b9a\u4e49\u641c\u7d22 API \u51ed\u8bc1\u3002\u5728\u4f60\u7684\u73af\u5883\u4e2d\u8bbe\u7f6e\u5b83\u4eec\uff1a   <pre><code># \u5728\u4f60\u7684 .env \u6587\u4ef6\u6216\u73af\u5883\u53d8\u91cf\u4e2d\nGOOGLE_API_KEY=your_google_api_key_here\nGOOGLE_SEARCH_ENGINE_ID=your_search_engine_id_here\n</code></pre></p> </li> <li> <p>\u83b7\u53d6\u51ed\u8bc1\uff1a</p> </li> <li>\u5728 Google Cloud Console \u521b\u5efa\u9879\u76ee</li> <li>\u542f\u7528\u81ea\u5b9a\u4e49\u641c\u7d22 API</li> <li>\u521b\u5efa API \u51ed\u8bc1</li> <li>\u5728 https://cse.google.com/cse/ \u8bbe\u7f6e\u81ea\u5b9a\u4e49\u641c\u7d22\u5f15\u64ce</li> </ul>"},{"location":"zh/tutorial/tools.html#33-searchgooglefree","title":"3.3 SearchGoogleFree","text":"<p>SearchGoogleFree \u5de5\u5177\u63d0\u4f9b\u7f51\u7edc\u641c\u7d22\u529f\u80fd\uff0c\u65e0\u9700\u4efb\u4f55 API \u5bc6\u94a5\u6216\u8ba4\u8bc1\u3002\u5b83\u63d0\u4f9b\u4e86\u5b98\u65b9 Google API \u7684\u66f4\u7b80\u5355\u66ff\u4ee3\u65b9\u6848\uff0c\u5177\u6709\u9002\u5408\u5927\u591a\u6570\u4e00\u822c\u67e5\u8be2\u7684\u57fa\u672c\u641c\u7d22\u7ed3\u679c\u3002</p>"},{"location":"zh/tutorial/tools.html#331-\u8bbe\u7f6e","title":"3.3.1 \u8bbe\u7f6e","text":"<pre><code>from evoagentx.tools.search_google_f import SearchGoogleFree\n\n# \u521d\u59cb\u5316\u514d\u8d39 Google \u641c\u7d22\ngoogle_free = SearchGoogleFree(\n    num_search_pages=3,\n    max_content_words=500\n)\n</code></pre>"},{"location":"zh/tutorial/tools.html#332-\u53ef\u7528\u65b9\u6cd5","title":"3.3.2 \u53ef\u7528\u65b9\u6cd5","text":"<p><code>SearchGoogleFree</code> \u63d0\u4f9b\u4ee5\u4e0b\u53ef\u8c03\u7528\u65b9\u6cd5\uff1a</p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-searchquery_2","title":"\u65b9\u6cd5: search(query)","text":"<p>\u63cf\u8ff0\uff1a\u65e0\u9700 API \u5bc6\u94a5\u5373\u53ef\u641c\u7d22 Google \u83b7\u53d6\u4e0e\u67e5\u8be2\u5339\u914d\u7684\u5185\u5bb9\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u65e0\u9700 API \u5bc6\u94a5\u641c\u7d22 Google\nresults = google_free.search(\n    query=\"reinforcement learning algorithms\"\n)\n\n# \u5904\u7406\u7ed3\u679c\nfor i, result in enumerate(results.get(\"results\", [])):\n    print(f\"\u7ed3\u679c {i+1}: {result['title']}\")\n    print(f\"URL: {result['url']}\")\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>dict</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>{\n    \"results\": [\n        {\n            \"title\": \"Introduction to Reinforcement Learning Algorithms\",\n            \"url\": \"https://example.com/intro-rl\",\n            \"snippet\": \"A comprehensive overview of reinforcement learning algorithms including Q-learning, SARSA, and policy gradient methods.\"\n        },\n        {\n            \"title\": \"Top 10 Reinforcement Learning Algorithms for Beginners\",\n            \"url\": \"https://example.com/top-rl\",\n            \"snippet\": \"Learn about the most commonly used reinforcement learning algorithms with practical examples and implementation tips.\"\n        }\n    ]\n}\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#4-mcp-\u5de5\u5177","title":"4. MCP \u5de5\u5177","text":"<p>\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\uff08MCP\uff09\u5de5\u5177\u5305\u63d0\u4f9b\u4e86\u4e00\u79cd\u901a\u8fc7 MCP \u534f\u8bae\u8fde\u63a5\u5230\u5916\u90e8\u670d\u52a1\u7684\u6807\u51c6\u5316\u65b9\u6cd5\u3002\u5b83\u4f7f\u4ee3\u7406\u80fd\u591f\u8bbf\u95ee\u4e13\u95e8\u7684\u5de5\u5177\uff0c\u5982\u5de5\u4f5c\u641c\u7d22\u670d\u52a1\u3001\u6570\u636e\u5904\u7406\u5b9e\u7528\u7a0b\u5e8f\u548c\u5176\u4ed6 MCP \u517c\u5bb9\u7684 API\uff0c\u800c\u65e0\u9700\u76f4\u63a5\u96c6\u6210\u6bcf\u4e2a\u670d\u52a1\u3002</p>"},{"location":"zh/tutorial/tools.html#411-\u8bbe\u7f6e","title":"4.1.1 \u8bbe\u7f6e","text":"<pre><code>from evoagentx.tools.mcp import MCPToolkit\n\n# \u4f7f\u7528\u914d\u7f6e\u6587\u4ef6\u521d\u59cb\u5316\nmcp_toolkit = MCPToolkit(config_path=\"examples/sample_mcp.config\")\n\n# \u6216\u4f7f\u7528\u914d\u7f6e\u5b57\u5178\u521d\u59cb\u5316\nconfig = {\n    \"mcpServers\": {\n        \"hirebase\": {\n            \"command\": \"uvx\",\n            \"args\": [\"hirebase-mcp\"],\n            \"env\": {\"HIREBASE_API_KEY\": \"your_api_key_here\"}\n        }\n    }\n}\nmcp_toolkit = MCPToolkit(config=config)\n</code></pre>"},{"location":"zh/tutorial/tools.html#412-\u53ef\u7528\u65b9\u6cd5","title":"4.1.2 \u53ef\u7528\u65b9\u6cd5","text":"<p><code>MCPToolkit</code> \u63d0\u4f9b\u4ee5\u4e0b\u53ef\u8c03\u7528\u65b9\u6cd5\uff1a</p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-1-get_tools","title":"\u65b9\u6cd5 1: get_tools()","text":"<p>\u63cf\u8ff0\uff1a\u8fd4\u56de\u4ece\u8fde\u63a5\u7684 MCP \u670d\u52a1\u5668\u83b7\u53d6\u7684\u6240\u6709\u53ef\u7528\u5de5\u5177\u5217\u8868\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u83b7\u53d6\u6240\u6709\u53ef\u7528\u7684 MCP \u5de5\u5177\ntools = mcp_toolkit.get_tools()\n\n# \u663e\u793a\u53ef\u7528\u5de5\u5177\nfor i, tool in enumerate(tools):\n    print(f\"\u5de5\u5177 {i+1}: {tool.name}\")\n    print(f\"\u63cf\u8ff0: {tool.descriptions[0]}\")\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>List[Tool]</code></p> <p>\u793a\u4f8b\u8fd4\u56de\uff1a <pre><code>[MCPTool(name=\"HirebaseSearch\", descriptions=[\"\u901a\u8fc7\u63d0\u4f9b\u5173\u952e\u8bcd\u641c\u7d22\u5de5\u4f5c\u4fe1\u606f\"]), \n MCPTool(name=\"HirebaseAnalyze\", descriptions=[\"\u5206\u6790\u7ed9\u5b9a\u6280\u80fd\u7684\u5de5\u4f5c\u5e02\u573a\u8d8b\u52bf\"])]\n</code></pre></p>"},{"location":"zh/tutorial/tools.html#\u65b9\u6cd5-2-disconnect","title":"\u65b9\u6cd5 2: disconnect()","text":"<p>\u63cf\u8ff0\uff1a\u65ad\u5f00\u4e0e\u6240\u6709 MCP \u670d\u52a1\u5668\u7684\u8fde\u63a5\u5e76\u6e05\u7406\u8d44\u6e90\u3002</p> <p>\u4f7f\u7528\u793a\u4f8b\uff1a <pre><code># \u4f7f\u7528\u5b8c MCP \u5de5\u5177\u5305\u540e\nmcp_toolkit.disconnect()\n</code></pre></p> <p>\u8fd4\u56de\u7c7b\u578b\uff1a<code>None</code></p>"},{"location":"zh/tutorial/tools.html#413-\u4f7f\u7528-mcp-\u5de5\u5177","title":"4.1.3 \u4f7f\u7528 MCP \u5de5\u5177","text":"<p>\u4e00\u65e6\u4ece MCPToolkit \u83b7\u53d6\u4e86\u5de5\u5177\uff0c\u4f60\u53ef\u4ee5\u50cf\u4f7f\u7528\u4efb\u4f55\u5176\u4ed6 EvoAgentX \u5de5\u5177\u4e00\u6837\u4f7f\u7528\u5b83\u4eec\uff1a</p> <pre><code># \u4ece\u5de5\u5177\u5305\u83b7\u53d6\u6240\u6709\u5de5\u5177\ntools = mcp_toolkit.get_tools()\n\n# \u67e5\u627e\u7279\u5b9a\u5de5\u5177\nhirebase_tool = None\nfor tool in tools:\n    if \"hire\" in tool.name.lower() or \"search\" in tool.name.lower():\n        hirebase_tool = tool\n        break\n\nif hirebase_tool:\n    # \u4f7f\u7528\u5de5\u5177\u641c\u7d22\u4fe1\u606f\n    search_query = \"data scientist\"\n    result = hirebase_tool.tools[0](**{\"query\": search_query})\n\n    print(f\"'{search_query}' \u7684\u641c\u7d22\u7ed3\u679c\uff1a\")\n    print(result)\n</code></pre>"},{"location":"zh/tutorial/tools.html#414-\u8bbe\u7f6e\u63d0\u793a","title":"4.1.4 \u8bbe\u7f6e\u63d0\u793a","text":"<ul> <li> <p>\u914d\u7f6e\u6587\u4ef6\uff1a\u914d\u7f6e\u6587\u4ef6\u5e94\u9075\u5faa MCP \u534f\u8bae\u7684\u670d\u52a1\u5668\u914d\u7f6e\u683c\u5f0f\uff1a   <pre><code>{\n    \"mcpServers\": {\n        \"serverName\": {\n            \"command\": \"executable_command\",\n            \"args\": [\"command_arguments\"],\n            \"env\": {\"ENV_VAR_NAME\": \"value\"}\n        }\n    }\n}\n</code></pre></p> </li> <li> <p>\u670d\u52a1\u5668\u7c7b\u578b\uff1a</p> </li> <li>\u57fa\u4e8e\u547d\u4ee4\u7684\u670d\u52a1\u5668\uff1a\u4f7f\u7528 <code>command</code> \u5b57\u6bb5\u6307\u5b9a\u53ef\u6267\u884c\u6587\u4ef6</li> <li> <p>\u57fa\u4e8e URL \u7684\u670d\u52a1\u5668\uff1a\u4f7f\u7528 <code>url</code> \u5b57\u6bb5\u6307\u5b9a\u670d\u52a1\u5668\u7aef\u70b9</p> </li> <li> <p>\u8fde\u63a5\u7ba1\u7406\uff1a</p> </li> <li>\u4f7f\u7528\u5b8c MCPToolkit \u540e\u59cb\u7ec8\u8c03\u7528 <code>disconnect()</code> \u4ee5\u91ca\u653e\u8d44\u6e90</li> <li> <p>\u4f7f\u7528 try-finally \u5757\u8fdb\u884c\u81ea\u52a8\u6e05\u7406\uff1a     <pre><code>try:\n    toolkit = MCPToolkit(config_path=\"config.json\")\n    tools = toolkit.get_tools()\n    # \u5728\u8fd9\u91cc\u4f7f\u7528\u5de5\u5177\nfinally:\n    toolkit.disconnect()\n</code></pre></p> </li> <li> <p>\u9519\u8bef\u5904\u7406\uff1a</p> </li> <li>\u5982\u679c\u65e0\u6cd5\u8fde\u63a5\u5230\u670d\u52a1\u5668\uff0cMCPToolkit \u5c06\u8bb0\u5f55\u8b66\u544a\u6d88\u606f</li> <li> <p>\u6700\u597d\u5728\u5de5\u5177\u8c03\u7528\u5468\u56f4\u5b9e\u73b0\u9519\u8bef\u5904\u7406\uff1a     <pre><code>try:\n    result = tool.tools[0](**{\"query\": \"example query\"})\nexcept Exception as e:\n    print(f\"\u8c03\u7528 MCP \u5de5\u5177\u65f6\u51fa\u9519\uff1a{str(e)}\")\n</code></pre></p> </li> <li> <p>\u73af\u5883\u53d8\u91cf\uff1a</p> </li> <li>API \u5bc6\u94a5\u548c\u5176\u4ed6\u654f\u611f\u4fe1\u606f\u53ef\u4ee5\u901a\u8fc7\u914d\u7f6e\u4e2d\u7684\u73af\u5883\u53d8\u91cf\u63d0\u4f9b</li> <li>\u4f60\u4e5f\u53ef\u4ee5\u5728\u8fd0\u884c\u5e94\u7528\u7a0b\u5e8f\u4e4b\u524d\u5728\u73af\u5883\u4e2d\u8bbe\u7f6e\u5b83\u4eec</li> </ul>"},{"location":"zh/tutorial/tools.html#\u603b\u7ed3","title":"\u603b\u7ed3","text":"<p>\u5728\u672c\u6559\u7a0b\u4e2d\uff0c\u6211\u4eec\u63a2\u7d22\u4e86 EvoAgentX \u4e2d\u7684\u5de5\u5177\u751f\u6001\u7cfb\u7edf\uff1a</p> <ol> <li>\u5de5\u5177\u67b6\u6784\uff1a\u7406\u89e3\u4e86\u57fa\u7840 Tool \u7c7b\u53ca\u5176\u6807\u51c6\u5316\u63a5\u53e3</li> <li>\u4ee3\u7801\u89e3\u91ca\u5668\uff1a\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528 Python \u548c Docker \u89e3\u91ca\u5668\u5b89\u5168\u6267\u884c Python \u4ee3\u7801</li> <li>\u641c\u7d22\u5de5\u5177\uff1a\u53d1\u73b0\u4e86\u5982\u4f55\u4f7f\u7528 Wikipedia \u548c Google \u641c\u7d22\u5de5\u5177\u8bbf\u95ee\u7f51\u7edc\u4fe1\u606f</li> <li>MCP \u5de5\u5177\uff1a\u5b66\u4e60\u4e86\u5982\u4f55\u4f7f\u7528\u6a21\u578b\u4e0a\u4e0b\u6587\u534f\u8bae\u8fde\u63a5\u5230\u5916\u90e8\u670d\u52a1</li> </ol> <p>EvoAgentX \u4e2d\u7684\u5de5\u5177\u901a\u8fc7\u63d0\u4f9b\u5bf9\u5916\u90e8\u8d44\u6e90\u548c\u8ba1\u7b97\u7684\u8bbf\u95ee\u6765\u6269\u5c55\u4f60\u7684\u4ee3\u7406\u529f\u80fd\u3002\u901a\u8fc7\u5c06\u8fd9\u4e9b\u5de5\u5177\u4e0e\u4ee3\u7406\u548c\u5de5\u4f5c\u6d41\u7ed3\u5408\uff0c\u4f60\u53ef\u4ee5\u6784\u5efa\u5f3a\u5927\u7684 AI \u7cfb\u7edf\uff0c\u80fd\u591f\u68c0\u7d22\u4fe1\u606f\u3001\u6267\u884c\u8ba1\u7b97\u5e76\u4e0e\u4e16\u754c\u4ea4\u4e92\u3002</p> <p>\u6709\u5173\u66f4\u9ad8\u7ea7\u7684\u7528\u6cd5\u548c\u81ea\u5b9a\u4e49\u9009\u9879\uff0c\u8bf7\u53c2\u8003 API \u6587\u6863 \u5e76\u63a2\u7d22\u4ed3\u5e93\u4e2d\u7684\u793a\u4f8b\u3002 </p>"}]}