import asyncio
import os
from typing import Dict, List
import re
from collections import Counter

from dotenv import load_dotenv
from tqdm.asyncio import tqdm as aio_tqdm

# å‡è®¾ evo2_optimizer.py æ–‡ä»¶ä¸æ­¤è„šæœ¬åœ¨åŒä¸€ç›®å½•æˆ–åœ¨ Python è·¯å¾„ä¸­
# è¯¥æ–‡ä»¶åº”åŒ…å«æˆ‘ä»¬ä¹‹å‰è®¨è®ºè¿‡çš„ EvopromptOptimizer, GAOptimizer, DEOptimizer ç±»
from evoagentx.optimizers.evo2_optimizer import DEOptimizer, GAOptimizer
from evoagentx.benchmark.bigbenchhard import BIGBenchHard
from evoagentx.models import OpenAILLM, OpenAILLMConfig
from evoagentx.optimizers.engine.registry import ParamRegistry

class SinglePromptSarcasmClassifier:
    """
    ä¸€ä¸ªä½¿ç”¨å•ä¸€ã€å¯è¿›åŒ–çš„â€œæ€ç»´é“¾â€å‰ç¼€æ¥åˆ†ç±»è®½åˆºçš„ç¨‹åºã€‚
    """
    def __init__(self, model: OpenAILLM):
        self.model = model

        # æ ¸å¿ƒä»»åŠ¡æŒ‡ä»¤æ˜¯å›ºå®šçš„
        self.task_instruction = "From the two sentences provided, (A) and (B), determine which one is sarcastic. Respond with your final choice wrapped like this: FINAL_ANSWER((A))"
        
        # è¿™æ˜¯ä¸€ä¸ªå¯è¿›åŒ–çš„â€œèŠ‚ç‚¹â€ï¼Œå®ƒçš„åˆå§‹ç§ç¾¤ç”±ä¸‹é¢çš„åˆ—è¡¨æä¾›ã€‚
        # ä¼˜åŒ–å™¨å°†åœ¨è¿™ä¸ªç§å­åº“çš„åŸºç¡€ä¸Šè¿›è¡Œæ¼”åŒ–ã€‚
        self.chain_of_thought_prefix = [
            "Let's think step by step.",
            "Letâ€™s work this out in a step by step way to be sure we have the right answer.",
            "First,",
            "Letâ€™s think about this logically.",
            "Letâ€™s solve this problem by splitting it into steps.",
            "Letâ€™s be realistic and think step by step.",
            "Letâ€™s think like a detective step by step.",
            "Letâ€™s think",
            "Before we dive into the answer,",
            "The answer is after the proof.",
            "Let's break this problem down step by step.",
            "We'll tackle this math task one piece at a time.",
            "Let's approach this logically, step by step.",
            "We'll solve this by analyzing each part of the problem.",
            "Let's unravel this mathematical challenge gradually.",
            "We'll methodically work through this problem together.",
            "Let's systematically dissect this math task.",
            "We'll take this mathematical reasoning challenge one step at a time.",
            "Let's meticulously examine each aspect of this problem.",
            "We'll thoughtfully progress through this task step by step."
        ]

    def __call__(self, input: str) -> tuple[str, dict]:
        # å°†å¯è¿›åŒ–çš„å‰ç¼€å’Œå›ºå®šçš„ä»»åŠ¡æŒ‡ä»¤ç»„åˆæˆä¸€ä¸ªå®Œæ•´çš„æç¤º
        # åœ¨ä¼˜åŒ–è¿‡ç¨‹ä¸­ï¼Œoptimizerä¼šä¸æ–­æ›´æ–° self.chain_of_thought_prefix çš„å€¼
        full_prompt = f"{self.chain_of_thought_prefix}\n\n{self.task_instruction}\n\nText:\n{input}"
        
        response = self.model.generate(prompt=full_prompt)
        prediction = response.content.strip()
        
        pattern = r'FINAL_ANSWER\((\([^)]*\))\)'
        match = re.search(pattern, prediction)
        
        if match:
            answer = match.group(1)
        else:
            answer = "N/A"
            
        return answer, {"full_prompt": full_prompt}

    def save(self, path: str):
        pass

    def load(self, path: str):
        pass

async def run_node_evolution_example(algorithm="DE", combination_sample_size=10):
    """
    è¿è¡Œå•èŠ‚ç‚¹æ¼”åŒ–ç¤ºä¾‹ï¼Œè¯¥èŠ‚ç‚¹çš„åˆå§‹ç§ç¾¤ç”±ä¸€ä¸ªåˆ—è¡¨æä¾›ã€‚
    """
    load_dotenv()
    OPENAI_API_KEY = os.getenv("OPENAI_API_KEY")
    if not OPENAI_API_KEY:
        raise ValueError("OPENAI_API_KEY not found in environment variables.")

    # --- é…ç½® ---
    # ç”±äºæ‚¨æä¾›äº†20ä¸ªç§å­ï¼Œæˆ‘ä»¬å¯ä»¥å°†ç§ç¾¤å¤§å°è®¾ç½®ä¸º20ä»¥ä¾¿ç›´æ¥ä½¿ç”¨
    POPULATION_SIZE = 10 
    ITERATIONS = 10 # å¢åŠ è¿­ä»£æ¬¡æ•°ä»¥çœ‹åˆ°æ›´æ˜æ˜¾çš„è¿›åŒ–æ•ˆæœ
    CONCURRENCY_LIMIT = 100
    BENCHMARK_DEV_SAMPLES = 50 # å¢åŠ è¯„ä¼°æ ·æœ¬æ•°ä»¥è·å¾—æ›´å¯é çš„åˆ†æ•°

    # --- è®¾ç½® LLM ---
    llm_config_optimizer = OpenAILLMConfig(
        model="gpt-3.5-turbo-0125", # ä½¿ç”¨æ›´æ–°ã€æ€§ä»·æ¯”æ›´é«˜çš„æ¨¡å‹
        openai_key=OPENAI_API_KEY,
        stream=False,
        temperature=0.5,  # è®¾ç½®æ¸©åº¦ä»¥å¢åŠ ç”Ÿæˆçš„å¤šæ ·æ€§
        top_p=0.95  # ä½¿ç”¨ top-p é‡‡æ ·æ¥æ§åˆ¶ç”Ÿæˆçš„å¤šæ ·æ€§
    )
    llm_config_inference = OpenAILLMConfig(
        model="gpt-3.5-turbo-0125", # ä½¿ç”¨æ›´æ–°ã€æ€§ä»·æ¯”æ›´é«˜çš„æ¨¡å‹
        openai_key=OPENAI_API_KEY,
        stream=False,
        temperature=0,  # è®¾ç½®æ¸©åº¦ä»¥å¢åŠ ç”Ÿæˆçš„å¤šæ ·æ€§
    )
    llm = OpenAILLM(config=llm_config_inference)

    # --- è®¾ç½®åŸºå‡†æµ‹è¯• ---
    benchmark = BIGBenchHard("snarks", dev_sample_num=BENCHMARK_DEV_SAMPLES)
    benchmark._load_data()

    # --- åˆ›å»ºç¨‹åº ---
    program = SinglePromptSarcasmClassifier(model=llm)

    # --- å°†æç¤ºæ³¨å†Œä¸ºç‹¬ç«‹èŠ‚ç‚¹ä»¥è¿›è¡Œç‹¬ç«‹æ¼”åŒ– ---
    # ğŸ”¥æ ¸å¿ƒä¿®æ”¹ï¼šç°åœ¨æˆ‘ä»¬åªè¿½è¸ªä¸€ä¸ªèŠ‚ç‚¹ï¼Œå³åŒ…å«ç§å­åº“çš„ "chain_of_thought_prefix"
    registry = ParamRegistry()
    registry.track(program, "chain_of_thought_prefix", name="cot_prefix_node")

    # --- é€‰æ‹©ä¼˜åŒ–å™¨ ---
    optimizer_class = DEOptimizer if algorithm == "DE" else GAOptimizer
    optimizer = optimizer_class(
        registry=registry,
        program=program,
        population_size=POPULATION_SIZE,
        iterations=ITERATIONS,
        llm_config=llm_config_optimizer,
        concurrency_limit=CONCURRENCY_LIMIT,
        combination_sample_size=combination_sample_size
    )

    # --- è¿è¡Œä¼˜åŒ– ---
    print(f"\n--- Running {algorithm} Optimization ---")
    best_config, _, _ = await optimizer.optimize(benchmark=benchmark)
    
    # ä¼˜åŒ–å™¨å·²å°† program.chain_of_thought_prefix è®¾ç½®ä¸ºæ‰¾åˆ°çš„æœ€ä½³æç¤º
    print(f"\nâœ… Optimization Complete! Best prompt found: '{program.chain_of_thought_prefix}'")


    # --- åœ¨æµ‹è¯•é›†ä¸Šè¯„ä¼°ä¼˜åŒ–åçš„ç¨‹åº ---
    print("\n--- Evaluating on Test Set ---")
    test_data = benchmark.get_test_data()

    async def evaluate_example_concurrently(example: Dict) -> float:
        prediction, _ = await asyncio.to_thread(
            program,
            input=example["input"]
        )
        score_dict = benchmark.evaluate(prediction, benchmark.get_label(example))
        return score_dict.get("em", 0.0)

    if test_data:
        tasks = [evaluate_example_concurrently(ex) for ex in test_data]
        results = await aio_tqdm.gather(*tasks, desc="Evaluating on Test Set")
        correct_count = sum(results)
        test_accuracy = correct_count / len(test_data)
        print(f"Test Accuracy: {test_accuracy:.4f}")
    else:
        test_accuracy = 0.0
    
    # å°†æœ€ç»ˆæ‰¾åˆ°çš„æœ€ä½³é…ç½®ï¼ˆå°½ç®¡åªæœ‰ä¸€ä¸ªï¼‰å’Œæµ‹è¯•å‡†ç¡®ç‡è¿”å›
    return best_config, test_accuracy

async def main():
    """
    ä¸»å‡½æ•°ï¼Œæ¼”ç¤ºå¯¹å•ä¸ªèŠ‚ç‚¹çš„æ¼”åŒ–ï¼Œè¯¥èŠ‚ç‚¹ä»ä¸€ä¸ªç§å­åº“åˆå§‹åŒ–ã€‚
    """
    # ä¾æ¬¡è¿è¡Œ DE å’Œ GA ç®—æ³•
    results = {}
    for algo in ["DE", "GA"]:
        # ç”±äºåªæœ‰ä¸€ä¸ªèŠ‚ç‚¹ï¼Œcombination_sample_size ä¸å†æœ‰æ„ä¹‰ï¼Œå¯ä»¥è®¾ä¸º None
        config, accuracy = await run_node_evolution_example(
            algorithm=algo, 
            combination_sample_size=None
        )
        results[algo] = {"config": config, "accuracy": accuracy}
    
    # --- è¾“å‡ºç»“æœåˆ°CSVæ—¥å¿—æ–‡ä»¶ ---
    import csv
    from datetime import datetime
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    log_path = f"single_node_evolution_log_{timestamp}.csv"
    with open(log_path, mode="w", newline="", encoding="utf-8") as f:
        writer = csv.writer(f)
        writer.writerow(["Algorithm", "Best Prompt", "Test Accuracy"])
        for algo, res in results.items():
            # ä»é…ç½®å­—å…¸ä¸­æå–å‡ºæç¤ºæ–‡æœ¬
            prompt_text = list(res["config"].values())[0]
            writer.writerow([algo, prompt_text, f"{res['accuracy']:.4f}"])
            
    print(f"\nğŸ“Š Final results saved to {log_path}")

if __name__ == "__main__":
    # åœ¨è¿è¡Œå‰ï¼Œè¯·ç¡®ä¿æ‚¨å·²åˆ›å»º .env æ–‡ä»¶å¹¶å¡«å…¥äº† OPENAI_API_KEY
    # åŒæ—¶ï¼Œç¡®ä¿ evo2_optimizer.py æ–‡ä»¶å­˜åœ¨ä¸”åŒ…å«äº†æˆ‘ä»¬ä¹‹å‰ä¿®æ”¹è¿‡çš„ä»£ç 
    asyncio.run(main())