#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
CSVæ•°æ®è½¬LLM JSONæ ¼å¼è½¬æ¢å™¨
å°†è‚¡ç¥¨æ•°æ®CSVæ–‡ä»¶è½¬æ¢ä¸ºé€‚åˆLLMåˆ†æçš„JSONæ ¼å¼
"""

import os
import json
import pandas as pd
from pathlib import Path
from typing import Dict, List, Optional, Union

class CSVToLLMConverter:
    """CSVè½¬LLM JSONæ ¼å¼è½¬æ¢å™¨"""
    
    def __init__(self, data_dir: str):
        """
        åˆå§‹åŒ–è½¬æ¢å™¨
        
        Args:
            data_dir (str): æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¦‚ output_300750ï¼‰
        """
        self.data_dir = Path(data_dir)
        
        # æ–‡ä»¶ä¼˜å…ˆçº§å’Œè¡Œæ•°é…ç½®
        self.file_priority = {
            'stock_daily_catl': {'weight': 'high', 'max_rows': 30},
            'institution_recommendation_catl': {'weight': 'high', 'max_rows': 20},
            'stock_news_catl': {'weight': 'high', 'max_rows': 15},
            'china_cpi': {'weight': 'medium', 'max_rows': 10},
            'china_gdp': {'weight': 'medium', 'max_rows': 10},
            'industry_fund_flow': {'weight': 'medium', 'max_rows': 15},
            'market_overview': {'weight': 'normal', 'max_rows': 5},
            'regional_indices': {'weight': 'normal', 'max_rows': 10},
            'option_volatility': {'weight': 'normal', 'max_rows': 8},
            'fund_flow_industry': {'weight': 'normal', 'max_rows': 12}
        }
    
    def find_csv_files(self) -> Dict[str, Dict]:
        """æŸ¥æ‰¾å¹¶åˆ†ç±»CSVæ–‡ä»¶"""
        csv_files = {}
        
        if not self.data_dir.exists():
            print(f"âŒ æ•°æ®ç›®å½•ä¸å­˜åœ¨: {self.data_dir}")
            return csv_files
        
        for file_path in self.data_dir.glob("*.csv"):
            filename = file_path.name
            
            # è·³è¿‡collection_reportæ–‡ä»¶
            if 'collection_report' in filename.lower():
                continue
            
            # é€šè¿‡æ–‡ä»¶åè¯†åˆ«æ•°æ®ç±»å‹
            file_type = self._identify_file_type(filename)
            if file_type:
                csv_files[file_type] = {
                    'file_path': file_path,
                    'filename': filename,
                    'config': self.file_priority.get(file_type, {'weight': 'normal', 'max_rows': 10})
                }
        
        return csv_files
    
    def _identify_file_type(self, filename: str) -> Optional[str]:
        """æ ¹æ®æ–‡ä»¶åè¯†åˆ«æ•°æ®ç±»å‹"""
        filename_lower = filename.lower()
        
        # å®šä¹‰æ–‡ä»¶åå…³é”®è¯æ˜ å°„
        type_mapping = {
            'stock_daily_catl': ['stock_daily'],
            'institution_recommendation_catl': ['institution_recommendation'],
            'stock_news_catl': ['stock_news'],
            'china_cpi': ['china_cpi'],
            'china_gdp': ['china_gdp'],
            'industry_fund_flow': ['industry_fund_flow'],
            'market_overview': ['market_overview'],
            'regional_indices': ['regional_indices'],
            'option_volatility': ['option_volatility'],
            'fund_flow_industry': ['fund_flow_industry']
        }
        
        for file_type, keywords in type_mapping.items():
            if any(keyword in filename_lower for keyword in keywords):
                return file_type
        
        return None
    
    def read_and_process_csv(self, file_path: Path, max_rows: int, weight: str) -> List[Dict]:
        """è¯»å–å¹¶å¤„ç†CSVæ–‡ä»¶"""
        try:
            df = pd.read_csv(file_path, encoding='utf-8-sig')
            
            if df.empty:
                print(f"âš ï¸ æ–‡ä»¶ä¸ºç©º: {file_path.name}")
                return []
            
            # æ ¹æ®æƒé‡é€‰æ‹©æ•°æ®è¡Œ
            if weight == 'high':
                # é«˜ä¼˜å…ˆçº§ï¼šå–æœ€æ–°çš„æ•°æ®ï¼ˆæœ«å°¾ï¼‰
                processed_df = df.tail(max_rows)
            else:
                # å…¶ä»–ä¼˜å…ˆçº§ï¼šå–å¼€å¤´çš„æ•°æ®
                processed_df = df.head(max_rows)
            
            # å¡«å……NaNå€¼
            processed_df = processed_df.fillna('')
            
            # è½¬æ¢ä¸ºå­—å…¸åˆ—è¡¨
            records = processed_df.to_dict(orient='records')
            
            print(f"âœ… å¤„ç†å®Œæˆ {file_path.name}: {len(records)} æ¡è®°å½•")
            return records
            
        except Exception as e:
            print(f"âŒ å¤„ç†æ–‡ä»¶å¤±è´¥ {file_path.name}: {e}")
            return []
    
    def generate_llm_analysis_prompt(self) -> str:
        """ç”Ÿæˆé€‚åˆLLMåˆ†æçš„æç¤ºæ ¼å¼"""
        csv_files = self.find_csv_files()
        
        if not csv_files:
            return "No valid CSV files found in the specified directory."
        
        # æŒ‰æƒé‡æ’åºï¼Œè‚¡ç¥¨æ—¥çº¿æ•°æ®ä¼˜å…ˆ
        def sort_priority(item):
            file_type, file_info = item
            weight = file_info['config']['weight']
            
            # è‚¡ç¥¨æ—¥çº¿æ•°æ®æœ€ä¼˜å…ˆ
            if 'stock_daily_catl' in file_type:
                return (0, 0)  # æœ€é«˜ä¼˜å…ˆçº§
            
            weight_order = {'high': 1, 'medium': 2, 'normal': 3}
            base_priority = weight_order.get(weight, 4)
            
            # åœ¨åŒæƒé‡å†…ï¼ŒæŒ‰æ–‡ä»¶ç±»å‹ç»†åˆ†
            if weight == 'high':
                if 'institution_recommendation' in file_type:
                    return (base_priority, 1)
                elif 'stock_news' in file_type:
                    return (base_priority, 2)
            
            return (base_priority, 0)
        
        sorted_files = sorted(csv_files.items(), key=sort_priority)
        
        # æ„å»ºLLMåˆ†ææç¤º
        prompt_parts = []
        
        # æ·»åŠ æ€»ä½“è¯´æ˜
        stock_code = self._extract_stock_code()
        prompt_parts.append(f"# è‚¡ç¥¨ {stock_code} ç»¼åˆæ•°æ®åˆ†æ")
        prompt_parts.append("\nä»¥ä¸‹æ˜¯è¯¥è‚¡ç¥¨çš„å„ç±»æ•°æ®ï¼Œè¯·è¿›è¡Œç»¼åˆåˆ†æå¹¶ç»™å‡ºæŠ•èµ„å»ºè®®ï¼š\n")
        
        # æ·»åŠ æ•°æ®æ¦‚è§ˆ
        prompt_parts.append("## ğŸ“Š æ•°æ®æ¦‚è§ˆ")
        for i, (file_type, file_info) in enumerate(sorted_files, 1):
            weight_emoji = {"high": "ğŸ”¥", "medium": "â­", "normal": "ğŸ“‹"}
            emoji = weight_emoji.get(file_info['config']['weight'], "ğŸ“‹")
            prompt_parts.append(f"{i}. {emoji} {self._get_chinese_name(file_type)} ({file_info['filename']})")
        
        prompt_parts.append("\n## ğŸ“ˆ è¯¦ç»†æ•°æ®\n")
        
        # æ·»åŠ æ¯ä¸ªæ•°æ®é›†
        for i, (file_type, file_info) in enumerate(sorted_files, 1):
            file_path = file_info['file_path']
            config = file_info['config']
            
            # è¯»å–å’Œå¤„ç†æ•°æ®
            data = self.read_and_process_csv(file_path, config['max_rows'], config['weight'])
            
            if not data:
                continue
            
            # æ·»åŠ æ•°æ®é›†æ ‡é¢˜
            chinese_name = self._get_chinese_name(file_type)
            priority_label = {"high": "(é‡ç‚¹å…³æ³¨)", "medium": "(é‡è¦å‚è€ƒ)", "normal": "(èƒŒæ™¯ä¿¡æ¯)"}
            priority = priority_label.get(config['weight'], "")
            
            prompt_parts.append(f"### Dataset {i}: {chinese_name} {priority}")
            prompt_parts.append(f"æ–‡ä»¶: {file_info['filename']}")
            prompt_parts.append(f"æ•°æ®é‡: {len(data)} æ¡è®°å½•\n")
            
            # æ·»åŠ JSONæ•°æ®
            json_data = json.dumps(data, ensure_ascii=False, indent=2)
            prompt_parts.append("```json")
            prompt_parts.append(json_data)
            prompt_parts.append("```\n")
        
        # æ·»åŠ åˆ†æè¦æ±‚
        prompt_parts.append("## ğŸ¯ åˆ†æè¦æ±‚")
        prompt_parts.append("è¯·åŸºäºä»¥ä¸Šæ•°æ®è¿›è¡Œä»¥ä¸‹åˆ†æï¼š")
        prompt_parts.append("1. **ä»·æ ¼è¶‹åŠ¿åˆ†æ**: æ ¹æ®è‚¡ç¥¨æ—¥çº¿æ•°æ®åˆ†æä»·æ ¼èµ°åŠ¿")
        prompt_parts.append("2. **æŠ€æœ¯æŒ‡æ ‡è¯„ä¼°**: ç»“åˆç§»åŠ¨å¹³å‡çº¿ã€æˆäº¤é‡ç­‰æŠ€æœ¯æŒ‡æ ‡")
        prompt_parts.append("3. **æœºæ„è§‚ç‚¹**: åˆ†ææœºæ„è¯„çº§å’Œç›®æ ‡ä»·")
        prompt_parts.append("4. **å¸‚åœºç¯å¢ƒ**: è€ƒè™‘å®è§‚ç»æµæ•°æ®å’Œè¡Œä¸šèµ„é‡‘æµå‘")
        prompt_parts.append("5. **æ–°é—»å½±å“**: è¯„ä¼°ç›¸å…³æ–°é—»å¯¹è‚¡ä»·çš„æ½œåœ¨å½±å“")
        prompt_parts.append("6. **æŠ•èµ„å»ºè®®**: ç»™å‡ºæ˜ç¡®çš„ä¹°å…¥/æŒæœ‰/å–å‡ºå»ºè®®åŠç†ç”±")
        prompt_parts.append("\nè¯·ç”¨ä¸­æ–‡å›ç­”ï¼Œå¹¶æä¾›å…·ä½“çš„æ•°æ®æ”¯æ’‘ã€‚")
        
        return "\n".join(prompt_parts)
    
    def _extract_stock_code(self) -> str:
        """ä»ç›®å½•åæå–è‚¡ç¥¨ä»£ç """
        dir_name = self.data_dir.name
        if 'output_' in dir_name:
            return dir_name.replace('output_', '')
        return dir_name
    
    def _get_chinese_name(self, file_type: str) -> str:
        """è·å–æ•°æ®ç±»å‹çš„ä¸­æ–‡åç§°"""
        name_mapping = {
            'stock_daily_catl': 'Stock Daily Price Data (è‚¡ç¥¨æ—¥çº¿æ•°æ®)',
            'institution_recommendation_catl': 'Institution Recommendations (æœºæ„è¯„çº§)',
            'stock_news_catl': 'Stock News (è‚¡ç¥¨æ–°é—»)',
            'china_cpi': 'China CPI (ä¸­å›½CPI)',
            'china_gdp': 'China GDP (ä¸­å›½GDP)',
            'industry_fund_flow': 'Industry Fund Flow (è¡Œä¸šèµ„é‡‘æµ)',
            'market_overview': 'Market Overview (å¸‚åœºæ¦‚å†µ)',
            'regional_indices': 'Regional Indices (åŒºåŸŸæŒ‡æ•°)',
            'option_volatility': 'Option Volatility (æœŸæƒæ³¢åŠ¨ç‡)',
            'fund_flow_industry': 'Fund Flow Industry (è¡Œä¸šèµ„é‡‘æµå‘)'
        }
        return name_mapping.get(file_type, file_type)
    
    def save_prompt_to_file(self, output_path: str = None) -> str:
        """ä¿å­˜æç¤ºå†…å®¹åˆ°æ–‡ä»¶"""
        if output_path is None:
            output_path = self.data_dir / "llm_analysis_prompt.txt"
        
        prompt_content = self.generate_llm_analysis_prompt()
        
        try:
            with open(output_path, 'w', encoding='utf-8') as f:
                f.write(prompt_content)
            
            file_size = os.path.getsize(output_path)
            print(f"âœ… LLMåˆ†ææç¤ºå·²ä¿å­˜: {output_path}")
            print(f"ğŸ“„ æ–‡ä»¶å¤§å°: {file_size:,} å­—èŠ‚")
            
            return str(output_path)
            
        except Exception as e:
            print(f"âŒ ä¿å­˜æ–‡ä»¶å¤±è´¥: {e}")
            return ""
    
    def get_json_data(self) -> Dict[str, List[Dict]]:
        """ç›´æ¥è·å–JSONæ ¼å¼çš„æ•°æ®å­—å…¸"""
        csv_files = self.find_csv_files()
        json_data = {}
        
        for file_type, file_info in csv_files.items():
            config = file_info['config']
            data = self.read_and_process_csv(
                file_info['file_path'], 
                config['max_rows'], 
                config['weight']
            )
            if data:
                chinese_name = self._get_chinese_name(file_type)
                json_data[chinese_name] = data
        
        return json_data


def convert_csv_to_llm_json(data_dir: str, output_file: str = None) -> str:
    """
    å¿«é€Ÿè½¬æ¢CSVæ•°æ®ä¸ºLLM JSONæ ¼å¼çš„ä¸»å‡½æ•°
    
    Args:
        data_dir (str): æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¦‚ output_300750ï¼‰
        output_file (str): è¾“å‡ºæ–‡ä»¶è·¯å¾„ï¼ˆå¯é€‰ï¼‰
        
    Returns:
        str: ç”Ÿæˆçš„æç¤ºæ–‡ä»¶è·¯å¾„
        
    Example:
        convert_csv_to_llm_json("output_300750")
        convert_csv_to_llm_json("output_600519", "my_prompt.txt")
    """
    print(f"ğŸ”„ å¼€å§‹è½¬æ¢ {data_dir} ä¸­çš„CSVæ•°æ®...")
    
    converter = CSVToLLMConverter(data_dir)
    result_path = converter.save_prompt_to_file(output_file)
    
    if result_path:
        print(f"âœ… è½¬æ¢å®Œæˆ: {os.path.abspath(result_path)}")
    else:
        print("âŒ è½¬æ¢å¤±è´¥")
    
    return result_path


def get_stock_data_json(data_dir: str) -> Dict[str, List[Dict]]:
    """
    è·å–è‚¡ç¥¨æ•°æ®çš„JSONæ ¼å¼å­—å…¸
    
    Args:
        data_dir (str): æ•°æ®ç›®å½•è·¯å¾„ï¼ˆå¦‚ output_300750ï¼‰
        
    Returns:
        Dict[str, List[Dict]]: åŒ…å«æ‰€æœ‰æ•°æ®çš„å­—å…¸
        
    Example:
        data = get_stock_data_json("output_300750")
        print(data.keys())  # æŸ¥çœ‹æ‰€æœ‰æ•°æ®ç±»å‹
    """
    converter = CSVToLLMConverter(data_dir)
    return converter.get_json_data()


if __name__ == "__main__":
    # æ¼”ç¤ºç”¨æ³•
    print("ğŸš€ CSVè½¬LLM JSONæ ¼å¼è½¬æ¢å™¨")
    print("=" * 50)
    
    # ç¤ºä¾‹ï¼šè½¬æ¢å®å¾·æ—¶ä»£æ•°æ®
    example_dir = "output_300750"
    
    if os.path.exists(example_dir):
        print(f"ğŸ“Š è½¬æ¢ç¤ºä¾‹: {example_dir}")
        
        # æ–¹æ³•1ï¼šç”ŸæˆLLMæç¤ºæ–‡ä»¶
        prompt_file = convert_csv_to_llm_json(example_dir)
        
        # æ–¹æ³•2ï¼šç›´æ¥è·å–JSONæ•°æ®
        json_data = get_stock_data_json(example_dir)
        print(f"\nğŸ“‹ è·å–åˆ° {len(json_data)} ä¸ªæ•°æ®ç±»å‹:")
        for data_type in json_data.keys():
            print(f"   ğŸ“Š {data_type}")
    
    else:
        print(f"âš ï¸ ç¤ºä¾‹ç›®å½•ä¸å­˜åœ¨: {example_dir}")
        print("è¯·å…ˆè¿è¡Œæ•°æ®æ”¶é›†ç¨‹åºç”Ÿæˆè‚¡ç¥¨æ•°æ®")
    
    print("\nğŸ’¡ ä½¿ç”¨æ–¹æ³•:")
    print("1. convert_csv_to_llm_json('output_è‚¡ç¥¨ä»£ç ')")
    print("2. get_stock_data_json('output_è‚¡ç¥¨ä»£ç ')") 